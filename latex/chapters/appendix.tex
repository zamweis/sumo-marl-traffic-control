\appendix

Dieser Anhang enthält die vollständigen Python-Skripte, die zur Validierung, Reparatur und Steuerung der SUMO-basierten Reinforcement-Learning-Umgebung eingesetzt wurden. Jedes Unterkapitel dokumentiert ein spezifisches Tool oder Modul aus dem Projekt.

\section{Trainings-Skripte}

\subsection{\texttt{train.py} – Trainingsskript für PPO über mehrere Seeds}
\label{app:train_script}
Das folgende Skript enthält die vollständige Trainingslogik für das Reinforcement Learning mit \texttt{sumo-rl} unter Verwendung von \texttt{Stable-Baselines3}.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
# ====== Bibliotheken und Module ======
# Standard-Module für Dateiverwaltung, Zeit und Regex
import os
import re
import time
import datetime
import random

# SUMO-Interface (TraCI) für Simulation
import traci

# Mathematische und numerische Berechnungen
import numpy as np

# PyTorch für neuronale Netze und Reproduzierbarkeit
import torch

# Stable-Baselines3 (RL-Algorithmen, hier PPO)
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CallbackList

# SUMO-RL-Umgebung (PettingZoo-kompatibel)
from sumo_rl.environment.env import parallel_env

# SuperSuit – Hilfsfunktionen, um PettingZoo-Umgebungen mit SB3 zu verwenden
from supersuit import (
    pad_observations_v0,          # Padding für Beobachtungen, um feste Größe zu garantieren
    pad_action_space_v0,          # Padding für Aktionsraum
    pettingzoo_env_to_vec_env_v1, # Konvertierung zu SB3-kompatiblem Vektor-Env
    concat_vec_envs_v1            # Mehrere Envs parallel laufen lassen
)

# Gymnasium für RL-Umgebungs-Schnittstellen
import gymnasium as gym
from gymnasium import Wrapper


# ====== Trainings-Setup ======
SEEDS = [143534, 456, 635768, 13755]  # Verschiedene Zufalls-Seed-Werte für reproduzierbare Runs
ROUTE_FILES = [
    "flows_low.rou.xml",
    "flows_medium.rou.xml",
    "flows_high.rou.xml",
]

# ====== Schedules für Hyperparameter-Anpassung ======
# (Funktionen, die während des Trainings den Wert z. B. von Lernrate oder Clip-Bereich dynamisch anpassen)
def adaptive_entropy_schedule(start=0.01):
    return lambda progress: max(0.001, start * (1 - progress))

def dynamic_clip_range(start=0.2, end=0.1):
    return lambda pr: end + (start - end) * pr

def cosine_clip(start=0.2, end=0.1):
    return lambda pr: end + (start - end) * 0.5 * (1 + np.cos(np.pi * (1 - pr)))

def linear_schedule(start):
    return lambda progress: start * (1 - progress)

def cosine_warmup_floor(start=3e-4, warmup_frac=0.05, min_lr_frac=0.1):
    """
    Lernrate: Erst linear hochfahren (Warmup), dann mit Cosinus auf Minimalwert absenken.
    """
    min_lr = start * min_lr_frac
    warmup_frac = max(0.0, min(0.5, warmup_frac))
    def schedule(progress_remaining: float) -> float:
        t = 1.0 - progress_remaining
        if t < warmup_frac:
            base = start * 0.1 + (start - start * 0.1) * (t / warmup_frac)
        else:
            tt = (t - warmup_frac) / max(1e-8, (1.0 - warmup_frac))
            cos_term = 0.5 * (1 + np.cos(np.pi * tt))
            base = min_lr + (start - min_lr) * cos_term
        return float(base)
    return schedule

# ====== Hilfsfunktionen und Callbacks ======
# (Modelle finden, Checkpoints speichern, Metriken loggen, bestes Modell sichern)
# ====== Letzten vollständigen Run finden ======
def find_latest_complete_run(base_dir="runs", prefix="ppo_sumo_"):
    """
    Sucht im 'runs'-Ordner nach dem neuesten Trainingslauf, der
    - eine gespeicherte VecNormalize-Instanz hat
    - und entweder ein finales Modell oder mindestens einen Checkpoint.
    Gibt die Pfade zu Run-Ordner, Modell und Normalisierungsdatei zurück.
    """
    subdirs = sorted(
        [d for d in os.listdir(base_dir) if d.startswith(prefix)],
        reverse=True
    )
    for d in subdirs:
        dir_path = os.path.join(base_dir, d)
        norm_path = os.path.join(dir_path, "vecnormalize.pkl")
        if not os.path.exists(norm_path):
            continue

        # Prüfe auf finales Modell
        final_model = os.path.join(dir_path, "model.zip")
        if os.path.exists(final_model):
            return dir_path, final_model, norm_path

        # Falls kein finales Modell: Prüfe auf Checkpoints
        checkpoint_models = [
            f for f in os.listdir(dir_path)
            if re.match(r"ppo_sumo_model_(\d+)_steps\.zip", f)
        ]
        if checkpoint_models:
            checkpoint_models.sort(key=lambda x: int(re.findall(r"\d+", x)[0]), reverse=True)
            best_checkpoint = checkpoint_models[0]
            return dir_path, os.path.join(dir_path, best_checkpoint), norm_path

    return None

def make_env(seed, route_files):
    def _init():
        env = parallel_env(
            net_file="map.net.xml",
            route_file=route_files[0],  # Platzhalter
            use_gui=False,
            num_seconds=4096,
            reward_fn="diff-waiting-time",
            min_green=5,
            max_depart_delay=100,
            sumo_seed=seed,
            add_system_info=True,
            add_per_agent_info=False,
        )
        if hasattr(env, "seed"):
            env.seed(seed)

        orig_reset = env.reset
        idx = {"i": -1}  # mutierbares Zähl-Objekt im Closure

        def reset_with_round_robin(**kwargs):
            idx["i"] = (idx["i"] + 1) % len(route_files)
            new_route = route_files[idx["i"]]
            env.route_file = new_route
            if hasattr(env, "sumo_seed"):
                env.sumo_seed = seed
            print(f"\n[DEBUG] Reset → Route: {new_route} | Seed: {seed}\n", flush=True)
            return orig_reset(**kwargs)

        env.reset = reset_with_round_robin
        return env
    return _init


def shorten_key(orig_key: str) -> str:
    return orig_key.replace("system_", "")

# ====== Callback: Zeitbasiertes Speichern ======
class TimeBasedCheckpointCallback(BaseCallback):
    """
    Speichert Modell und Normalisierungsdaten in festen Zeitintervallen (Sekunden).
    """
    def __init__(self, save_interval_sec, save_path, name_prefix="ppo_sumo_model", verbose=0):
        super().__init__(verbose)
        self.save_interval_sec = save_interval_sec
        self.save_path = save_path
        self.name_prefix = name_prefix
        self.last_save_time = time.time()

    def _on_step(self) -> bool:
        return True  # Keine Aktion bei jedem einzelnen Step

    def _on_rollout_end(self) -> bool:
        # Am Ende eines Rollouts prüfen, ob das Zeitintervall abgelaufen ist
        current_time = time.time()
        if current_time - self.last_save_time >= self.save_interval_sec:
            timestep = self.num_timesteps
            filename = f"{self.name_prefix}_{timestep}_steps"
            self.model.save(os.path.join(self.save_path, filename + ".zip"))
            if hasattr(self.training_env, "save"):
                self.training_env.save(os.path.join(self.save_path, f"{filename}_vecnormalize.pkl"))
            print(f"[Checkpoint] Modell gespeichert bei {timestep} Schritten ({filename})")
            self.last_save_time = current_time
        return True


# ====== Callback: Metriken aus der Env loggen ======
class EpisodeMetricsLoggerCallback(BaseCallback):
    def __init__(self, prefix="episode", verbose=0):
        super().__init__(verbose)
        self.prefix = prefix
        self.verbose = verbose
        self.sums = {}
        self.counts = {}
        self.last_totals = {}

    def _on_step(self) -> bool:
        dones = self.locals.get("dones")
        infos = self.locals.get("infos")
        if infos is None:
            return True

        for i, info in enumerate(infos):
            if not isinstance(info, dict):
                continue

            if dones is not None and dones[i]:
                # --- Episode zu Ende ---
                fin = info.get("final_info") or info.get("terminal_info")
                if isinstance(fin, dict):
                    for k, v in fin.items():
                        if not isinstance(v, (int, float)) or not np.isfinite(v):
                            continue
                        if k.startswith("system_mean_"):
                            self.sums[k] = self.sums.get(k, 0.0) + float(v)
                            self.counts[k] = self.counts.get(k, 0) + 1
                        elif k.startswith("system_total_"):
                            self.last_totals[k] = float(v)
            else:
                # --- Nur Zwischenschritt, solange Episode noch läuft ---
                for k, v in info.items():
                    if not isinstance(v, (int, float)) or not np.isfinite(v):
                        continue
                    if k.startswith("system_mean_") or k in [
                        "system_total_waiting_time",
                        "system_total_stopped",
                        "system_total_running",
                    ]:
                        self.sums[k] = self.sums.get(k, 0.0) + float(v)
                        self.counts[k] = self.counts.get(k, 0) + 1
                    elif k.startswith("system_total_"):
                        self.last_totals[k] = float(v)

        # Episode fertig → loggen
        if dones is not None and any(dones):
            for k, total in self.sums.items():
                mean_val = total / max(1, self.counts.get(k, 1))
                short_key = shorten_key(k)
                self.logger.record(f"{self.prefix}/{short_key}", mean_val)
                if self.verbose:
                    print(f"[EpisodeMetrics] {short_key} (mean) = {mean_val:.3f}")

            for k, v in self.last_totals.items():
                short_key = shorten_key(k)
                self.logger.record(f"{self.prefix}/{short_key}", v)
                if self.verbose:
                    print(f"[EpisodeMetrics] {short_key} (total) = {v:.0f}")

            # Reset für nächste Episode
            self.sums.clear()
            self.counts.clear()
            self.last_totals.clear()

        return True

# ====== Callback: Bestes Modell speichern ======
class BestModelSaverCallback(BaseCallback):
    """
    Speichert das Modell mit dem bisher höchsten mittleren Episodenreward.
    """
    def __init__(self, save_path, verbose=0):
        super().__init__(verbose)
        self.best_mean_reward = -float('inf')
        self.save_path = save_path

    def _on_step(self) -> bool:
        return True

    def _on_rollout_end(self):
        ep_info_buffer = self.model.ep_info_buffer
        if len(ep_info_buffer) > 0:
            mean_rew = np.mean([ep_info['r'] for ep_info in ep_info_buffer])
            if mean_rew > self.best_mean_reward:
                self.best_mean_reward = mean_rew
                model_path = os.path.join(self.save_path, "best_model.zip")
                self.model.save(model_path)
                if hasattr(self.model.env, "save"):
                    norm_path = os.path.join(self.save_path, "best_model_vecnormalize.pkl")
                    self.model.env.save(norm_path)
                print(f"[AUTOLOG] Neuer Bestwert {mean_rew:.2f} → best_model gespeichert!", flush=True)


# ====== Haupt-Trainingsschleife ======
for SEED in SEEDS:
    # Reproduzierbarkeit sicherstellen
    np.random.seed(SEED)
    torch.manual_seed(SEED)

    # Log-Verzeichnis erstellen
    now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_dir = os.path.join("runs", f"ppo_sumo_{SEED}_{now}")
    os.makedirs(log_dir, exist_ok=True)

    print(f"\n[INFO] Starte Training mit Seed: {SEED}")

    # SUMO-Umgebung initialisieren
    env = make_env(SEED, ROUTE_FILES)()

    # Falls die Env einen seed()-Aufruf unterstützt
    if hasattr(env, "seed"):
        env.seed(SEED)

    # Anpassung der Beobachtungen und Aktionen an SB3
    env = pad_observations_v0(env)
    env = pad_action_space_v0(env)
    env = pettingzoo_env_to_vec_env_v1(env)

    # WICHTIG: trotzdem concat_vec_envs_v1 mit num_vec_envs=1
    env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class="stable_baselines3")


    # Logging und Normalisierung
    env = VecMonitor(env, filename=os.path.join(log_dir, "monitor.csv"))
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)

    # PPO-Agent erstellen
    model = PPO(
        policy="MlpPolicy",      # Mehrschicht-Perzeptron-Policy
        env=env,
        verbose=1,               # Ausführliches Logging
        tensorboard_log=log_dir, # TensorBoard-Pfad
        batch_size=256,          # Minibatch-Größe für PPO
        n_steps=2048,            # Rollout-Länge
        learning_rate=cosine_warmup_floor(start=3e-4, warmup_frac=0.05, min_lr_frac=0.1),
        clip_range=cosine_clip(), # Clipping-Range dynamisch
        ent_coef=0.01,            # Entropie-Koeffizient (Exploration)
        gamma=0.99,               # Diskontfaktor
        gae_lambda=0.95,          # Lambda für GAE
        device="cpu",             # Training auf CPU
        policy_kwargs=dict(net_arch=dict(pi=[128, 128], vf=[128, 128])), # Netzarchitekturgit
    )

    # Callback-Liste: Checkpoints, Logging, Best-Model-Speicherung
    callbacks = CallbackList([
        TimeBasedCheckpointCallback(
            save_interval_sec=3600, # Jede Stunde speichern
            save_path=log_dir,
            name_prefix="ppo_sumo_model",
            verbose=1,
        ),
        EpisodeMetricsLoggerCallback(),
        BestModelSaverCallback(save_path=log_dir),
    ])

    # Training starten
    try:
        time.sleep(3) # Kurze Pause für saubere Konsolenlogs
        model.learn(
            total_timesteps=2_000_000,
            callback=callbacks,
        )
        # Nach Abschluss final speichern
        model.save(os.path.join(log_dir, "model.zip"))
        env.save(os.path.join(log_dir, "vecnormalize.pkl"))
        print(f"\n[INFO] Training abgeschlossen für Seed {SEED}. Modell gespeichert unter: {log_dir}")

    # Falls Training manuell abgebrochen wird (Strg+C)
    except KeyboardInterrupt:
        print("[ABBRUCH] Manuelles Beenden erkannt. Speichere aktuellen Stand...")
        model.save(os.path.join(log_dir, "model_interrupt.zip"))
        env.save(os.path.join(log_dir, "vecnormalize_interrupt.pkl"))

    # Generelle Fehlerbehandlung
    except Exception as e:
        print(f"\n[FEHLER] Während des Trainings bei Seed {SEED} aufgetreten: {e}")

    # Cleanup: Env schließen und Normalisierungsdaten sichern
    finally:
        try:
            env.save(os.path.join(log_dir, "vecnormalize.pkl"))
        except Exception as e:
            print(f"[WARNUNG] VecNormalize konnte nicht gespeichert werden: {e}")
        env.close()

\end{minted}

\subsection{\texttt{continuetrain.py} – Trainingsskript zum Weitertrainieren}
\label{app:continuetrain}
Startet für jede einzelne Ampelkreuzung eine Minimalumgebung und überprüft, ob diese in \texttt{sumo-rl} trainierbar ist.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import os
import re
import time
import datetime
import traci
import numpy as np
import torch
import json
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
from sumo_rl.environment.env import parallel_env
from supersuit import (
    pad_observations_v0,
    pad_action_space_v0,
    pettingzoo_env_to_vec_env_v1,
    concat_vec_envs_v1
)
from gym import Wrapper

# ==== Seed setzen ====
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

# ==== Adaptive Parameter-Schedules ====
def dynamic_clip_range(start=0.2):
    return lambda progress: max(0.1, start * (1 - 0.5 * progress))

def linear_schedule(start):
    return lambda progress: start * (1 - progress)

# ==== Finde letzten vollständigen Run ====
def find_latest_complete_run(base_dir="runs", prefix="ppo_sumo_"):
    subdirs = sorted(
        [d for d in os.listdir(base_dir) if d.startswith(prefix)],
        reverse=True
    )
    for d in subdirs:
        dir_path = os.path.join(base_dir, d)
        norm_path = os.path.join(dir_path, "vecnormalize.pkl")
        if not os.path.exists(norm_path):
            continue

        final_model = os.path.join(dir_path, "model.zip")
        if os.path.exists(final_model):
            return dir_path, final_model, norm_path

        checkpoint_models = [
            f for f in os.listdir(dir_path)
            if re.match(r"ppo_sumo_model_(\d+)_steps\.zip", f)
        ]
        if checkpoint_models:
            checkpoint_models.sort(key=lambda x: int(re.findall(r"\d+", x)[0]), reverse=True)
            best_checkpoint = checkpoint_models[0]
            return dir_path, os.path.join(dir_path, best_checkpoint), norm_path

    return None

# ==== Zeitbasierter Checkpoint Callback ====
class TimeBasedCheckpointCallback(BaseCallback):
    def __init__(self, save_interval_sec, save_path, name_prefix="ppo_sumo_model", verbose=0):
        super().__init__(verbose)
        self.save_interval_sec = save_interval_sec
        self.save_path = save_path
        self.name_prefix = name_prefix
        self.last_save_time = time.time()

    def _on_step(self) -> bool:
        return True
        
    def _on_rollout_end(self) -> bool:
        current_time = time.time()
        if current_time - self.last_save_time >= self.save_interval_sec:
            timestep = self.num_timesteps
            filename = f"{self.name_prefix}_{timestep}_steps"
            self.model.save(os.path.join(self.save_path, filename + ".zip"))
            if hasattr(self.training_env, "save"):
                self.training_env.save(os.path.join(self.save_path, f"{filename}_vecnormalize.pkl"))
            print(f"[Checkpoint] Modell gespeichert bei {timestep} Schritten ({filename})")
            self.last_save_time = current_time
        return True

# ==== Learning Rate Logger Callback ====
class LearningRateLoggerCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        lr = self.model.lr_schedule(self.num_timesteps / self.model._total_timesteps)
        self.logger.record("train/learning_rate", lr)
        return True

# ==== Logging ====
now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_dir = os.path.join("runs", f"ppo_sumo_{now}")
os.makedirs(log_dir, exist_ok=True)

# ==== SUMO-RL Umgebung ====
env = parallel_env(
    net_file="network.net.xml",
    route_file="flow.rou.xml",
    use_gui=False,
    num_seconds=4096,
    reward_fn="diff-waiting-time",
    min_green=5,
    max_depart_delay=100,
    sumo_seed=SEED,
    add_system_info=True,
    add_per_agent_info=False,
)

if hasattr(env, "seed"):
    env.seed(SEED)

# ==== Wrapping ====
env = pad_observations_v0(env)
env = pad_action_space_v0(env)
env = pettingzoo_env_to_vec_env_v1(env)
env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=8, base_class="stable_baselines3")
env = VecMonitor(env)

# ==== Modell laden oder neu starten ====
result = find_latest_complete_run()
if result:
    latest_run_dir, model_path, normalize_path = result
    print("Fortsetzung wird gestartet mit:")
    print(f"Verzeichnis : {latest_run_dir}")
    print(f"Modell      : {model_path}")
    print(f"Normalize   : {normalize_path}\n")

    env = VecNormalize.load(normalize_path, env)
    env.training = True
    env.norm_reward = True

    model = PPO.load(model_path, env=env, tensorboard_log=log_dir, verbose=1, device="cpu")
    print(f"[INFO] Modell startet bei {model.num_timesteps} Timesteps.")
else:
    print("[INFO] Kein vorheriges Modell gefunden. Starte frisches Training.\n")
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)
    model = PPO(
        policy="MlpPolicy",      # Mehrschicht-Perzeptron-Policy
        env=env,
        verbose=1,               # Ausführliches Logging
        tensorboard_log=log_dir, # TensorBoard-Pfad
        batch_size=256,          # Minibatch-Größe für PPO
        n_steps=2048,            # Rollout-Länge
        learning_rate=cosine_warmup_floor(start=3e-4, warmup_frac=0.05, min_lr_frac=0.1),
        clip_range=cosine_clip(), # Clipping-Range dynamisch
        ent_coef=0.01,            # Entropie-Koeffizient (Exploration)
        gamma=0.99,               # Diskontfaktor
        gae_lambda=0.95,          # Lambda für GAE
        device="cpu",             # Training auf CPU
        policy_kwargs=dict(net_arch=dict(pi=[128, 128], vf=[128, 128])), # Netzarchitekturgit
    )

# ==== Automatisches Speichern bei verbessertem ep_rew_mean ====
class BestModelSaverCallback(BaseCallback):
    def __init__(self, save_path, verbose=0):
        super().__init__(verbose)
        self.best_mean_reward = -float('inf')
        self.save_path = save_path

    def _on_step(self) -> bool:
        # Muss vorhanden sein, selbst wenn sie nichts tut
        return True
        
    def _on_rollout_end(self):
        ep_info_buffer = self.model.ep_info_buffer
        if len(ep_info_buffer) > 0:
            mean_rew = np.mean([ep_info['r'] for ep_info in ep_info_buffer])
            
            if mean_rew > self.best_mean_reward:
                self.best_mean_reward = mean_rew

                model_path = os.path.join(self.save_path, "best_model.zip")
                self.model.save(model_path)

                if hasattr(self.model.env, "save"):
                    norm_path = os.path.join(self.save_path, "best_model_vecnormalize.pkl")
                    self.model.env.save(norm_path)

                print(f"[AUTOLOG] Neuer Bestwert {mean_rew:.2f} → best_model gespeichert!", flush=True)

# ==== Callbacks kombinieren ====
callbacks = CallbackList([
    TimeBasedCheckpointCallback(
        save_interval_sec=3600,
        save_path=log_dir,
        name_prefix="ppo_sumo_model",
        verbose=1,
    ),
    LearningRateLoggerCallback(),
    BestModelSaverCallback(save_path=log_dir),
])

# ==== Training starten ====
try:
    model.learn(
        total_timesteps=1_000_000,
        callback=callbacks,
    )
    model.save(os.path.join(log_dir, "model.zip"))
    env.save(os.path.join(log_dir, "vecnormalize.pkl"))
    print(f"\n[INFO] Training abgeschlossen. Modell gespeichert unter: {log_dir}")

except KeyboardInterrupt:
    print("[ABBRUCH] Manuelles Beenden erkannt. Speichere aktuellen Stand...")
    model.save(os.path.join(log_dir, "model_interrupt.zip"))
    env.save(os.path.join(log_dir, "vecnormalize_interrupt.pkl"))

except Exception as e:
    print(f"\n[FEHLER] Während des Trainings aufgetreten: {e}")

finally:
    try:
        env.save(os.path.join(log_dir, "vecnormalize.pkl"))
    except Exception as e:
        print(f"[WARNUNG] VecNormalize konnte nicht gespeichert werden: {e}")
    env.close()
\end{minted}

\section{Belohnungsfunktionen}
\label{app:rewardfunktionen}

\subsection{\texttt{diff-waiting-time}}
\label{app:reward_diff_waiting_time}
Diese Belohnungsfunktion misst die Differenz der kumulierten Wartezeit zwischen zwei Zeitschritten. Sie belohnt eine Abnahme der Gesamtwartezeit.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
def diff_waiting_time_reward(traffic_signal):
    ts_wait = sum(traffic_signal.get_accumulated_waiting_time_per_lane()) / 100.0
    reward = traffic_signal.last_ts_waiting_time - ts_wait
    traffic_signal.last_ts_waiting_time = ts_wait
    return reward
\end{minted}

\subsection{\texttt{queue}}
\label{app:reward_queue}
Hier wird die Anzahl an gestoppten Fahrzeugen direkt als negativer Reward verwendet. Weniger Stau → höherer Reward.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
def queue_reward(traffic_signal):
    return -traffic_signal.get_total_queued()

def get_total_queued(traffic_signal) -> int:
    """Returns the total number of vehicles halting in the intersection."""
    return sum(traffic_signal.sumo.lane.getLastStepHaltingNumber(lane) for lane in traffic_signal.lanes)
\end{minted}

\subsection{\texttt{realworld}}
\label{app:reward_realworld}
Diese Funktion kombiniert Geschwindigkeit, Warteschlangenlänge und mittlere Wartezeit in einem additiven Reward.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
def realworld_reward(traffic_signal):
    # Speed (0–7.5 m/s -> 0–1)
    avg_speed = traffic_signal.get_average_speed()
    speed_term = min(max(avg_speed, 0.0), 7.5) / 7.5

    # Queue (0–20 Fzg -> 0–1)
    total_queue = traffic_signal.get_total_queued()
    queue_term = min(max(total_queue, 0), 20) / 20.0

    # Mean waiting time (0–10 s -> 0–1)
    waits_per_lane = traffic_signal.get_accumulated_waiting_time_per_lane()
    mean_wait = sum(waits_per_lane) / len(waits_per_lane) if waits_per_lane else 0.0
    wait_term = min(max(mean_wait, 0.0), 20.0) / 10.0

    reward = speed_term - queue_term - wait_term
    return reward
\end{minted}

\subsection{\texttt{emissions}}
\label{app:reward_emissions}
Diese Variante erweitert den Reward zusätzlich um einen Term für die CO\textsubscript{2}-Emissionen, sodass sowohl Verkehrsfluss als auch Nachhaltigkeit berücksichtigt werden.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
def emissions_reward(traffic_signal):
    env = getattr(traffic_signal, "env", None)
    if env is None or getattr(env, "sumo", None) is None:
        return 0.0
    if float(env.sim_step) >= float(env.sim_max_time):
        return 0.0

    sumo = env.sumo

    # Speed (0–7.5 m/s → 0..1)
    avg_speed = traffic_signal.get_average_speed()
    speed_term = min(max(avg_speed, 0.0), 7.5) / 7.5

    # Queue (0–20 → 0..1)
    total_queue = traffic_signal.get_total_queued()
    queue_term = min(max(total_queue, 0), 20) / 20.0

    # Wait (0–10 s → 0..1)
    waits = traffic_signal.get_accumulated_waiting_time_per_lane()
    mean_wait = (sum(waits) / len(waits)) if waits else 0.0
    wait_term = min(max(mean_wait, 0.0), 10.0) / 10.0

    # Emissionen
    try:
        lanes = getattr(traffic_signal, "lanes", [])
        total_co2 = sum(sumo.lane.getCO2Emission(lane) for lane in lanes)
        n_veh = sum(sumo.lane.getLastStepVehicleNumber(lane) for lane in lanes)
        BASELINE = 300.0 * max(1, len(lanes))
        CAP      = 2000.0 * max(1, len(lanes))
        co2_term = max(0.0, min(total_co2 - BASELINE, CAP - BASELINE)) / (CAP - BASELINE)
    except Exception:
        co2_term = 0.0

    reward = speed_term - queue_term - wait_term - co2_term
    return reward
\end{minted}

\section{Evaluierungs-Skripte}

\subsection{\texttt{evaluate.py} – Evaluationsskript für PPO-Modelle und Baselines}
\label{app:evaluate_script}
Dieses Skript führt die Evaluation aller trainierten PPO-Modelle in SUMO durch und vergleicht sie mit den Baselines \emph{Fixed-Time} und \emph{Actuated}.
Es rollt mehrere Episoden über verschiedene Seeds aus, extrahiert Metriken (z.\,B. Wartezeit, Geschwindigkeit, Emissionen) und speichert die Ergebnisse als \texttt{eval\_results.json}.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import os, json, numpy as np
import glob
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize, VecMonitor
from stable_baselines3.common.logger import configure
from sumo_rl.environment.env import parallel_env
from supersuit import pad_observations_v0, pad_action_space_v0
from supersuit import pettingzoo_env_to_vec_env_v1, concat_vec_envs_v1

# ----- Config -----
RUNS = sorted(glob.glob(os.path.join("runs", "ppo_sumo_*")))
MODEL_NAME  = "best_model.zip"
N_EPISODES  = 10
EP_LENGTH_S = 4096
EP_SEEDS    = [12345, 67890, 13579, 24680, 11223, 44556, 77889, 99100, 31415, 27182]
SCENARIOS   = [
    {"name": "morning_peak", "route_file": "flows_morning.rou.xml"},
    {"name": "evening_peak", "route_file": "flows_evening.rou.xml"},
    {"name": "uniform",      "route_file": "flows_uniform.rou.xml"},
    {"name": "random_heavy", "route_file": "flows_random_heavy.rou.xml"},
]

# ----- Env Factory -----
def make_env(route_file, sumo_seed):
    print(f"[DEBUG] Creating SUMO env with route={route_file}, seed={sumo_seed}")
    env = parallel_env(
        net_file="map.net.xml",
        route_file=route_file,
        use_gui=False,
        num_seconds=EP_LENGTH_S,
        reward_fn=dummy_reward,
        min_green=5,
        max_depart_delay=100,
        sumo_seed=sumo_seed,
        add_system_info=True,
        add_per_agent_info=False,
    )
    env = pad_observations_v0(env)
    env = pad_action_space_v0(env)
    env = pettingzoo_env_to_vec_env_v1(env)
    env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class="stable_baselines3")
    env = VecMonitor(env)
    return env

# ----- Model Loader -----
def load_model_and_norm(env, run_dir):
    vecnorm_path = os.path.join(run_dir, "vecnormalize.pkl")
    model_path   = os.path.join(run_dir, MODEL_NAME)

    #print(f"[DEBUG] Loading VecNormalize from {vecnorm_path}")
    env = VecNormalize.load(vecnorm_path, env)
    env.training = False
    env.norm_reward = False

    #print(f"[DEBUG] Loading PPO model from {model_path}")
    model = PPO.load(model_path, env=env, device="cpu")
    return model, env

# ----- Rollout -----
def rollout(model, env):
    obs = env.reset()
    dones = [False]

    sums = {}
    counts = {}
    last_totals = {}

    while True:
        action, _ = model.predict(obs, deterministic=True)
        obs, rewards, dones, infos = env.step(action)

        info = infos[0] if isinstance(infos, list) else infos
        if not isinstance(info, dict):
            info = {}

        # Wenn Episode zu Ende ist:
        if dones[0]:
            # Falls vorhanden, final_info/terminal_info verwenden
            fin = info.get("final_info") or info.get("terminal_info")
            if isinstance(fin, dict):
                # Mittelwerte vom finalen Step noch einrechnen
                for k, v in fin.items():
                    if k.startswith("system_mean_") and isinstance(v, (int, float)) and np.isfinite(v):
                        sums[k] = sums.get(k, 0.0) + float(v)
                        counts[k] = counts.get(k, 0) + 1
                # Totals aus final_info (echte Endstände)
                for k, v in fin.items():
                    if k.startswith("system_total_") and isinstance(v, (int, float)) and np.isfinite(v):
                        last_totals[k] = float(v)
            break

        # Normaler Zwischenschritt: Mittelwerte sammeln + Totals „letzten gültigen“ merken
        for k, v in info.items():
            if not isinstance(v, (int, float)) or not np.isfinite(v):
                continue
            if k.startswith("system_mean_") or k in ["system_total_waiting_time", "system_total_stopped", "system_total_running"]:
                # momentane Werte mitteln
                sums[k] = sums.get(k, 0.0) + float(v)
                counts[k] = counts.get(k, 0) + 1
            elif k.startswith("system_total_"):
                # Totals: nur letzten Wert merken
                last_totals[k] = float(v)

    mean_metrics = {k: (sums[k] / max(1, counts.get(k, 0))) for k in sums}
    mean_metrics.update(last_totals)
    return mean_metrics


def shorten_key(orig_key: str) -> str:
    return orig_key.replace("system_", "")

# ----- Env Factory für Baselines -----
def make_env_baseline(route_file, sumo_seed, fixed_time=True):
    """
    Erstellt eine SUMO-Umgebung, die den internen Controller verwendet.
    fixed_time=True  -> Fester Phasenplan aus net.xml
    fixed_time=False -> SUMO Actuated Control (falls in net.xml konfiguriert)
    """
    env = parallel_env(
        net_file="map.net.xml",
        route_file=route_file,
        use_gui=False,
        num_seconds=EP_LENGTH_S,
        reward_fn=dummy_reward,            # Kein RL-Reward
        fixed_ts=fixed_time,       # True = fixed, False = actuated
        sumo_seed=sumo_seed,
        add_system_info=True,
        add_per_agent_info=False,
    )
    env = pad_observations_v0(env)
    env = pad_action_space_v0(env)
    env = pettingzoo_env_to_vec_env_v1(env)
    env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class="stable_baselines3")
    env = VecMonitor(env)
    return env

def dummy_reward(_ts):
    return 0.0

def rollout_baseline(env):
    obs = env.reset()
    dones = [False]

    # Mittelwerte über die Episode
    sums = {}
    counts = {}
    # Letzte gültige Totals (vor Reset)
    last_totals = {}

    # gültige Dummy-Aktion aus dem Action Space
    dummy_action = np.array([env.action_space.sample() for _ in range(env.num_envs)])

    while True:
        obs, rewards, dones, infos = env.step(dummy_action)

        info = infos[0] if isinstance(infos, list) else infos
        if not isinstance(info, dict):
            info = {}

        # Wenn Episode zu Ende ist:
        if dones[0]:
            # Falls vorhanden, final_info/terminal_info verwenden
            fin = info.get("final_info") or info.get("terminal_info")
            if isinstance(fin, dict):
                # Mittelwerte vom finalen Step noch einrechnen
                for k, v in fin.items():
                    if k.startswith("system_mean_") and isinstance(v, (int, float)) and np.isfinite(v):
                        sums[k] = sums.get(k, 0.0) + float(v)
                        counts[k] = counts.get(k, 0) + 1
                # Totals aus final_info (echte Endstände)
                for k, v in fin.items():
                    if k.startswith("system_total_") and isinstance(v, (int, float)) and np.isfinite(v):
                        last_totals[k] = float(v)
            break

        # Normaler Zwischenschritt: Mittelwerte sammeln + Totals „letzten gültigen“ merken
        for k, v in info.items():
            if not isinstance(v, (int, float)) or not np.isfinite(v):
                continue
            if k.startswith("system_mean_") or k in ["system_total_waiting_time", "system_total_stopped", "system_total_running"]:
                # momentane Werte mitteln
                sums[k] = sums.get(k, 0.0) + float(v)
                counts[k] = counts.get(k, 0) + 1
            elif k.startswith("system_total_"):
                # Totals: nur letzten Wert merken
                last_totals[k] = float(v)

    # Mittelwerte berechnen
    mean_metrics = {k: (sums[k] / max(1, counts.get(k, 0))) for k in sums}
    # Letzte gültige Totals übernehmen
    mean_metrics.update(last_totals)

    return mean_metrics


def to_serializable(obj):
    if isinstance(obj, (np.integer,)):
        return int(obj)
    elif isinstance(obj, (np.floating,)):
        return float(obj)
    elif isinstance(obj, (np.ndarray,)):
        return obj.tolist()
    return str(obj)

# ----- Evaluation Loop -----
# ----- Evaluation Loop -----
def evaluate():
    results = []
    log_dir_root = os.path.join("evaluation", "logs")

    # Zählung: 2 Baselines + len(RUNS) RL pro (scenario × episode)
    total_episodes = (2 + len(RUNS)) * len(SCENARIOS) * N_EPISODES
    ep_counter = 0

    for sc in SCENARIOS:
        scen_log_dir = os.path.join(log_dir_root, f"eval_{sc['name']}")
        os.makedirs(scen_log_dir, exist_ok=True)
        logger = configure(scen_log_dir, ["tensorboard", "stdout"])

        print(f"[INFO] Evaluating scenario={sc['name']}")

        for ep in range(N_EPISODES):
            ep_seed = EP_SEEDS[ep]

            # --- 1) Fixed-Time ---
            env = make_env_baseline(sc["route_file"], sumo_seed=ep_seed, fixed_time=True)
            ep_counter += 1
            print(f"[PROGRESS] FixedTime | {sc['name']} | Ep {ep+1}/{N_EPISODES} "
                  f"({ep_counter}/{total_episodes})")
            m = rollout_baseline(env)
            m.update({
                "scenario": sc["name"],
                "episode": ep,
                "method": "Baseline_FixedTime"
            })
            results.append(m)

            # --- 2) Actuated ---
            env = make_env_baseline(sc["route_file"], sumo_seed=ep_seed, fixed_time=False)
            ep_counter += 1
            print(f"[PROGRESS] Actuated | {sc['name']} | Ep {ep+1}/{N_EPISODES} "
                  f"({ep_counter}/{total_episodes})")
            m = rollout_baseline(env)
            m.update({
                "scenario": sc["name"],
                "episode": ep,
                "method": "Baseline_Actuated"
            })
            results.append(m)

            # --- 3) RL-Modelle ---
            for run_dir in RUNS:
                env_raw = make_env(sc["route_file"], sumo_seed=ep_seed)
                model, env = load_model_and_norm(env_raw, run_dir)
                ep_counter += 1
                model_name = os.path.basename(run_dir)
                print(f"[PROGRESS] RL | {sc['name']} | {model_name} "
                      f"| Ep {ep+1}/{N_EPISODES} ({ep_counter}/{total_episodes})")
                m = rollout(model, env)
                
                # Seed extrahieren (3. Teil vom Namen)
                parts = model_name.split("_")
                model_seed = parts[2] if len(parts) > 2 else "unknown"

                m.update({
                    "scenario": sc["name"],
                    "episode": ep,
                    "method": f"{model_name}_{model_seed}"
                })
                results.append(m)

            # --- Logging dieser Episode (Baselines + alle RL) ---
            for entry in results[-(2 + len(RUNS)):]:
                for k, v in entry.items():
                    if isinstance(v, (int, float)) and k not in ["episode", "ep_seed"]:
                        short_key = shorten_key(k)
                        logger.record(f"{entry['method']}/{short_key}", v)
            logger.dump(step=ep)

    results_path = os.path.join("evaluation", "eval_results.json")
    os.makedirs(os.path.dirname(results_path), exist_ok=True)
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2, default=to_serializable)

    print(f"[INFO] Evaluation abgeschlossen. Ergebnisse: {results_path}")

if __name__ == "__main__":
    evaluate()

\end{minted}

\section{Postprocessing der Evaluationsergebnisse}

\subsection{\texttt{json2csv.py} – Konvertierung und Aggregation von Evaluationsergebnissen}
\label{app:json2csv_script}
Dieses Skript verarbeitet die von \texttt{evaluate.py} erzeugte JSON-Datei \texttt{eval\_results.json}.
Es wandelt die Rohdaten zunächst in ein CSV-Format um, berechnet anschließend Mittelwerte und Standardabweichungen pro \emph{Scenario × Methode} und erzeugt sowohl eine aggregierte Gesamttabelle als auch separate CSV-Dateien pro Methode.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import json
import pandas as pd
import os

# Pfade
json_path = "evaluation/eval_results.json"
raw_csv_path = "evaluation/eval_results_raw.csv"
agg_csv_path = "evaluation/eval_results_agg.csv"

# -----------------------------
# Schritt 1: JSON -> Raw CSV
# -----------------------------
print(f"Lese JSON-Datei: {json_path}")
with open(json_path, "r") as f:
    data = json.load(f)

df = pd.DataFrame(data)
df.to_csv(raw_csv_path, index=False)
print(f"Raw CSV geschrieben: {raw_csv_path}")

# -----------------------------
# Schritt 2: Aggregation
# -----------------------------
# numerische Spalten automatisch finden (alles außer scenario, method, episode)
numeric_cols = df.select_dtypes(include="number").columns.tolist()
numeric_cols = [c for c in numeric_cols if c not in ["episode"]]  # episode nicht mitteln

# Aggregationsdict
agg_dict = {}
for col in numeric_cols:
    agg_dict[f"{col}_mean"] = (col, "mean")
    agg_dict[f"{col}_std"] = (col, "std")

# Gruppieren nach Szenario + Methode
agg = df.groupby(["scenario", "method"]).agg(**agg_dict).reset_index()

# Gesamte Aggregation speichern
agg.to_csv(agg_csv_path, index=False)
print(f"Aggregierte Datei geschrieben: {agg_csv_path}")
print("Zeilen:", len(agg))

# -----------------------------
# Schritt 3: Pro-Methode CSVs
# -----------------------------
for method, df_method in agg.groupby("method"):
    safe_name = method.replace(" ", "_").replace("/", "_")
    out_path = f"evaluation/{safe_name}.csv"
    df_method.to_csv(out_path, index=False)
    print(f"Datei für Methode '{method}' geschrieben: {out_path}")

\end{minted}

\section{Netzwerk-Skripte}

\subsection{\texttt{check\_tls\_consistency.py} – Prüfung inkonsistenter Phasenlängen}
\label{app:check_tls_consistency}
Dieses Tool analysiert alle TLS im SUMO-Netz und prüft, ob die Länge des \texttt{state}-Strings mit der Anzahl der kontrollierten Verbindungen übereinstimmt.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

# === Konfiguration ===
net_file = "map.net.xml"

# === Einlesen ===
tree = ET.parse(net_file)
root = tree.getroot()

# === Alle controlledLinks zählen ===
tls_controlled_links = {}
for connection in root.findall("connection"):
    if "tl" in connection.attrib and "linkIndex" in connection.attrib:
        tls_id = connection.attrib["tl"]
        tls_controlled_links.setdefault(tls_id, set()).add(int(connection.attrib["linkIndex"]))

# === Alle Phasen prüfen ===
def check_tls_lengths():
    print("Überprüfe alle TLS auf inkonsistente Phasenlängen...\n")
    any_issues = False
    for logic in root.findall("tlLogic"):
        tls_id = logic.attrib["id"]
        expected_len = len(tls_controlled_links.get(tls_id, []))

        if expected_len == 0:
            print(f" TLS '{tls_id}' hat keine controlledLinks (wird evtl. nicht gesteuert)")
            continue

        for i, phase in enumerate(logic.findall("phase")):
            actual_len = len(phase.attrib["state"])
            if actual_len != expected_len:
                print(f" Phase {i} von TLS '{tls_id}' hat Länge {actual_len}, erwartet: {expected_len}")
                print(f"    → state=\"{phase.attrib['state']}\"")
                any_issues = True

    if not any_issues:
        print(" Alle TLS-Phasen stimmen mit ihren controlledLinks überein!")

check_tls_lengths()
\end{minted}

\subsection{\texttt{check\_tls\_requests.py} – Prüfung ungültiger \texttt{<request>}-Indizes}
\label{app:check_tls_requests}
Prüft, ob alle \texttt{request}-Indizes innerhalb der zulässigen Grenzen liegen, um Laufzeitfehler in \texttt{sumo-rl} zu vermeiden.

\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

net_file = "map.net.xml"
tree = ET.parse(net_file)
root = tree.getroot()

# Zähle für jedes TLS wie viele signal indices es gibt (controlled links)
tls_signal_indices = {}
for conn in root.findall("connection"):
    if "tl" in conn.attrib and "linkIndex" in conn.attrib:
        tls_id = conn.attrib["tl"]
        tls_signal_indices.setdefault(tls_id, set()).add(int(conn.attrib["linkIndex"]))

# Vergleiche mit den request-Elementen
print("Überprüfe request-Indizes gegen Signalindizes...\n")
any_issues = False
for junction in root.findall("junction"):
    tls_id = junction.attrib.get("id")
    requests = junction.findall("request")
    if tls_id in tls_signal_indices:
        expected_max = len(tls_signal_indices[tls_id])
        for req in requests:
            index = int(req.attrib["index"])
            if index >= expected_max:
                print(f"Junction '{tls_id}': request index {index} > max signal index {expected_max - 1}")
                any_issues = True

if not any_issues:
    print("Alle request-Indizes passen zu den TLS-Signalindizes!")
\end{minted}

\subsection{\texttt{fix\_requests.py} – Automatische Korrektur von Requests und Phasen}
\label{app:fix_requests}
Dieses Skript bereinigt überzählige \texttt{<request>}-Einträge und passt \texttt{state}-Strings in den Phasenlängen an.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

net_file = "map.net.xml"
output_file = "map_fixed_tls.net.xml"

tree = ET.parse(net_file)
root = tree.getroot()

# Finde maximal verwendete Signal-Indices pro TLS
tls_max_index = {}
for conn in root.findall("connection"):
    tl = conn.get("tl")
    idx = conn.get("linkIndex")
    if tl and idx:
        idx = int(idx)
        tls_max_index[tl] = max(tls_max_index.get(tl, -1), idx)

# Bereinigung
total_removed_requests = 0
total_adjusted_phases = 0
changed_tls = []

for junction in root.findall("junction"):
    tls_id = junction.get("id")
    if tls_id not in tls_max_index:
        continue

    max_idx = tls_max_index[tls_id]
    requests = list(junction.findall("request"))
    removed = 0

    for req in requests:
        req_idx = int(req.get("index"))
        if req_idx > max_idx:
            junction.remove(req)
            removed += 1

    if removed > 0:
        print(f"TLS '{tls_id}': {removed} ungültige <request>-Einträge entfernt.")
        total_removed_requests += removed
        changed_tls.append(tls_id)

    # Kürze zugehörige Phasen
    for tl in root.findall("tlLogic"):
        if tl.get("id") == tls_id:
            adjusted = 0
            for phase in tl.findall("phase"):
                state = phase.get("state")
                if len(state) > max_idx + 1:
                    old_len = len(state)
                    phase.set("state", state[:max_idx + 1])
                    adjusted += 1
            if adjusted > 0:
                print(f" TLS '{tls_id}': {adjusted} <phase>-Strings auf Länge {max_idx + 1} gekürzt.")
                total_adjusted_phases += adjusted
                if tls_id not in changed_tls:
                    changed_tls.append(tls_id)

# Speichern
tree.write(output_file, encoding="utf-8")
print("\n Reparatur abgeschlossen.")
print(f" Gesamt entfernte <request>-Einträge: {total_removed_requests}")
print(f" Gesamt angepasste <phase>-Einträge: {total_adjusted_phases}")
print(f" Betroffene TLS-IDs: {len(changed_tls)} Stück")
for tls in changed_tls:
    print(f"  - {tls}")
print(f"\n Bereinigte Datei gespeichert unter: {output_file}")
\end{minted}

\subsection{\texttt{repair\_net.py} – manuelle TLS-Reparatur auf Basis eines Referenz-Dictionaries}
\label{app:repair_net}
Repariert TLS-Definitionen durch Abgleich mit einer vordefinierten Mapping-Tabelle von korrekten Phasenlängen.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
from xml.etree import ElementTree as ET

# Manuell gepflegte Dictionary mit {TLS-ID: Anzahl controlledLinks}
controlled_links = {
    "1720933516": 6,
    "3538953167": 2,
    "3664415977": 10,
    "cluster_14795187_1720919996_2670370290_2670370291": 11,
    "cluster_14795804_55474925_6655074904_765746891_#1more": 49,
    "cluster_15431428_1719671850_1720917935": 20,
    "cluster_1590912233_3664415976_5083348337_5083348350": 11,
    "cluster_1692973685_1692973722_1718084055_1718084058_#11more": 36,
    "cluster_1729190097_3687504105": 8,
    "cluster_1744031943_5131521735": 10,
    "joinedS_1623835169_cluster_1137679587_1626739216_1728272870_1728272909_#17more": 33,
    "joinedS_309108716_cluster_11001804363_1125509937_12515596172_1784859792_#5more": 14,
    "joinedS_5092985445_cluster_1590912226_2911376263": 10,
    # ggf. mehr hinzufügen
}

tree = ET.parse("map.net.xml")
root = tree.getroot()
changed = False

for logic in root.findall("tlLogic"):
    tl_id = logic.attrib["id"]
    if tl_id not in controlled_links:
        continue

    correct_len = controlled_links[tl_id]
    for phase in logic.findall("phase"):
        state = phase.attrib["state"]
        if len(state) != correct_len:
            new_state = state[:correct_len].ljust(correct_len, 'r')
            print(f" Fixing {tl_id}: {len(state)} → {correct_len}")
            phase.attrib["state"] = new_state
            changed = True

if changed:
    tree.write("karlsruhe_fixed.net.xml")
    print(" Bereinigte Datei gespeichert: karlsruhe_fixed.net.xml")
else:
    print(" Alle Phasen bereits korrekt.")

\end{minted}

\subsection{\texttt{statecheck.py} – Prüfung auf Ziel-Phasenlänge}
\label{app:statecheck}
Hilft bei der Kontrolle einheitlicher Phasenlängen über das gesamte Netz hinweg (z.\,B. Zielwert = 57).
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
from xml.etree import ElementTree as ET

tree = ET.parse("map.net.xml")
root = tree.getroot()

for logic in root.findall("tlLogic"):
    tl_id = logic.attrib["id"]
    for i, phase in enumerate(logic.findall("phase")):
        state = phase.attrib["state"]
        if len(state) != 57:
            print(f" Phase {i} of TLS '{tl_id}' has length {len(state)}")
\end{minted}

\subsection{\texttt{find\_valid\_tls.py} – Validierung lauffähiger TLS für SUMO-RL}
\label{app:find_valid_tls}
Startet für jede einzelne Ampelkreuzung eine Minimalumgebung und überprüft, ob diese in \texttt{sumo-rl} trainierbar ist.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
from sumo_rl import SumoEnvironment
import traci
import os

def test_tls(tls_id):
    try:
        env = SumoEnvironment(
            net_file="map.net.xml",
            route_file="map.rou.xml",
            use_gui=False,
            single_agent=True
        )
        env.ts_ids = [tls_id]
        env.reset()
        env.close()
        return True
    except Exception as e:
        print(f" TLS {tls_id} nicht gültig: {e}")
        return False

# Alle TLS holen
try:
    env = SumoEnvironment(
        net_file="map.net.xml",
        route_file="map.rou.xml",
        use_gui=False,
        single_agent=True
    )
    all_tls = env.ts_ids
    env.close()
except Exception as e:
    print(" Konnte TLS nicht auslesen:", e)
    all_tls = []

print(f" Teste {len(all_tls)} TLS auf Gültigkeit...\n")
valid_tls = []

for tls_id in all_tls:
    if test_tls(tls_id):
        valid_tls.append(tls_id)

print("\n Gültige TLS:")
print(valid_tls)
\end{minted}


\subsection{\texttt{find\_relevant\_edges.py} – Suche alle relevanten edges}
\label{app:find_relevant_edges}
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

# Konfiguration: Pfad zur .net.xml-Datei und Suchbegriffe
NET_FILE = "network.net.xml"
SUCHBEGRIFFE = ["B10", "B36", "L605", "Durlacher Allee", "Reinhold-Frank-Straße"]

# Ausgabe-Datei für gefundene Kanten
OUTPUT_FILE = "edges.txt"

def finde_relevante_kanten(net_file, suchbegriffe):
    tree = ET.parse(net_file)
    root = tree.getroot()
    
    relevante_kanten = []
    
    for edge in root.findall("edge"):
        name = edge.get("name")
        if name:
            for begriff in suchbegriffe:
                if begriff.lower() in name.lower():
                    relevante_kanten.append((edge.get("id"), name))
                    break  # nicht doppelt eintragen, falls mehrere Begriffe passen
                    
    return relevante_kanten

if __name__ == "__main__":
    kanten = finde_relevante_kanten(NET_FILE, SUCHBEGRIFFE)
    
    # Ergebnisse speichern
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for edge_id, name in kanten:
            f.write(f"{edge_id}\t{name}\n")
    
    print(f"{len(kanten)} relevante Kanten gefunden.")
    print(f"Ergebnisse in '{OUTPUT_FILE}' gespeichert.")

\end{minted}

\section{Sumo-Konfiguration}
\subsection{\texttt{sumoconfig\_.sumocfg}}
\label{app:sumocfg}
Die folgende Konfigurationsdatei definiert die zentralen Eingaben und
Parameter für die Simulation in SUMO. Sie verweist auf die zu ladende
Netzdatei und die zugehörigen Routendateien sowie auf den zu simulierenden
Zeitraum.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{xml}
<configuration>
  <input>
    <net-file value="network.net.xml"/>
    <route-files value="routes.xml"/>
  </input>
  <time>
    <begin value="0"/>
    <end value="5000"/>
  </time>
</configuration>
\end{minted}