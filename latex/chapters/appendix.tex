\appendix

Dieser Anhang enthält die vollständigen Python-Skripte, die zur Validierung, Reparatur und Steuerung der SUMO-basierten Reinforcement-Learning-Umgebung eingesetzt wurden. Jedes Unterkapitel dokumentiert ein spezifisches Tool oder Modul aus dem Projekt.

\section{Trainings-Skripte}

\subsection{\texttt{train.py} – Trainingsskript für PPO über mehrere Seeds}
\label{app:train_script}
Das folgende Skript enthält die vollständige Trainingslogik für das Reinforcement Learning mit \texttt{sumo-rl} unter Verwendung von \texttt{Stable-Baselines3}.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
# ====== Bibliotheken und Module ======
# Standard-Module für Dateiverwaltung, Zeit und Regex
import os
import re
import time
import datetime

# SUMO-Interface (TraCI) für Simulation
import traci

# Mathematische und numerische Berechnungen
import numpy as np

# PyTorch für neuronale Netze und Reproduzierbarkeit
import torch
from collections import defaultdict

# Stable-Baselines3 (RL-Algorithmen, hier PPO)
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CallbackList

# SUMO-RL-Umgebung (PettingZoo-kompatibel)
from sumo_rl.environment.env import parallel_env

# SuperSuit – Hilfsfunktionen, um PettingZoo-Umgebungen mit SB3 zu verwenden
from supersuit import (
    pad_observations_v0,          # Padding für Beobachtungen, um feste Größe zu garantieren
    pad_action_space_v0,          # Padding für Aktionsraum
    pettingzoo_env_to_vec_env_v1, # Konvertierung zu SB3-kompatiblem Vektor-Env
    concat_vec_envs_v1            # Mehrere Envs parallel laufen lassen
)

# Gymnasium für RL-Umgebungs-Schnittstellen
import gymnasium as gym
from gymnasium import Wrapper


# ====== Trainings-Setup ======
SEEDS = [546456, 678678, 234256, 678]  # Verschiedene Zufalls-Seed-Werte für reproduzierbare Runs

# ====== Gemeinsamer Step-Cache ======
_STEP_CACHE = {"step": None}

# ====== Reward-Funktion: Emissions-Variante ======
def emissions_with_speed_reward(ts):
    current_step = int(traci.simulation.getTime())
    if _STEP_CACHE.get("emis_step") != current_step:
        vids = traci.vehicle.getIDList()
        n_veh = len(vids)

        if n_veh:
            total_emis = sum(traci.vehicle.getCO2Emission(v) for v in vids)
            total_speed = sum(traci.vehicle.getSpeed(v) for v in vids)
            flow = traci.simulation.getArrivedNumber()
            queue = ts.get_total_queued()
            mean_emis = total_emis / n_veh
            mean_speed = total_speed / n_veh
        else:
            mean_emis = mean_speed = flow = queue = 0.0

        _STEP_CACHE.update({
            "emis_step": current_step,
            "mean_emis": mean_emis,
            "mean_speed": mean_speed,
            "flow": flow,
            "queue": queue,
        })

    if not hasattr(ts, "_emis_state"):
        ts._emis_state = {"ema_emis": 0.0, "ema_speed": 0.0, "ema_flow": 0.0}

    alpha = 0.3
    ts._emis_state["ema_emis"]  = (1 - alpha) * ts._emis_state["ema_emis"]  + alpha * _STEP_CACHE["mean_emis"]
    ts._emis_state["ema_speed"] = (1 - alpha) * ts._emis_state["ema_speed"] + alpha * _STEP_CACHE["mean_speed"]
    ts._emis_state["ema_flow"]  = (1 - alpha) * ts._emis_state["ema_flow"]  + alpha * _STEP_CACHE["flow"]

    emis_term  = -np.tanh(ts._emis_state["ema_emis"] / 2000.0)
    speed_term =  np.tanh(ts._emis_state["ema_speed"] / 15.0)
    flow_term  =  np.tanh(ts._emis_state["ema_flow"] / 5.0)
    queue_term = -np.tanh(_STEP_CACHE["queue"] / 30.0)

    return float(np.clip(
        0.4*emis_term + 0.3*speed_term + 0.2*flow_term + 0.1*queue_term,
        -1.0, 1.0
    ))


# ====== Reward-Funktion: Reisezeit-Variante ======
def travel_time_reward(ts):
    current_step = int(traci.simulation.getTime())
    if _STEP_CACHE.get("travel_step") != current_step:
        vids = traci.vehicle.getIDList()
        n_veh = len(vids)
        if n_veh:
            travel_times = [
                traci.vehicle.getAccumulatedWaitingTime(v) + traci.vehicle.getTimeLoss(v)
                for v in vids
            ]
            mean_time = float(np.mean(travel_times))
        else:
            mean_time = 0.0
        _STEP_CACHE.update({
            "travel_step": current_step,
            "mean_travel_time": mean_time,
            "n_veh": n_veh
        })

    if _STEP_CACHE["n_veh"] <= 0:
        return 0.0

    return float(np.clip(np.tanh((60.0 - _STEP_CACHE["mean_travel_time"]) / 60.0), -1.0, 1.0))


# ====== Reward-Funktion: RealWorld-Variante ======
def realworld_reward(ts):
    current_step = int(traci.simulation.getTime())
    if _STEP_CACHE.get("rw_step") != current_step:
        _STEP_CACHE["queue"] = ts.get_total_queued()
        _STEP_CACHE["flow"] = traci.simulation.getArrivedNumber()
        _STEP_CACHE["rw_step"] = current_step

    # Reset bei neuem Episode
    if getattr(ts, "step_count", 0) == 0:
        ts._rw_state = None

    q = _STEP_CACHE["queue"]
    f = _STEP_CACHE["flow"]

    if not hasattr(ts, "_rw_state") or ts._rw_state is None:
        ts._rw_state = {"prev_q": q, "ema_q": float(q), "ema_f": float(f)}
        return 0.0

    # Konstanten
    ema = 0.3
    clip = 5.0
    max_storage = 40.0
    max_outflow_per_step = 8.0
    w_q, w_build, w_flow, w_switch = 1.0, 0.8, 0.7, 0.1

    # Phase-Change (direkter Zugriff, falls Attribut immer existiert)
    phase_sw = 1.0 if getattr(ts, "phase_changed", False) else 0.0

    st = ts._rw_state
    ema_q = (1 - ema) * st["ema_q"] + ema * q
    ema_f = (1 - ema) * st["ema_f"] + ema * f

    build = max(0, q - st["prev_q"])

    # Normierungen (inline clamping statt np.clip)
    q_norm = ema_q / max_storage
    if q_norm < 0.0: q_norm = 0.0
    elif q_norm > 1.5: q_norm = 1.5

    b_norm = build / (max_storage * 0.2)
    if b_norm < 0.0: b_norm = 0.0
    elif b_norm > 1.5: b_norm = 1.5

    f_norm = ema_f / max_outflow_per_step
    if f_norm < 0.0: f_norm = 0.0
    elif f_norm > 1.5: f_norm = 1.5

    r = -w_q*q_norm - w_build*b_norm + w_flow*f_norm - w_switch*phase_sw

    # Clip inline
    if r < -clip: r = -clip
    elif r > clip: r = clip

    st["prev_q"], st["ema_q"], st["ema_f"] = q, ema_q, ema_f
    ts._rw_state = st
    return float(r)


# ====== Reward-Funktion: Custom-Variante ======
def custom_reward(ts):
    current_step = int(traci.simulation.getTime())
    if _STEP_CACHE.get("custom_step") != current_step:
        queue = ts.get_total_queued()
        wait_sum_total = float(np.sum(ts.get_accumulated_waiting_time_per_lane()))
        arrived = traci.simulation.getArrivedNumber()
        teleports = traci.simulation.getStartingTeleportNumber()
        collisions = traci.simulation.getCollidingVehiclesNumber()
        _STEP_CACHE.update({
            "custom_step": current_step,
            "queue": queue,
            "wait_sum_total": wait_sum_total,
            "arrived": arrived,
            "teleports": teleports,
            "collisions": collisions
        })

    if not hasattr(ts, "_prev_queue"):
        ts._prev_queue = 0
    if not hasattr(ts, "_prev_wait_sum"):
        ts._prev_wait_sum = 0.0

    queue = _STEP_CACHE["queue"]
    wait_sum_total = _STEP_CACHE["wait_sum_total"]
    arrived = _STEP_CACHE["arrived"]
    teleports = _STEP_CACHE["teleports"]
    collisions = _STEP_CACHE["collisions"]

    delta_queue = queue - ts._prev_queue
    delta_wait = max(0.0, wait_sum_total - ts._prev_wait_sum)
    ts._prev_queue = queue
    ts._prev_wait_sum = wait_sum_total

    q_term       = -np.tanh(queue / 30.0)
    dq_term      = -np.tanh(max(0, delta_queue) / 10.0)
    wait_term    = -np.tanh(delta_wait / 60.0)
    arrived_term =  np.tanh(arrived / 5.0)
    tp_term      = -np.tanh(teleports / 1.0)
    col_term     = -np.tanh(collisions / 1.0)

    raw = (
        0.35*q_term +
        0.20*dq_term +
        0.45*wait_term +
        0.40*arrived_term +
        0.90*tp_term +
        1.20*col_term
    )

    if teleports > 0:
        raw -= 0.5
    if collisions > 0:
        raw -= 0.8

    return float(np.tanh(raw))


# ====== Reward-Funktion: Time-Emission-Variante ======
def travel_time_emissions_reward(ts, w_emis=0.2):
    """
    Einfacher Mix aus Reisezeit-Reward und Emissions-Reward.
    - w_emis: Gewicht der Emissionen (0 = ignorieren, 1 = nur Emissionen)
    """
    current_step = int(traci.simulation.getTime())
    if _STEP_CACHE.get("tte_step") != current_step:
        vids = traci.vehicle.getIDList()
        n_veh = len(vids)

        if n_veh:
            total_time = 0.0
            total_emis = 0.0
            for v in vids:
                total_time += traci.vehicle.getAccumulatedWaitingTime(v)
                total_time += traci.vehicle.getTimeLoss(v)
                total_emis += traci.vehicle.getCO2Emission(v)

            mean_time = total_time / n_veh
            mean_emis = total_emis / n_veh
        else:
            mean_time = 0.0
            mean_emis = 0.0

        _STEP_CACHE.update({
            "tte_step": current_step,
            "tte_n_veh": n_veh,
            "tte_mean_time": mean_time,
            "tte_mean_emis": mean_emis,
        })

    if _STEP_CACHE["tte_n_veh"] <= 0:
        return 0.0

    # Reisezeit-Reward
    r_time = np.tanh((60.0 - _STEP_CACHE["tte_mean_time"]) / 60.0)

    # Emissions-Reward: einfach skalieren, niedriger ist besser
    r_emis = -np.tanh(_STEP_CACHE["tte_mean_emis"] / 2000.0)

    # Kombinieren
    return float((1 - w_emis) * r_time + w_emis * r_emis)


# ====== Schedules für Hyperparameter-Anpassung ======
# (Funktionen, die während des Trainings den Wert z. B. von Lernrate oder Clip-Bereich dynamisch anpassen)
def adaptive_entropy_schedule(start=0.01):
    return lambda progress: max(0.001, start * (1 - progress))

def dynamic_clip_range(start=0.2, end=0.1):
    return lambda pr: end + (start - end) * pr

def cosine_clip(start=0.2, end=0.1):
    return lambda pr: end + (start - end) * 0.5 * (1 + np.cos(np.pi * (1 - pr)))

def linear_schedule(start):
    return lambda progress: start * (1 - progress)

def cosine_warmup_floor(start=3e-4, warmup_frac=0.05, min_lr_frac=0.1):
    """
    Lernrate: Erst linear hochfahren (Warmup), dann mit Cosinus auf Minimalwert absenken.
    """
    min_lr = start * min_lr_frac
    warmup_frac = max(0.0, min(0.5, warmup_frac))
    def schedule(progress_remaining: float) -> float:
        t = 1.0 - progress_remaining
        if t < warmup_frac:
            base = start * 0.1 + (start - start * 0.1) * (t / warmup_frac)
        else:
            tt = (t - warmup_frac) / max(1e-8, (1.0 - warmup_frac))
            cos_term = 0.5 * (1 + np.cos(np.pi * tt))
            base = min_lr + (start - min_lr) * cos_term
        return float(base)
    return schedule


# ====== Hilfsfunktionen und Callbacks ======
# (Modelle finden, Checkpoints speichern, Metriken loggen, bestes Modell sichern)
# ====== Letzten vollständigen Run finden ======
def find_latest_complete_run(base_dir="runs", prefix="ppo_sumo_"):
    """
    Sucht im 'runs'-Ordner nach dem neuesten Trainingslauf, der
    - eine gespeicherte VecNormalize-Instanz hat
    - und entweder ein finales Modell oder mindestens einen Checkpoint.
    Gibt die Pfade zu Run-Ordner, Modell und Normalisierungsdatei zurück.
    """
    subdirs = sorted(
        [d for d in os.listdir(base_dir) if d.startswith(prefix)],
        reverse=True
    )
    for d in subdirs:
        dir_path = os.path.join(base_dir, d)
        norm_path = os.path.join(dir_path, "vecnormalize.pkl")
        if not os.path.exists(norm_path):
            continue

        # Prüfe auf finales Modell
        final_model = os.path.join(dir_path, "model.zip")
        if os.path.exists(final_model):
            return dir_path, final_model, norm_path

        # Falls kein finales Modell: Prüfe auf Checkpoints
        checkpoint_models = [
            f for f in os.listdir(dir_path)
            if re.match(r"ppo_sumo_model_(\d+)_steps\.zip", f)
        ]
        if checkpoint_models:
            checkpoint_models.sort(key=lambda x: int(re.findall(r"\d+", x)[0]), reverse=True)
            best_checkpoint = checkpoint_models[0]
            return dir_path, os.path.join(dir_path, best_checkpoint), norm_path

    return None


# ====== Callback: Zeitbasiertes Speichern ======
class TimeBasedCheckpointCallback(BaseCallback):
    """
    Speichert Modell und Normalisierungsdaten in festen Zeitintervallen (Sekunden).
    """
    def __init__(self, save_interval_sec, save_path, name_prefix="ppo_sumo_model", verbose=0):
        super().__init__(verbose)
        self.save_interval_sec = save_interval_sec
        self.save_path = save_path
        self.name_prefix = name_prefix
        self.last_save_time = time.time()

    def _on_step(self) -> bool:
        return True  # Keine Aktion bei jedem einzelnen Step

    def _on_rollout_end(self) -> bool:
        # Am Ende eines Rollouts prüfen, ob das Zeitintervall abgelaufen ist
        current_time = time.time()
        if current_time - self.last_save_time >= self.save_interval_sec:
            timestep = self.num_timesteps
            filename = f"{self.name_prefix}_{timestep}_steps"
            self.model.save(os.path.join(self.save_path, filename + ".zip"))
            if hasattr(self.training_env, "save"):
                self.training_env.save(os.path.join(self.save_path, f"{filename}_vecnormalize.pkl"))
            print(f"[Checkpoint] Modell gespeichert bei {timestep} Schritten ({filename})")
            self.last_save_time = current_time
        return True

# ====== Callback: Metriken aus der Env loggen ======
class EnvMetricsLoggerCallback(BaseCallback):
    def __init__(self, prefix="env", verbose=0):
        super().__init__(verbose)
        self.prefix = prefix
        self.sums = defaultdict(float)
        self.counts = defaultdict(int)
        self.key_map = {}  # Cache für umbenannte Keys
        self._emi_sum = 0.0
        self._emi_count = 0
        self._last_logged_step = None

    def _on_rollout_start(self) -> None:
        self.sums.clear()
        self.counts.clear()
        self.key_map.clear()
        self._emi_sum = 0.0
        self._emi_count = 0
        self._last_logged_step = None

    def _on_step(self) -> bool:
        infos = self.locals.get("infos")
        if infos:
            for info in infos:
                if not isinstance(info, dict):
                    continue
                for orig_key, v in info.items():
                    # Schneller als np.isfinite()
                    if not isinstance(v, (int, float)) or v != v or v in (float("inf"), float("-inf")):
                        continue
                    mapped_key = self.key_map.get(orig_key)
                    if mapped_key is None:
                        if orig_key.startswith("system_"):
                            short_key = orig_key[len("system_"):]
                            if short_key.startswith("total"):
                                short_key = short_key[len("total_"):]
                            short_key = "mean_" + short_key
                        else:
                            short_key = orig_key
                        mapped_key = f"{self.prefix}/{short_key}"
                        self.key_map[orig_key] = mapped_key
                    self.sums[mapped_key] += float(v)
                    self.counts[mapped_key] += 1

        # Emissionslogging nur 1× pro Sim-Step
        try:
            current_step = int(traci.simulation.getTime())
            if self._last_logged_step != current_step:
                vids = traci.vehicle.getIDList()
                n_veh = len(vids)
                if n_veh:
                    total_emis = sum(traci.vehicle.getCO2Emission(vid) for vid in vids)
                    mean_emis = total_emis / n_veh
                else:
                    mean_emis = 0.0
                self._emi_sum += mean_emis
                self._emi_count += 1
                self._last_logged_step = current_step
        except Exception:
            pass

        return True

    def _on_rollout_end(self):
        for tag, total in self.sums.items():
            mean_val = total / max(1, self.counts[tag])
            self.logger.record(tag, mean_val)

        if self._emi_count > 0:
            self.logger.record(
                f"{self.prefix}/mean_emissions_per_vehicle",
                self._emi_sum / self._emi_count
            )


# ====== Callback: Bestes Modell speichern ======
class BestModelSaverCallback(BaseCallback):
    """
    Speichert das Modell mit dem bisher höchsten mittleren Episodenreward.
    """
    def __init__(self, save_path, verbose=0):
        super().__init__(verbose)
        self.best_mean_reward = -float('inf')
        self.save_path = save_path

    def _on_step(self) -> bool:
        return True

    def _on_rollout_end(self):
        ep_info_buffer = self.model.ep_info_buffer
        if len(ep_info_buffer) > 0:
            mean_rew = np.mean([ep_info['r'] for ep_info in ep_info_buffer])
            if mean_rew > self.best_mean_reward:
                self.best_mean_reward = mean_rew
                model_path = os.path.join(self.save_path, "best_model.zip")
                self.model.save(model_path)
                if hasattr(self.model.env, "save"):
                    norm_path = os.path.join(self.save_path, "best_model_vecnormalize.pkl")
                    self.model.env.save(norm_path)
                print(f"[AUTOLOG] Neuer Bestwert {mean_rew:.2f} → best_model gespeichert!", flush=True)


# ====== Haupt-Trainingsschleife ======
for SEED in SEEDS:
    # Reproduzierbarkeit sicherstellen
    np.random.seed(SEED)
    torch.manual_seed(SEED)

    # Log-Verzeichnis erstellen
    now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_dir = os.path.join("runs", f"ppo_sumo_{SEED}_{now}")
    os.makedirs(log_dir, exist_ok=True)

    print(f"\n[INFO] Starte Training mit Seed: {SEED}")

    # SUMO-Umgebung initialisieren
    env = parallel_env(
        net_file="map.net.xml",
        route_file="map.rou.xml",
        use_gui=False,            # Kein GUI (schnelleres Training)
        num_seconds=5000,         # Episodenlänge in Simulationssekunden
        reward_fn=emissions_with_speed_reward,  # Reward-Funktion
        min_green=5,              # Minimale Grünphase
        max_depart_delay=100,     # Max. Verzögerung bei Fahrzeugstart
        sumo_seed=SEED,           # Seed für SUMO
        add_system_info=True,     # Zusätzliche Systemmetriken
        add_per_agent_info=False, # Keine Metriken pro Agent (nur global)
    )

    # Falls die Env einen seed()-Aufruf unterstützt
    if hasattr(env, "seed"):
        env.seed(SEED)

    # Anpassung der Beobachtungen und Aktionen an SB3
    env = pad_observations_v0(env)
    env = pad_action_space_v0(env)
    env = pettingzoo_env_to_vec_env_v1(env)
    env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=8, base_class="stable_baselines3")

    # Logging und Normalisierung
    env = VecMonitor(env, filename=os.path.join(log_dir, "monitor.csv"))
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)

    # PPO-Agent erstellen
    model = PPO(
        policy="MlpPolicy",      # Mehrschicht-Perzeptron-Policy
        env=env,
        verbose=1,               # Ausführliches Logging
        tensorboard_log=log_dir, # TensorBoard-Pfad
        batch_size=512,          # Minibatch-Größe für PPO
        n_steps=2048,            # Rollout-Länge
        learning_rate=cosine_warmup_floor(start=3e-4, warmup_frac=0.05, min_lr_frac=0.1),
        clip_range=cosine_clip(), # Clipping-Range dynamisch
        ent_coef=0.01,            # Entropie-Koeffizient (Exploration)
        gamma=0.99,               # Diskontfaktor
        gae_lambda=0.95,          # Lambda für GAE
        device="cpu",             # Training auf CPU
        policy_kwargs=dict(net_arch=dict(pi=[128, 128], vf=[128, 128])), # Netzarchitektur
    )

    # Callback-Liste: Checkpoints, Logging, Best-Model-Speicherung
    callbacks = CallbackList([
        TimeBasedCheckpointCallback(
            save_interval_sec=3600, # Jede Stunde speichern
            save_path=log_dir,
            name_prefix="ppo_sumo_model",
            verbose=1,
        ),
        EnvMetricsLoggerCallback(),
        BestModelSaverCallback(save_path=log_dir),
    ])

    # Training starten
    try:
        time.sleep(3) # Kurze Pause für saubere Konsolenlogs
        model.learn(
            total_timesteps=2_000_000,
            callback=callbacks,
        )
        # Nach Abschluss final speichern
        model.save(os.path.join(log_dir, "model.zip"))
        env.save(os.path.join(log_dir, "vecnormalize.pkl"))
        print(f"\n[INFO] Training abgeschlossen für Seed {SEED}. Modell gespeichert unter: {log_dir}")

    # Falls Training manuell abgebrochen wird (Strg+C)
    except KeyboardInterrupt:
        print("[ABBRUCH] Manuelles Beenden erkannt. Speichere aktuellen Stand...")
        model.save(os.path.join(log_dir, "model_interrupt.zip"))
        env.save(os.path.join(log_dir, "vecnormalize_interrupt.pkl"))

    # Generelle Fehlerbehandlung
    except Exception as e:
        print(f"\n[FEHLER] Während des Trainings bei Seed {SEED} aufgetreten: {e}")

    # Cleanup: Env schließen und Normalisierungsdaten sichern
    finally:
        try:
            env.save(os.path.join(log_dir, "vecnormalize.pkl"))
        except Exception as e:
            print(f"[WARNUNG] VecNormalize konnte nicht gespeichert werden: {e}")
        env.close()
\end{minted}

\subsection{\texttt{continuetrain.py} – Trainingsskript zum Weitertrainieren}
\label{app:continuetrain}
Startet für jede einzelne Ampelkreuzung eine Minimalumgebung und überprüft, ob diese in \texttt{sumo-rl} trainierbar ist.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import os
import re
import time
import datetime
import traci
import numpy as np
import torch
import json
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
from sumo_rl.environment.env import parallel_env
from supersuit import (
    pad_observations_v0,
    pad_action_space_v0,
    pettingzoo_env_to_vec_env_v1,
    concat_vec_envs_v1
)
from gym import Wrapper

# ==== Seed setzen ====
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

# ==== Custom Reward Function ====
def custom_reward(traffic_signal):
    if not hasattr(traffic_signal, "prev_queue"):
        traffic_signal.prev_queue = traffic_signal.get_total_queued()

    queue = traffic_signal.get_total_queued()
    waiting = np.sum(traffic_signal.get_accumulated_waiting_time_per_lane())

    sim = traci.simulation
    arrived = sim.getArrivedNumber()
    teleport = sim.getStartingTeleportNumber()
    collisions = sim.getCollidingVehiclesNumber()

    delta_queue = traffic_signal.prev_queue - queue
    traffic_signal.prev_queue = queue

    reward = (
        -0.1 * queue
        - 0.05 * waiting
        - 2.0 * teleport
        - 10.0 * collisions
        + 1.0 * arrived
        + 0.3 * delta_queue
    )

    if teleport > 10 or collisions > 5:
        reward -= 20

    return np.clip(reward, -100, 100)

# ==== Adaptive Parameter-Schedules ====
def dynamic_clip_range(start=0.2):
    return lambda progress: max(0.1, start * (1 - 0.5 * progress))

def linear_schedule(start):
    return lambda progress: start * (1 - progress)

# ==== Finde letzten vollständigen Run ====
def find_latest_complete_run(base_dir="runs", prefix="ppo_sumo_"):
    subdirs = sorted(
        [d for d in os.listdir(base_dir) if d.startswith(prefix)],
        reverse=True
    )
    for d in subdirs:
        dir_path = os.path.join(base_dir, d)
        norm_path = os.path.join(dir_path, "vecnormalize.pkl")
        if not os.path.exists(norm_path):
            continue

        final_model = os.path.join(dir_path, "model.zip")
        if os.path.exists(final_model):
            return dir_path, final_model, norm_path

        checkpoint_models = [
            f for f in os.listdir(dir_path)
            if re.match(r"ppo_sumo_model_(\d+)_steps\.zip", f)
        ]
        if checkpoint_models:
            checkpoint_models.sort(key=lambda x: int(re.findall(r"\d+", x)[0]), reverse=True)
            best_checkpoint = checkpoint_models[0]
            return dir_path, os.path.join(dir_path, best_checkpoint), norm_path

    return None

# ==== Zeitbasierter Checkpoint Callback ====
class TimeBasedCheckpointCallback(BaseCallback):
    def __init__(self, save_interval_sec, save_path, name_prefix="ppo_sumo_model", verbose=0):
        super().__init__(verbose)
        self.save_interval_sec = save_interval_sec
        self.save_path = save_path
        self.name_prefix = name_prefix
        self.last_save_time = time.time()

    def _on_step(self) -> bool:
        return True
        
    def _on_rollout_end(self) -> bool:
        current_time = time.time()
        if current_time - self.last_save_time >= self.save_interval_sec:
            timestep = self.num_timesteps
            filename = f"{self.name_prefix}_{timestep}_steps"
            self.model.save(os.path.join(self.save_path, filename + ".zip"))
            if hasattr(self.training_env, "save"):
                self.training_env.save(os.path.join(self.save_path, f"{filename}_vecnormalize.pkl"))
            print(f"[Checkpoint] Modell gespeichert bei {timestep} Schritten ({filename})")
            self.last_save_time = current_time
        return True

# ==== Learning Rate Logger Callback ====
class LearningRateLoggerCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        lr = self.model.lr_schedule(self.num_timesteps / self.model._total_timesteps)
        self.logger.record("train/learning_rate", lr)
        return True

# ==== Logging ====
now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_dir = os.path.join("runs", f"ppo_sumo_{now}")
os.makedirs(log_dir, exist_ok=True)

# ==== SUMO-RL Umgebung ====
env = parallel_env(
    net_file="map.net.xml",
    route_file="map.rou.xml",
    use_gui=False,
    num_seconds=1000,
    reward_fn=custom_reward,
    min_green=5,
    max_depart_delay=100,
    sumo_seed=SEED,
    add_system_info=True,
    add_per_agent_info=True, 
)

if hasattr(env, "seed"):
    env.seed(SEED)

# ==== Wrapping ====
env = pad_observations_v0(env)
env = pad_action_space_v0(env)
env = pettingzoo_env_to_vec_env_v1(env)
env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=8, base_class="stable_baselines3")
env = VecMonitor(env)

# ==== Modell laden oder neu starten ====
result = find_latest_complete_run()
if result:
    latest_run_dir, model_path, normalize_path = result
    print("Fortsetzung wird gestartet mit:")
    print(f"Verzeichnis : {latest_run_dir}")
    print(f"Modell      : {model_path}")
    print(f"Normalize   : {normalize_path}\n")

    env = VecNormalize.load(normalize_path, env)
    env.training = True
    env.norm_reward = True

    model = PPO.load(model_path, env=env, tensorboard_log=log_dir, verbose=1, device="cpu")
    print(f"[INFO] Modell startet bei {model.num_timesteps} Timesteps.")
else:
    print("[INFO] Kein vorheriges Modell gefunden. Starte frisches Training.\n")
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)
    model = PPO(
        policy="MlpPolicy",
        env=env,
        verbose=1,
        tensorboard_log=log_dir,
        batch_size=2048,
        n_steps=2048,
        learning_rate=linear_schedule(3e-4),
        clip_range=dynamic_clip_range(0.2),
        ent_coef=0.005,
        gamma=0.99,
        gae_lambda=0.95,
        device="cpu",
        policy_kwargs=dict(net_arch=dict(pi=[128, 128], vf=[128, 128])),
    )

# ==== Automatisches Speichern bei verbessertem ep_rew_mean ====
class BestModelSaverCallback(BaseCallback):
    def __init__(self, save_path, verbose=0):
        super().__init__(verbose)
        self.best_mean_reward = -float('inf')
        self.save_path = save_path

    def _on_step(self) -> bool:
        # Muss vorhanden sein, selbst wenn sie nichts tut
        return True
        
    def _on_rollout_end(self):
        ep_info_buffer = self.model.ep_info_buffer
        if len(ep_info_buffer) > 0:
            mean_rew = np.mean([ep_info['r'] for ep_info in ep_info_buffer])
            
            if mean_rew > self.best_mean_reward:
                self.best_mean_reward = mean_rew

                model_path = os.path.join(self.save_path, "best_model.zip")
                self.model.save(model_path)

                if hasattr(self.model.env, "save"):
                    norm_path = os.path.join(self.save_path, "best_model_vecnormalize.pkl")
                    self.model.env.save(norm_path)

                print(f"[AUTOLOG] Neuer Bestwert {mean_rew:.2f} → best_model gespeichert!", flush=True)

# ==== Callbacks kombinieren ====
callbacks = CallbackList([
    TimeBasedCheckpointCallback(
        save_interval_sec=300,
        save_path=log_dir,
        name_prefix="ppo_sumo_model",
        verbose=1,
    ),
    LearningRateLoggerCallback(),
    BestModelSaverCallback(save_path=log_dir),
])

# ==== Training starten ====
try:
    model.learn(
        total_timesteps=1_000_000,
        callback=callbacks,
    )
    model.save(os.path.join(log_dir, "model.zip"))
    env.save(os.path.join(log_dir, "vecnormalize.pkl"))
    print(f"\n[INFO] Training abgeschlossen. Modell gespeichert unter: {log_dir}")

except KeyboardInterrupt:
    print("[ABBRUCH] Manuelles Beenden erkannt. Speichere aktuellen Stand...")
    model.save(os.path.join(log_dir, "model_interrupt.zip"))
    env.save(os.path.join(log_dir, "vecnormalize_interrupt.pkl"))

except Exception as e:
    print(f"\n[FEHLER] Während des Trainings aufgetreten: {e}")

finally:
    try:
        env.save(os.path.join(log_dir, "vecnormalize.pkl"))
    except Exception as e:
        print(f"[WARNUNG] VecNormalize konnte nicht gespeichert werden: {e}")
    env.close()
\end{minted}

\section{Netzwerk-Skripte}

\subsection{\texttt{check\_tls\_consistency.py} – Prüfung inkonsistenter Phasenlängen}
\label{app:check_tls_consistency}
Dieses Tool analysiert alle TLS im SUMO-Netz und prüft, ob die Länge des \texttt{state}-Strings mit der Anzahl der kontrollierten Verbindungen übereinstimmt.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

# === Konfiguration ===
net_file = "karlsruhe.net.xml"

# === Einlesen ===
tree = ET.parse(net_file)
root = tree.getroot()

# === Alle controlledLinks zählen ===
tls_controlled_links = {}
for connection in root.findall("connection"):
    if "tl" in connection.attrib and "linkIndex" in connection.attrib:
        tls_id = connection.attrib["tl"]
        tls_controlled_links.setdefault(tls_id, set()).add(int(connection.attrib["linkIndex"]))

# === Alle Phasen prüfen ===
def check_tls_lengths():
    print("Überprüfe alle TLS auf inkonsistente Phasenlängen...\n")
    any_issues = False
    for logic in root.findall("tlLogic"):
        tls_id = logic.attrib["id"]
        expected_len = len(tls_controlled_links.get(tls_id, []))

        if expected_len == 0:
            print(f" TLS '{tls_id}' hat keine controlledLinks (wird evtl. nicht gesteuert)")
            continue

        for i, phase in enumerate(logic.findall("phase")):
            actual_len = len(phase.attrib["state"])
            if actual_len != expected_len:
                print(f" Phase {i} von TLS '{tls_id}' hat Länge {actual_len}, erwartet: {expected_len}")
                print(f"    → state=\"{phase.attrib['state']}\"")
                any_issues = True

    if not any_issues:
        print(" Alle TLS-Phasen stimmen mit ihren controlledLinks überein!")

check_tls_lengths()
\end{minted}

\subsection{\texttt{check\_tls\_requests.py} – Prüfung ungültiger \texttt{<request>}-Indizes}
\label{app:check_tls_requests}
Prüft, ob alle \texttt{request}-Indizes innerhalb der zulässigen Grenzen liegen, um Laufzeitfehler in \texttt{sumo-rl} zu vermeiden.

\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

net_file = "karlsruhe.net.xml"
tree = ET.parse(net_file)
root = tree.getroot()

# Zähle für jedes TLS wie viele signal indices es gibt (controlled links)
tls_signal_indices = {}
for conn in root.findall("connection"):
    if "tl" in conn.attrib and "linkIndex" in conn.attrib:
        tls_id = conn.attrib["tl"]
        tls_signal_indices.setdefault(tls_id, set()).add(int(conn.attrib["linkIndex"]))

# Vergleiche mit den request-Elementen
print("Überprüfe request-Indizes gegen Signalindizes...\n")
any_issues = False
for junction in root.findall("junction"):
    tls_id = junction.attrib.get("id")
    requests = junction.findall("request")
    if tls_id in tls_signal_indices:
        expected_max = len(tls_signal_indices[tls_id])
        for req in requests:
            index = int(req.attrib["index"])
            if index >= expected_max:
                print(f"Junction '{tls_id}': request index {index} > max signal index {expected_max - 1}")
                any_issues = True

if not any_issues:
    print("Alle request-Indizes passen zu den TLS-Signalindizes!")
\end{minted}

\subsection{\texttt{fix\_requests.py} – Automatische Korrektur von Requests und Phasen}
\label{app:fix_requests}
Dieses Skript bereinigt überzählige \texttt{<request>}-Einträge und passt \texttt{state}-Strings in den Phasenlängen an.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

net_file = "karlsruhe.net.xml"
output_file = "karlsruhe_fixed_tls.net.xml"

tree = ET.parse(net_file)
root = tree.getroot()

# Finde maximal verwendete Signal-Indices pro TLS
tls_max_index = {}
for conn in root.findall("connection"):
    tl = conn.get("tl")
    idx = conn.get("linkIndex")
    if tl and idx:
        idx = int(idx)
        tls_max_index[tl] = max(tls_max_index.get(tl, -1), idx)

# Bereinigung
total_removed_requests = 0
total_adjusted_phases = 0
changed_tls = []

for junction in root.findall("junction"):
    tls_id = junction.get("id")
    if tls_id not in tls_max_index:
        continue

    max_idx = tls_max_index[tls_id]
    requests = list(junction.findall("request"))
    removed = 0

    for req in requests:
        req_idx = int(req.get("index"))
        if req_idx > max_idx:
            junction.remove(req)
            removed += 1

    if removed > 0:
        print(f"TLS '{tls_id}': {removed} ungültige <request>-Einträge entfernt.")
        total_removed_requests += removed
        changed_tls.append(tls_id)

    # Kürze zugehörige Phasen
    for tl in root.findall("tlLogic"):
        if tl.get("id") == tls_id:
            adjusted = 0
            for phase in tl.findall("phase"):
                state = phase.get("state")
                if len(state) > max_idx + 1:
                    old_len = len(state)
                    phase.set("state", state[:max_idx + 1])
                    adjusted += 1
            if adjusted > 0:
                print(f" TLS '{tls_id}': {adjusted} <phase>-Strings auf Länge {max_idx + 1} gekürzt.")
                total_adjusted_phases += adjusted
                if tls_id not in changed_tls:
                    changed_tls.append(tls_id)

# Speichern
tree.write(output_file, encoding="utf-8")
print("\n Reparatur abgeschlossen.")
print(f" Gesamt entfernte <request>-Einträge: {total_removed_requests}")
print(f" Gesamt angepasste <phase>-Einträge: {total_adjusted_phases}")
print(f" Betroffene TLS-IDs: {len(changed_tls)} Stück")
for tls in changed_tls:
    print(f"  - {tls}")
print(f"\n Bereinigte Datei gespeichert unter: {output_file}")
\end{minted}

\subsection{\texttt{repair\_net.py} – manuelle TLS-Reparatur auf Basis eines Referenz-Dictionaries}
\label{app:repair_net}
Repariert TLS-Definitionen durch Abgleich mit einer vordefinierten Mapping-Tabelle von korrekten Phasenlängen.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
from xml.etree import ElementTree as ET

# Manuell gepflegte Dictionary mit {TLS-ID: Anzahl controlledLinks}
controlled_links = {
    "1720933516": 6,
    "3538953167": 2,
    "3664415977": 10,
    "cluster_14795187_1720919996_2670370290_2670370291": 11,
    "cluster_14795804_55474925_6655074904_765746891_#1more": 49,
    "cluster_15431428_1719671850_1720917935": 20,
    "cluster_1590912233_3664415976_5083348337_5083348350": 11,
    "cluster_1692973685_1692973722_1718084055_1718084058_#11more": 36,
    "cluster_1729190097_3687504105": 8,
    "cluster_1744031943_5131521735": 10,
    "joinedS_1623835169_cluster_1137679587_1626739216_1728272870_1728272909_#17more": 33,
    "joinedS_309108716_cluster_11001804363_1125509937_12515596172_1784859792_#5more": 14,
    "joinedS_5092985445_cluster_1590912226_2911376263": 10,
    # ggf. mehr hinzufügen
}

tree = ET.parse("karlsruhe.net.xml")
root = tree.getroot()
changed = False

for logic in root.findall("tlLogic"):
    tl_id = logic.attrib["id"]
    if tl_id not in controlled_links:
        continue

    correct_len = controlled_links[tl_id]
    for phase in logic.findall("phase"):
        state = phase.attrib["state"]
        if len(state) != correct_len:
            new_state = state[:correct_len].ljust(correct_len, 'r')
            print(f" Fixing {tl_id}: {len(state)} → {correct_len}")
            phase.attrib["state"] = new_state
            changed = True

if changed:
    tree.write("karlsruhe_fixed.net.xml")
    print(" Bereinigte Datei gespeichert: karlsruhe_fixed.net.xml")
else:
    print(" Alle Phasen bereits korrekt.")

\end{minted}

\subsection{\texttt{statecheck.py} – Prüfung auf Ziel-Phasenlänge}
\label{app:statecheck}
Hilft bei der Kontrolle einheitlicher Phasenlängen über das gesamte Netz hinweg (z.\,B. Zielwert = 57).
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
from xml.etree import ElementTree as ET

tree = ET.parse("karlsruhe.net.xml")
root = tree.getroot()

for logic in root.findall("tlLogic"):
    tl_id = logic.attrib["id"]
    for i, phase in enumerate(logic.findall("phase")):
        state = phase.attrib["state"]
        if len(state) != 57:
            print(f" Phase {i} of TLS '{tl_id}' has length {len(state)}")
\end{minted}

\subsection{\texttt{find\_valid\_tls.py} – Validierung lauffähiger TLS für SUMO-RL}
\label{app:find_valid_tls}
Startet für jede einzelne Ampelkreuzung eine Minimalumgebung und überprüft, ob diese in \texttt{sumo-rl} trainierbar ist.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
from sumo_rl import SumoEnvironment
import traci
import os

def test_tls(tls_id):
    try:
        env = SumoEnvironment(
            net_file="karlsruhe.net.xml",
            route_file="karlsruhe.rou.xml",
            use_gui=False,
            single_agent=True
        )
        env.ts_ids = [tls_id]
        env.reset()
        env.close()
        return True
    except Exception as e:
        print(f" TLS {tls_id} nicht gültig: {e}")
        return False

# Alle TLS holen
try:
    env = SumoEnvironment(
        net_file="karlsruhe.net.xml",
        route_file="karlsruhe.rou.xml",
        use_gui=False,
        single_agent=True
    )
    all_tls = env.ts_ids
    env.close()
except Exception as e:
    print(" Konnte TLS nicht auslesen:", e)
    all_tls = []

print(f" Teste {len(all_tls)} TLS auf Gültigkeit...\n")
valid_tls = []

for tls_id in all_tls:
    if test_tls(tls_id):
        valid_tls.append(tls_id)

print("\n Gültige TLS:")
print(valid_tls)
\end{minted}


\subsection{\texttt{find\_relevant\_edges.py} – Suche alle relevanten edges}
\label{app:find_relevant_edges}
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{python}
import xml.etree.ElementTree as ET

# Konfiguration: Pfad zur .net.xml-Datei und Suchbegriffe
NET_FILE = "network.net.xml"
SUCHBEGRIFFE = ["B10", "B36", "L605", "Durlacher Allee", "Reinhold-Frank-Straße"]

# Ausgabe-Datei für gefundene Kanten
OUTPUT_FILE = "edges.txt"

def finde_relevante_kanten(net_file, suchbegriffe):
    tree = ET.parse(net_file)
    root = tree.getroot()
    
    relevante_kanten = []
    
    for edge in root.findall("edge"):
        name = edge.get("name")
        if name:
            for begriff in suchbegriffe:
                if begriff.lower() in name.lower():
                    relevante_kanten.append((edge.get("id"), name))
                    break  # nicht doppelt eintragen, falls mehrere Begriffe passen
                    
    return relevante_kanten

if __name__ == "__main__":
    kanten = finde_relevante_kanten(NET_FILE, SUCHBEGRIFFE)
    
    # Ergebnisse speichern
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for edge_id, name in kanten:
            f.write(f"{edge_id}\t{name}\n")
    
    print(f"{len(kanten)} relevante Kanten gefunden.")
    print(f"Ergebnisse in '{OUTPUT_FILE}' gespeichert.")

\end{minted}

\section{Sumo-Konfiguration}
\subsection{\texttt{sumoconfig\_.sumocfg}}
\label{app:sumocfg}
Die folgende Konfigurationsdatei definiert die zentralen Eingaben und
Parameter für die Simulation in SUMO. Sie verweist auf die zu ladende
Netzdatei und die zugehörigen Routendateien sowie auf den zu simulierenden
Zeitraum.
\begin{minted}[fontsize=\small, linenos, frame=lines, breaklines, tabsize=4]{xml}
<configuration>
  <input>
    <net-file value="network.net.xml"/>
    <route-files value="routes.xml"/>
  </input>
  <time>
    <begin value="0"/>
    <end value="5000"/>
  </time>
</configuration>
\end{minted}