\section{Evaluation und Ergebnisse}
\label{sec:validation}

In diesem Kapitel werden die Ergebnisse der Evaluationsläufe präsentiert.
Alle Modelle, bestehend aus 4 Trainingsseeds pro Reward-Variante, sowie die beiden Baselines, Fixed-Time und Actuated. Diese
wurden in allen vier Szenarien \texttt{morning\_peak}, \texttt{evening\_peak}, \texttt{uniform} und \texttt{random\_heavy}
jeweils über zehn Episoden evaluiert.

\texttt{Anmerkung:} Die Resultate setzten sich aus Mittelwerten pro Kombination aus \emph{Methode} und \emph{Szenario} zusammen. Die Baseline Actuated hat in allen Evaluierungen verhältnismäßig schlecht abgeschlossen und wurde größten teils separat in Diagrammen dargestellt um sicherzustellen, dass die deutlich besser performende Baseline TimeFixed visuell deutlich mit den Modellen vergleichbar bleibt. In SUMO werden Fahrzeuge die sich mit einer Geschwindigkeit kleiner als 0.1 m/s bewegen als gestoppt eingestuft. \cite{sumo-doc}

\input{chapters/evaluation/diff-waiting-time.tex}
\input{chapters/evaluation/queue.tex}
\input{chapters/evaluation/realworld.tex}
\input{chapters/evaluation/emissions.tex}

\subsection{Robustheit und Replikationsanalyse}

Ein zentrales Ziel der Evaluation bestand darin, nicht nur die absolute Leistungsfähigkeit
der Modelle zu messen, sondern auch deren \textbf{Robustheit} und \textbf{Replikationsfähigkeit}
zu bewerten. Unter Robustheit wird hier die Fähigkeit verstanden, auch unter variierenden
Verkehrsbedingungen (verschiedene Szenarien) und unterschiedlichen Initialisierungen (Seeds)
konsistente Resultate zu erzielen. Replikationsfähigkeit bezeichnet hingegen die Eigenschaft,
dass ein Modell mit gleicher Konfiguration über mehrere Trainingsläufe hinweg vergleichbare
Ergebnisse liefert.

\paragraph{Methodisches Vorgehen}
Für die Robustheitsanalyse wurden alle Modelle sowie die beiden Baselines in den vier Szenarien
(morning\_peak, evening\_peak, uniform, random\_heavy) jeweils über zehn Episoden evaluiert.
Unterschiedliche Seeds stellten dabei sicher, dass sowohl Zufallseinflüsse im Verkehrsfluss
als auch in der Modellinitialisierung berücksichtigt wurden. Pro Reward-Funktion lagen vier
unabhängige Trainingsseeds vor, sodass neben der Szenario-Robustheit auch die Replikationsfähigkeit
geprüft werden konnte.

Die Evaluationsumgebung protokollierte sowohl \textit{mittlere Metriken} (z.\,B. durchschnittliche
Wartezeit, mittlere Geschwindigkeit) als auch \textit{Totals} (z.\,B. Anzahl ankommender Fahrzeuge,
Gesamtemissionen) pro Episode. Diese Auswertung erlaubte es, systematische Leistungsunterschiede
von stochastischen Schwankungen zu trennen. Besonders aufschlussreich war dabei die Betrachtung
der Standardabweichungen: hohe Varianz bei gleicher Reward-Funktion weist auf eine eingeschränkte
Replikationsfähigkeit hin.

\paragraph{Ergebnisse}
Über alle Reward-Funktionen hinweg zeigte sich ein konsistentes Muster:
\begin{itemize}
    \item \textbf{Baselines:} Die Actuated-Baseline erwies sich in allen Szenarien als instabil und deutlich unterlegen.
          Die Fixed-Time-Baseline blieb robust, konnte jedoch nur in Szenarien mit regulärer Last überzeugen.
    \item \textbf{Diff-Waiting-Time:} Modelle auf Basis dieser Reward-Funktion waren insgesamt robust, insbesondere
          Modell~1 und Modell~4. Modelle~2 und~3 zeigten dagegen in random\_heavy erhöhte Varianz und teilweise deutliche
          Leistungseinbrüche, was die eingeschränkte Replikationsfähigkeit einzelner Seeds verdeutlicht.
    \item \textbf{Queue:} Die Queue-basierten Modelle erzielten eine durchgängig hohe Stabilität und reduzierten
          Wartezeiten und Stopps stark. Lediglich Modell~2 zeigte in random\_heavy und morning\_peak eine geringere Robustheit.
    \item \textbf{Real-World:} Diese Reward-Funktion lieferte in allen Szenarien außer random\_heavy sehr stabile
          und nahe am Optimum liegende Ergebnisse. Unter hoher und unregelmäßiger Last traten jedoch erhöhte
          Standardabweichungen auf, was auf eingeschränkte Robustheit hindeutet.
    \item \textbf{CO\textsubscript{2}-Emissionen:} Die Emissions-basierten Modelle kombinierten hohe Effizienz mit
          ökologischer Optimierung und erwiesen sich als besonders stabil. Lediglich in Hochlastszenarien (insb. Modell~2 und~3)
          zeigten sich Replikationsprobleme, die sich jedoch auf ein Niveau im Bereich der Fixed-Time-Baseline einpendelten.
\end{itemize}

\paragraph{Schlussfolgerung}
Die Analyse zeigt, dass die Robustheit und Replikationsfähigkeit stark von der gewählten
Reward-Funktion abhängen. Während Queue- und CO\textsubscript{2}-basierte Ansätze eine besonders
stabile Performanz liefern, neigen Diff-Waiting-Time- und Real-World-Rewards unter Hochlastbedingungen
zu Instabilitäten. In allen Fällen bleiben die Modelle den klassischen Baselines jedoch überlegen.
Damit bestätigt sich, dass Deep-RL-basierte Verkehrssteuerungen grundsätzlich robuste Strategien
hervorbringen, deren Generalisierungsfähigkeit sich jedoch insbesondere in unregelmäßigen
Verkehrssituationen weiter verbessern muss.


\subsection{Gesamtevaluierung und Schlussfolgerung}

Die Evaluation hat gezeigt, dass alle untersuchten Reinforcement-Learning-Ansätze die klassischen Baselines (Fixed-Time und Actuated) in nahezu allen Metriken deutlich übertreffen. Während die Actuated-Baseline durchgängig instabil war, bestätigte sich die Fixed-Time-Strategie als solide Referenz, die jedoch ebenfalls von den RL-Modellen übertroffen werden konnte.

Über alle Reward-Funktionen hinweg führten die Modelle zu einer deutlichen Reduktion der mittleren Wartezeit, der Anzahl stoppender Fahrzeuge und zu höheren Durchschnittsgeschwindigkeiten, ohne Einbußen im Gesamtdurchsatz. Besonders stabil zeigten sich dabei Queue- und CO\textsubscript{2}-basierte Modelle. Diff-Waiting-Time- und Real-World-Ansätze waren zwar oft effizient, reagierten jedoch unter Hochlastbedingungen sensibler auf Zufallseinflüsse.

Insgesamt wird deutlich: Deep-RL kann klassische Steuerungsverfahren nicht nur erreichen, sondern in vielen Szenarien übertreffen. Gleichzeitig bleibt die Robustheit unter unregelmäßigen Bedingungen (z. B. random\_heavy) eine zentrale Herausforderung.