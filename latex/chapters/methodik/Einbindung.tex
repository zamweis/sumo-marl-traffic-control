\subsection{Einbindung des SUMO-Netzes in die RL-Umgebung}
\label{sec:sumo-rl-architektur}

Nach Abschluss der Netzbereinigung, der strukturellen Validierung und der Identifikation steuerbarer Lichtsignalanlagen (TLS) wurde das finale Verkehrsnetz in eine auf \texttt{sumo-rl} basierende Reinforcement-Learning-Umgebung integriert. Ziel war die Realisierung einer robusten, modularen Multiagentenumgebung, die eine lernbasierte Optimierung der Verkehrssteuerung unter realitätsnahen Bedingungen erlaubt.

\subsubsection{Gesamtsystem und Architektur}

Die Architektur der Lernumgebung ist als verteiltes Multiagentensystem ausgelegt, bei dem jede signalgesteuerte Kreuzung durch einen eigenständigen Agenten repräsentiert wird. Die Interaktion erfolgt über das TraCI-Protokoll \cite{TraCI} von SUMO \cite{sumo-doc}, das eine Echtzeitkommunikation zwischen Simulator und RL-Agenten ermöglicht. Die zentrale Steuerung und das Training der Agenten basiert auf der RL-Bibliothek \texttt{Stable-Baselines3} \cite{Stable-Baselines3}, konkret dem Algorithmus \texttt{Proximal Policy Optimization (PPO) \cite{PPO}}. \cite{sumo-rl_docs}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance=1cm]

    \node[box] (sim) {Verkehrssimulation\\\textbf{SUMO + TraCI}};
    \node[box, below=of sim] (mae) {Multi-Agent Environment (MAE)\\\texttt{sumo-rl.parallel\_env} + \texttt{emissions\_with\_speed\_reward}};
    \node[box, below=of mae] (wrap) {SuperSuit Wrappers\\\texttt{- pad\_obs / pad\_actions}\\\texttt{- pettingzoo\_to\_vec\_env}\\\texttt{- concat\_vec\_envs}};
    \node[box, below=of wrap] (vec) {VecNormalize + VecMonitor\\(Stable-Baselines3)};
    \node[box, below=of vec] (ppo) {PPO Agent (Stable-Baselines3)\\\texttt{- MLPPolicy}\\\texttt{- Schedules: LR (cosine warmup), Clip (cosine)}};
    \node[box, below=of ppo] (cb) {Callbacks\\\texttt{- TimeBasedCheckpoint}\\\texttt{- EnvMetricsLogger}\\\texttt{- BestModelSaver}};
    \node[box, below=of cb] (log) {Modell- \& Log-Speicherung\\\texttt{(pro Seed in runs/)}};

    \draw[arrow] (sim) -- (mae);
    \draw[arrow] (mae) -- (wrap);
    \draw[arrow] (wrap) -- (vec);
    \draw[arrow] (vec) -- (ppo);
    \draw[arrow] (ppo) -- (cb);
    \draw[arrow] (cb) -- (log);

  \end{tikzpicture}
  \caption{Architektur der RL-Trainingspipeline mit SUMO, MAE und Stable-Baselines3}
  \label{fig:rl_architektur}
\end{figure}

Zur Vereinheitlichung der Multiagentenumgebung kamen die Bibliotheken \texttt{PettingZoo} \cite{PettingZoo} und \texttt{SuperSuit} \cite{SuperSuit} zum Einsatz. \texttt{PettingZoo} stellt ein standardisiertes API für Multiagenten-Umgebungen bereit – analog zu \texttt{Gymnasium} \cite{Gymnasium}, jedoch speziell für Szenarien mit mehreren Agenten. \texttt{SuperSuit} erweitert diese Umgebungen durch eine Vielzahl an Wrappers, beispielsweise zur Vereinheitlichung von Beobachtungs- und Aktionsräumen (z.\,B. durch Padding) oder zur Umwandlung in vektorisierte Formate. Für parallele Ausführung ist die Umgebung mittels \texttt{concat\_vec\_envs\_v1} auf Mehrkernbetrieb vorbereitet (\texttt{num\_cpus = 8}); im vorliegenden Skript (siehe Anhang \ref{app:train_script}) wird eine Instanz (\texttt{num\_vec\_envs = 1}) betrieben, sodass die Skalierung ohne Codeänderungen möglich ist.

\subsubsection{Konfiguration der Umgebung}

Die Reinforcement-Learning-Umgebung wurde auf Basis der Klasse \texttt{parallel\_env} aus \texttt{sumo-rl} konfiguriert. Wichtige Parameter umfassen:

\begin{itemize}
  \item \texttt{net\_file}, \texttt{route\_file}: Pfade zum bereinigten Verkehrsnetz und zugehöriger Routendatei.
  \item \texttt{use\_gui}: Deaktiviert für schnelleres Training (\texttt{False}); bei Bedarf zur Laufzeit aktivierbar.
  \item \texttt{num\_seconds}: Dauer einer Simulationsepisode in Sekunden (hier 5000\,s); das Episodenende wird unabhängig vom Verkehrsaufkommen nach Ablauf dieser Zeit ausgelöst.
  \item \texttt{delta\_time}: Zeitabstand zwischen Agentenentscheidungen; nicht überschrieben, daher \textbf{Defaultwert 5\,s}.
  \item \texttt{reward\_fn}: Referenz auf die aktive Belohnungsfunktion; im Haupttraining \texttt{realworld\_reward}.
  \item \texttt{min\_green}: Minimale Grünphasenlänge (5\,s) zur Sicherstellung realistischer Signalzyklen.
  \item \texttt{max\_depart\_delay}: Maximale Verzögerung bei der Einfahrt eines Fahrzeugs (100\,s).
  \item \texttt{sumo\_seed}: Fester Zufalls-Seed aus der Seedliste $\{546456, 678678, 234256, 678\}$ zur Reproduzierbarkeit von Verkehrsflüssen und Routenentscheidungen.
  \item \texttt{add\_system\_info}: Wenn aktiviert, werden systemweite Kennzahlen in die \emph{Info}-Daten (\texttt{infos}) eingebettet (nicht Teil des Zustandsvektors).
  \item \texttt{add\_per\_agent\_info}: Ergänzt die lokale Beobachtung jedes Agenten um zusätzliche Kontextdaten (hier deaktiviert).
  \item \texttt{single\_agent = False}: Multiagentenmodus; jede steuerbare Ampel ist ein separater Agent.
\end{itemize}

Die Umgebung ist vollständig kompatibel mit \texttt{Gymnasium}, \texttt{PettingZoo} sowie den Wrapper-Bibliotheken \texttt{SuperSuit} und \texttt{VecEnv}, wodurch ein standardisiertes Interfacing mit RL-Algorithmen ermöglicht wird.

\subsubsection{Beobachtungen und Aktionsraum}

Jeder Agent erhält eine lokale Beobachtung, bestehend aus:
\begin{itemize}
  \item \textbf{Aktuelle Ampelphase:} als diskreter Index oder One-Hot-Encoding (dimensionslos),
  \item \textbf{Dauer der aktuellen Phase:} Sekunden seit Beginn der Phase [s],
  \item \textbf{Spurmesswerte:} für jede anliegende Fahrspur
        \begin{itemize}
          \item Anzahl wartender Fahrzeuge [Fz],
          \item mittlere Geschwindigkeit [m/s],
          \item Dichte (Verhältnis belegter zu maximal möglicher Fahrzeuge pro Spur, dimensionslos, $0\text{–}1$).
        \end{itemize}
\end{itemize}

Da sich die Anzahl an Fahrspuren pro Kreuzung unterscheidet, variiert die Rohdimension des Beobachtungsvektors.
Um eine feste Eingabelänge für die Policy zu gewährleisten, werden alle Vektoren mittels \texttt{pad\_observations\_v0} auf die maximale Spuranzahl aller gesteuerten Kreuzungen gepaddet; fehlende Spurwerte werden mit Null belegt.
Dadurch ergibt sich für alle Agenten eine identische Beobachtungsdimension von
\[
  n_{\text{features}} \times n_{\text{max\_spuren}} + n_{\text{phasenfeatures}}
\]
mit $n_{\text{features}} = 3$ (Queue, Geschwindigkeit, Dichte) und $n_{\text{phasenfeatures}} = 2$ (Phase, Phasendauer).

Der Aktionsraum ist diskret und erlaubt die Auswahl der nächsten Signalphase.
Ein Phasenwechsel wird durch SUMO automatisch mit einer Zwischenphase (Gelbphase) ergänzt.
Alle Agenten entscheiden synchron in Intervallen von \texttt{delta\_time = 5\,s}.


\subsubsection{Belohnungsfunktionen}

\texttt{sumo-rl} unterstützt verschiedene standardisierte Reward-Funktionen: \cite{sumo-rl_docs}

\begin{itemize}
  \item \texttt{"diff-waiting-time"}: Reduktion der Differenz kumulierter Wartezeiten,
  \item \texttt{"average-speed"}: Maximierung der mittleren Geschwindigkeit im Netz,
  \item \texttt{"queue"}: Minimierung der Gesamtlänge aller Warteschlangen.
\end{itemize}

Im Rahmen dieser Arbeit wurden zusätzlich mehrere eigene Reward-Funktionen implementiert (siehe Abschnitt~\ref{sec:rl-konzept}).
Für das Haupttraining wurde \texttt{realworld\_reward} eingesetzt, da diese ausschließlich auf Metriken basiert,
die sich in der Praxis mit vergleichsweise einfacher Sensorik (z.\,B. Kameras, Induktionsschleifen, Lichtsensoren) erfassen lassen,
wie Warteschlangenlänge, Anzahl anfahrender Fahrzeuge und durchschnittliche Geschwindigkeit.

Für Vergleichsläufe wurden weitere Varianten eingesetzt, darunter \texttt{emissions\_with\_speed\_reward}
(Kombination aus geglätteten, exponentiell gemittelten CO\(_2\)-Emissionen mit $\alpha=0.3$ (negativ), Geschwindigkeit (positiv),
Fluss (positiv) sowie Warteschlangenlänge (negativ), mit \texttt{tanh}-Clamping),
sowie \texttt{travel\_time\_reward}, \texttt{custom\_reward} und \texttt{travel\_time\_emissions\_reward} (siehe Anhang \ref{app:train_script}).

\subsubsection{Trainingsalgorithmus und Hyperparameter}

Das Training der Agenten erfolgte mittels \texttt{PPO}, wobei folgende Hyperparameter eingesetzt wurden (siehe Anhang \ref{app:train_script}):

\begin{itemize}
  \item \textbf{Policy-Architektur:} Zwei Hidden-Layer mit jeweils 128 Neuronen; getrennte Netze für \texttt{pi} und \texttt{vf} (\texttt{policy\_kwargs}).
  \item \textbf{Batchgröße:} 512.
  \item \textbf{Rollout-Länge (\texttt{n\_steps}):} 2048.
  \item \textbf{Lernrate:} Cosine-Warmup über 5\,\% der Trainingszeit mit anschließendem Cosine-Decay auf 10\,\% des Startwerts ($3\cdot10^{-4}$).
  \item \textbf{Clip-Range:} Cosine-Interpoliert von 0.2 auf 0.1.
  \item \textbf{Entropiekoeffizient:} 0.01 (Exploration).
  \item \textbf{Discount-Faktor:} $\gamma = 0.99$.
  \item \textbf{GAE-Lambda:} 0.95.
  \item \textbf{Device:} CPU (\texttt{torch.set\_num\_threads} auf Standard; SUMO-Simulation single-threaded).
\end{itemize}


Beobachtungen und Rewards wurden über \texttt{VecNormalize} \cite{VecNormalize} normalisiert (\texttt{clip\_obs = 10.0}, \texttt{clip\_reward = 10.0}, \texttt{norm\_obs = True}, \texttt{norm\_reward = True}), die Umgebung zusätzlich mit \texttt{VecMonitor} \cite{VecMonitor} überwacht (\texttt{monitor.csv}). Pro Seed wurden $2\cdot10^6$ Trainingsschritte durchgeführt.


Die Trainingsdauer pro Seed betrug im Mittel 2~Stunden für $2\cdot10^6$ Schritte
bei einer Episodenlänge von \texttt{num\_seconds = 5000},
inklusive periodischer Checkpoint-Speicherung und Logging. Dies variert mit der eingesetzten Belohnungsfunktion.
Kürzere Vergleichsläufe mit $5\cdot10^5$ Schritten wurden in etwa 35~Minuten abgeschlossen.


Die Wahl der Hyperparameter erfolgte auf Basis einer Kombination aus Literaturrecherche
und empirischen Vorversuchen auf einem synthetischen Verkehrsnetz,
das strukturell an das finale Zielnetz angelehnt war.
Die Netzarchitektur mit zwei Hidden-Layern à 128 Neuronen wurde gewählt,
da sie in vergleichbaren SUMO-RL-Szenarien
eine gute Balance zwischen Modellkapazität und Generalisierungsfähigkeit bietet \cite{Stable-Baselines3, PPO}.


Die Batchgröße von 512 und die Rollout-Länge von 2048 Schritten stellen sicher,
dass pro Policy-Update ausreichend diversifizierte Datenpunkte vorliegen,
ohne den Speicherbedarf zu stark zu erhöhen.
Der Cosine-Schedule für Lernrate und Clip-Range dient dazu,
zu Beginn eine hohe Exploration zuzulassen und gegen Ende des Trainings die Updates zu stabilisieren.
Der Entropiekoeffizient von 0{,}01 wurde so gewählt,
dass auch in späten Trainingsphasen eine gewisse Exploration erhalten bleibt.


\subsubsection{Checkpoints, Monitoring und Logging}

Zur Sicherstellung eines robusten Trainingsprozesses wurde eine Reihe von Callback-Mechanismen implementiert (siehe Anhang \ref{app:train_script}):

\begin{itemize}
  \item \textbf{TimeBasedCheckpointCallback:}
        Zeitbasierte Sicherung des Modells und der Normalisierungsdaten in festen Intervallen
        (standardmäßig stündlich). Gespeicherte Dateien:
        \begin{itemize}
          \item \texttt{ppo\_sumo\_model\_\{steps\}\_steps.zip} – Modellparameter,
          \item \texttt{ppo\_sumo\_model\_\{steps\}\_steps\_vecnormalize.pkl} – Normalisierungszustand.
        \end{itemize}

  \item \textbf{EnvMetricsLoggerCallback:}
        Aggregiert und loggt Umgebungsmetriken aus dem \texttt{infos}-Dictionary.
        Schlüssel mit Präfix \texttt{system\_} werden in globale Mittelwerte umbenannt (\texttt{mean\_...}).
        Geloggte Metriken umfassen:
        \begin{itemize}
          \item \texttt{mean\_arrived} – mittlere Anzahl neu eingetroffener Fahrzeuge,
          \item \texttt{mean\_departed} – mittlere Anzahl abgefahrener Fahrzeuge,
          \item \texttt{mean\_backlogged} – mittlere Anzahl nicht abgefertigter Fahrzeuge,
          \item \texttt{mean\_running} – mittlere Anzahl aktuell fahrender Fahrzeuge,
          \item \texttt{mean\_stopped} – mittlere Anzahl stehender Fahrzeuge,
          \item \texttt{mean\_mean\_speed} – mittlere Durchschnittsgeschwindigkeit,
          \item \texttt{mean\_mean\_waiting\_time} – mittlere mittlere Wartezeit pro Fahrzeug,
          \item \texttt{mean\_waiting\_time} – mittlere Wartezeit (gesamt),
          \item \texttt{mean\_teleported} – mittlere Anzahl Teleportationen pro Schritt,
          \item \texttt{mean\_emissions\_per\_vehicle} – mittlere CO\textsubscript{2}-Emission pro Fahrzeug.
        \end{itemize}

  \item \textbf{BestModelSaverCallback:}
        Automatische Speicherung des jeweils besten Modells (höchster mittlerer Episodenreward aus \texttt{ep\_info\_buffer}),
        inklusive zugehöriger Normalisierungsdaten:
        \begin{itemize}
          \item \texttt{best\_model.zip} – bestes Modell,
          \item \texttt{best\_model\_vecnormalize.pkl} – Normalisierungszustand des besten Modells.
        \end{itemize}
\end{itemize}


Alle Modellartefakte (\texttt{.zip}, \texttt{vecnormalize.pkl}) sowie die TensorBoard-Logs \cite{TensorBoard}
wurden pro Seed-Version strukturiert gespeichert. Das Log-Verzeichnis folgt dem Schema
\texttt{runs/ppo\_sumo\_<SEED>\_<YYYY-MM-DD\_HH-MM-SS>}.
\texttt{tensorboard\_log} verweist auf diesen Ordner; \texttt{VecMonitor} schreibt zusätzlich \texttt{monitor.csv}.
TensorBoard wurde während des gesamten Trainings eingesetzt, um den Verlauf zentraler Kennzahlen
(z.\,B. Episodenreward, mittlere Geschwindigkeit, Wartezeiten, CO\textsubscript{2}-Emissionen,
Lernrate, Clip-Range) in Echtzeit zu überwachen.
Dies ermöglichte eine frühzeitige Erkennung von Instabilitäten (z.\,B. plötzliche Reward-Einbrüche)
sowie die Analyse des Lernfortschritts in unterschiedlichen Trainingsphasen.
Die Visualisierungen wurden zudem genutzt, um Hyperparameter-Anpassungen
in Vorversuchen gezielt zu validieren.

\paragraph{Hardware-Setup}
Alle Trainingsläufe wurden auf einem System mit
Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i7-6700K Prozessor
(4~Kerne, 8~Threads, 4{,}0\,GHz) und 16\,GB RAM durchgeführt.
Aufgrund der single-threaded Ausführung der SUMO-Simulation
wurde der RL-Algorithmus ebenfalls auf der CPU betrieben (\texttt{device='cpu'}),
was eine konsistente Laufzeitmessung und Reproduzierbarkeit sicherstellte.
Der durchschnittliche Speicherverbrauch pro Trainingslauf lag bei rund 9\,GB.
Die Multi-Core-Parallellisierung über \texttt{concat\_vec\_envs\_v1} war vorbereitet,
wurde jedoch nicht aktiviert, um Speicherverbrauch und Interprozesskommunikations-Overhead zu vermeiden.
Während des Trainings wurde eine durchschnittliche CPU-Auslastung von 60\,\%–70\,\% verzeichnet.

\paragraph{Versionen und Systemumgebung}
Die Experimente wurden unter folgender Systemumgebung durchgeführt:
\begin{itemize}
  \item \textbf{Betriebssystem:} Windows~10~Pro~(64-Bit)
  \item \textbf{Python:}~3.10.12
  \item \textbf{SUMO:}~1.18.0 (mit HBEFA~4.1-Emissionsmodell)
  \item \textbf{Stable-Baselines3:}~2.1.0
  \item \textbf{Gymnasium:}~0.29.1
  \item \textbf{PettingZoo:}~1.23.0
  \item \textbf{SuperSuit:}~3.9.0
  \item \textbf{PyTorch:}~2.1.2+cpu
  \item \textbf{sumo-rl:}~1.4.5
\end{itemize}
Die Pfade zu SUMO wurden über die Umgebungsvariable \texttt{SUMO\_HOME} gesetzt. Alle Experimente wurden mit deaktiviertem GUI-Modus gestartet, um den Simulationsfaktor zu maximieren.

\paragraph{Reproduzierbarkeit und Laufverwaltung}
Zur Sicherstellung der Reproduzierbarkeit wurden Seeds für NumPy, PyTorch und SUMO gesetzt; zusätzlich wird (falls verfügbar) die Umgebung selbst mit \texttt{env.seed(SEED)} initialisiert. Es wurden vier verschiedene Seed-Werte $\{546456, 678678, 234256, 678\}$ verwendet. Eine Hilfsroutine (\texttt{find\_latest\_complete\_run}) erlaubt die Identifikation des letzten vollständigen Laufs (finales Modell oder jüngster Checkpoint inkl. \texttt{VecNormalize}) zur Wiederaufnahme. Evaluationsläufe können nach dem Training mit deaktivierter Exploration (\texttt{deterministic=True}) gestartet werden.

\paragraph{Fehler- und Abbruchbehandlung}
Bei manuellem Abbruch (\texttt{KeyboardInterrupt}) werden der aktuelle Modellstand (\texttt{model\_interrupt.zip}) und Normalisierungsdaten (\texttt{vecnormalize\_interrupt.pkl}) gesichert. Im \texttt{finally}-Block wird die Normalisierung zusätzlich persistiert und die Umgebung sauber geschlossen.

\subsubsection{Zusammenfassung}

Die konfigurierte RL-Umgebung erlaubt eine modulare und flexible Steuerung realer Verkehrsnetze auf Basis von SUMO. Durch die Kombination aus systematischer TLS-Auswahl, stabiler und geglätteter Reward-Funktion, adaptiven Hyperparameter-Schedules, vorbereiteter Multi-Core-Parallellisierung, konsequenter Reproduzierbarkeit und umfassendem Monitoring (inkl. Checkpoints, Metriklogging und Bestmodell-Erkennung) wurde eine solide Grundlage für die experimentelle Evaluation lernbasierter Verkehrssteuerung geschaffen.
