\subsection{Einbindung des SUMO-Netzes in die RL-Umgebung}
\label{sec:sumo-rl-architektur}

Nach Abschluss der Netzbereinigung, der strukturellen Validierung und der Identifikation steuerbarer Lichtsignalanlagen (TLS) wurde das finale Verkehrsnetz in eine auf \texttt{sumo-rl} basierende Reinforcement-Learning-Umgebung integriert. Ziel war die Realisierung einer robusten, modularen Multiagentenumgebung, die eine lernbasierte Optimierung der Verkehrssteuerung unter realitätsnahen Bedingungen erlaubt.

\subsubsection{Gesamtsystem und Architektur}

Die Architektur der Lernumgebung ist als verteiltes Multiagentensystem ausgelegt, bei dem jede signalgesteuerte Kreuzung durch einen eigenständigen Agenten repräsentiert wird. Die Interaktion erfolgt über das TraCI-Protokoll \cite{TraCI} von SUMO \cite{sumo-doc}, das eine Echtzeitkommunikation zwischen Simulator und RL-Agenten ermöglicht. Die zentrale Steuerung und das Training der Agenten basiert auf der RL-Bibliothek \texttt{Stable-Baselines3} \cite{Stable-Baselines3}, konkret dem Algorithmus \texttt{Proximal Policy Optimization (PPO) \cite{PPO}}. \cite{sumo-rl_docs}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance=1cm]

    \node[box] (sim) {Verkehrssimulation\\\textbf{SUMO + TraCI}};
    \node[box, below=of sim] (mae) {Multi-Agent Environment (MAE)\\\texttt{sumo-rl.parallel\_env} + \texttt{emissions\_with\_speed\_reward}};
    \node[box, below=of mae] (wrap) {SuperSuit Wrappers\\\texttt{- pad\_obs / pad\_actions}\\\texttt{- pettingzoo\_to\_vec\_env}\\\texttt{- concat\_vec\_envs}};
    \node[box, below=of wrap] (vec) {VecNormalize + VecMonitor\\(Stable-Baselines3)};
    \node[box, below=of vec] (ppo) {PPO Agent (Stable-Baselines3)\\\texttt{- MLPPolicy}\\\texttt{- Schedules: LR (cosine warmup), Clip (cosine)}};
    \node[box, below=of ppo] (cb) {Callbacks\\\texttt{- TimeBasedCheckpoint}\\\texttt{- EnvMetricsLogger}\\\texttt{- BestModelSaver}};
    \node[box, below=of cb] (log) {Modell- \& Log-Speicherung\\\texttt{(pro Seed in runs/)}};

    \draw[arrow] (sim) -- (mae);
    \draw[arrow] (mae) -- (wrap);
    \draw[arrow] (wrap) -- (vec);
    \draw[arrow] (vec) -- (ppo);
    \draw[arrow] (ppo) -- (cb);
    \draw[arrow] (cb) -- (log);

  \end{tikzpicture}
  \caption{Architektur der RL-Trainingspipeline mit SUMO, MAE und Stable-Baselines3}
  \label{fig:rl_architektur}
\end{figure}

Zur Vereinheitlichung der Multiagentenumgebung kamen die Bibliotheken \texttt{PettingZoo} \cite{PettingZoo} und \texttt{SuperSuit} \cite{SuperSuit} zum Einsatz. \texttt{PettingZoo} stellt ein standardisiertes API für Multiagenten-Umgebungen bereit – analog zu \texttt{Gymnasium} \cite{Gymnasium}, jedoch speziell für Szenarien mit mehreren Agenten. \texttt{SuperSuit} erweitert diese Umgebungen durch eine Vielzahl an Wrappers, beispielsweise zur Vereinheitlichung von Beobachtungs- und Aktionsräumen (z.\,B. durch Padding) oder zur Umwandlung in vektorisierte Formate. Für parallele Ausführung ist die Umgebung mittels \texttt{concat\_vec\_envs\_v1} auf Mehrkernbetrieb vorbereitet (\texttt{num\_cpus = 8}); im vorliegenden Skript (siehe Anhang \ref{app:train_script}) wird eine Instanz (\texttt{num\_vec\_envs = 1}) betrieben, sodass die Skalierung ohne Codeänderungen möglich ist.

\subsubsection{Agentenzuordnung im Multiagentensystem}
Jede steuerbare Lichtsignalanlage (TLS) wird einem eindeutigen Agenten zugeordnet.
Die Zuordnung erfolgt deterministisch anhand der in der SUMO-Netzdatei vergebenen TLS-IDs.
Die Agenten handeln vollständig lokal, es findet kein direkter Informationsaustausch zwischen ihnen statt.
Jede Beobachtung wird vor der Weitergabe an die Policy mittels \texttt{VecNormalize} standardisiert,
sodass alle Eingaben über verschiedene Kreuzungen hinweg vergleichbar sind.
Die Beobachtungsvektoren werden mit \texttt{pad\_observations\_v0} auf eine feste Länge gebracht,
die sich aus der maximalen Spuranzahl einer Kreuzung multipliziert mit der Anzahl Spurfeatures (Queue, Geschwindigkeit, Dichte)
zuzüglich der Phasenfeatures (Phase, Phasendauer) ergibt.

\subsubsection{Vermeidung von Überanpassung}
Um zu verhindern, dass die trainierten Agenten lediglich auf wiederkehrende Verkehrsmuster optimiert werden,
wurden die Routenflüsse mit \texttt{randomTrips.py} generiert.
Dadurch variiert die Nutzung einzelner Strecken, und es entstehen unterschiedliche Verkehrsverteilungen innerhalb der Episoden.
Zusätzlich wurden während des Trainings mit \texttt{TensorBoard} zentrale Leistungsmetriken (mittlere Geschwindigkeit, Wartezeit, CO$_2$-Emissionen, Durchsatz) in Echtzeit überwacht,
um sicherzustellen, dass sich die Leistungsverbesserungen nicht nur auf die Trainingsdaten beziehen.
Evaluationsläufe mit zuvor nicht genutzten Zufallsseeds dienen der abschließenden Validierung (siehe Kapitel~\ref{chap:validation}).


\subsubsection{Konfiguration der Umgebung}

Die Reinforcement-Learning-Umgebung wurde auf Basis der Klasse \texttt{parallel\_env} aus \texttt{sumo-rl} konfiguriert.
Wichtige Parameter umfassen:

\begin{itemize}
  \item \texttt{net\_file}, \texttt{route\_file}: Pfade zum bereinigten Verkehrsnetz und zugehöriger Routendatei.
  \item \texttt{use\_gui}: Deaktiviert für schnelleres Training (\texttt{False}); bei Bedarf zur Laufzeit aktivierbar.
  \item \texttt{num\_seconds}: Dauer einer Simulationsepisode in Sekunden (hier 5000\,s).
  \item \texttt{delta\_time}: Zeitabstand zwischen Agentenentscheidungen (Default: 5\,s).
  \item \texttt{reward\_fn}: Referenz auf die aktive Belohnungsfunktion.
        Für das Haupttraining wurde \texttt{realworld\_reward} verwendet (Details siehe Abschnitt~\ref{sec:rl-konzept}).
  \item \texttt{min\_green}: Minimale Grünphasenlänge (5\,s).
  \item \texttt{max\_depart\_delay}: Maximale Verzögerung bei der Einfahrt eines Fahrzeugs (100\,s).
  \item \texttt{sumo\_seed}: Fester Zufalls-Seed aus der Seedliste $\{546456, 678678, 234256, 678\}$.
  \item \texttt{add\_system\_info}: Ergänzt systemweite Kennzahlen in den \texttt{infos}.
  \item \texttt{add\_per\_agent\_info}: Kontextdaten pro Agent (hier deaktiviert).
  \item \texttt{single\_agent = False}: Multiagentenmodus; jede steuerbare Ampel ist ein separater Agent.
\end{itemize}

Die Umgebung ist vollständig kompatibel mit \texttt{Gymnasium}, \texttt{PettingZoo} sowie den Wrapper-Bibliotheken \texttt{SuperSuit} und \texttt{VecEnv}, wodurch ein standardisiertes Interfacing mit RL-Algorithmen ermöglicht wird.

\subsubsection{Beobachtungen und Aktionsraum}

Jeder Agent erhält eine lokale Beobachtung, bestehend aus:
\begin{itemize}
  \item \textbf{Aktuelle Ampelphase:} als diskreter Index oder One-Hot-Encoding (dimensionslos),
  \item \textbf{Dauer der aktuellen Phase:} Sekunden seit Beginn der Phase [s],
  \item \textbf{Spurmesswerte:} für jede anliegende Fahrspur
        \begin{itemize}
          \item Anzahl wartender Fahrzeuge [Fz],
          \item mittlere Geschwindigkeit [m/s],
          \item Dichte (Verhältnis belegter zu maximal möglicher Fahrzeuge pro Spur, dimensionslos, $0\text{–}1$).
        \end{itemize}
\end{itemize}

Da sich die Anzahl an Fahrspuren pro Kreuzung unterscheidet, variiert die Rohdimension des Beobachtungsvektors.
Um eine feste Eingabelänge für die Policy zu gewährleisten, werden alle Vektoren mittels \texttt{pad\_observations\_v0} auf die maximale Spuranzahl aller gesteuerten Kreuzungen gepaddet; fehlende Spurwerte werden mit Null belegt.
Dadurch ergibt sich für alle Agenten eine identische Beobachtungsdimension von
\[
  n_{\text{features}} \times n_{\text{max\_spuren}} + n_{\text{phasenfeatures}}
\]
mit $n_{\text{features}} = 3$ (Queue, Geschwindigkeit, Dichte) und $n_{\text{phasenfeatures}} = 2$ (Phase, Phasendauer).

Der Aktionsraum ist diskret und erlaubt die Auswahl der nächsten Signalphase.
Ein Phasenwechsel wird durch SUMO automatisch mit einer Zwischenphase (Gelbphase) ergänzt.
Alle Agenten entscheiden synchron in Intervallen von \texttt{delta\_time = 5\,s}.


\subsubsection{Trainingsalgorithmus und Hyperparameter}

Das Training der Agenten erfolgte mittels \texttt{PPO}, wobei folgende Hyperparameter eingesetzt wurden (siehe Anhang \ref{app:train_script}):

\begin{itemize}
  \item \textbf{Policy-Architektur:} Zwei Hidden-Layer mit jeweils 128 Neuronen; getrennte Netze für \texttt{pi} und \texttt{vf} (\texttt{policy\_kwargs}).
  \item \textbf{Batchgröße:} 512.
  \item \textbf{Rollout-Länge (\texttt{n\_steps}):} 2048.
  \item \textbf{Lernrate:} Cosine-Warmup über 5\,\% der Trainingszeit mit anschließendem Cosine-Decay auf 10\,\% des Startwerts ($3\cdot10^{-4}$).
  \item \textbf{Clip-Range:} Cosine-Interpoliert von 0.2 auf 0.1.
  \item \textbf{Entropiekoeffizient:} 0.01 (Exploration).
  \item \textbf{Discount-Faktor:} $\gamma = 0.99$.
  \item \textbf{GAE-Lambda:} 0.95.
  \item \textbf{Device:} CPU (\texttt{torch.set\_num\_threads} auf Standard; SUMO-Simulation single-threaded).
\end{itemize}


Beobachtungen und Rewards wurden über \texttt{VecNormalize} \cite{VecNormalize} normalisiert (\texttt{clip\_obs = 10.0}, \texttt{clip\_reward = 10.0}, \texttt{norm\_obs = True}, \texttt{norm\_reward = True}), die Umgebung zusätzlich mit \texttt{VecMonitor} \cite{VecMonitor} überwacht (\texttt{monitor.csv}). Pro Seed wurden $2\cdot10^6$ Trainingsschritte durchgeführt.


Die Trainingsdauer pro Seed betrug im Mittel 2~Stunden für $2\cdot10^6$ Schritte
bei einer Episodenlänge von \texttt{num\_seconds = 5000},
inklusive periodischer Checkpoint-Speicherung und Logging. Dies variert mit der eingesetzten Belohnungsfunktion.
Kürzere Vergleichsläufe mit $5\cdot10^5$ Schritten wurden in etwa 35~Minuten abgeschlossen.


Die Wahl der Hyperparameter erfolgte auf Basis einer Kombination aus Literaturrecherche
und empirischen Vorversuchen auf einem synthetischen Verkehrsnetz,
das strukturell an das finale Zielnetz angelehnt war.
Die Netzarchitektur mit zwei Hidden-Layern à 128 Neuronen wurde gewählt,
da sie in vergleichbaren SUMO-RL-Szenarien
eine gute Balance zwischen Modellkapazität und Generalisierungsfähigkeit bietet \cite{Stable-Baselines3, PPO}.


Die Batchgröße von 512 und die Rollout-Länge von 2048 Schritten stellen sicher,
dass pro Policy-Update ausreichend diversifizierte Datenpunkte vorliegen,
ohne den Speicherbedarf zu stark zu erhöhen.
Der Cosine-Schedule für Lernrate und Clip-Range dient dazu,
zu Beginn eine hohe Exploration zuzulassen und gegen Ende des Trainings die Updates zu stabilisieren.
Der Entropiekoeffizient von 0{,}01 wurde so gewählt,
dass auch in späten Trainingsphasen eine gewisse Exploration erhalten bleibt.


\subsubsection{Checkpoints, Monitoring und Logging}

Zur Sicherstellung eines robusten Trainingsprozesses wurde eine Reihe von Callback-Mechanismen implementiert (siehe Anhang \ref{app:train_script}):

\begin{itemize}
  \item \textbf{TimeBasedCheckpointCallback:}
        Zeitbasierte Sicherung des Modells und der Normalisierungsdaten in festen Intervallen
        (standardmäßig stündlich). Gespeicherte Dateien:
        \begin{itemize}
          \item \texttt{ppo\_sumo\_model\_\{steps\}\_steps.zip} – Modellparameter,
          \item \texttt{ppo\_sumo\_model\_\{steps\}\_steps\_vecnormalize.pkl} – Normalisierungszustand.
        \end{itemize}

  \item \textbf{EnvMetricsLoggerCallback:}
        Aggregiert und loggt Umgebungsmetriken aus dem \texttt{infos}-Dictionary.
        Schlüssel mit Präfix \texttt{system\_} werden in globale Mittelwerte umbenannt (\texttt{mean\_...}).
        Geloggte Metriken umfassen:
        \begin{itemize}
          \item \texttt{mean\_arrived} – mittlere Anzahl neu eingetroffener Fahrzeuge,
          \item \texttt{mean\_departed} – mittlere Anzahl abgefahrener Fahrzeuge,
          \item \texttt{mean\_backlogged} – mittlere Anzahl nicht abgefertigter Fahrzeuge,
          \item \texttt{mean\_running} – mittlere Anzahl aktuell fahrender Fahrzeuge,
          \item \texttt{mean\_stopped} – mittlere Anzahl stehender Fahrzeuge,
          \item \texttt{mean\_mean\_speed} – mittlere Durchschnittsgeschwindigkeit,
          \item \texttt{mean\_mean\_waiting\_time} – mittlere mittlere Wartezeit pro Fahrzeug,
          \item \texttt{mean\_waiting\_time} – mittlere Wartezeit (gesamt),
          \item \texttt{mean\_teleported} – mittlere Anzahl Teleportationen pro Schritt,
          \item \texttt{mean\_emissions\_per\_vehicle} – mittlere CO\textsubscript{2}-Emission pro Fahrzeug.
        \end{itemize}

  \item \textbf{BestModelSaverCallback:}
        Automatische Speicherung des jeweils besten Modells (höchster mittlerer Episodenreward aus \texttt{ep\_info\_buffer}),
        inklusive zugehöriger Normalisierungsdaten:
        \begin{itemize}
          \item \texttt{best\_model.zip} – bestes Modell,
          \item \texttt{best\_model\_vecnormalize.pkl} – Normalisierungszustand des besten Modells.
        \end{itemize}
\end{itemize}


Alle Modellartefakte (\texttt{.zip}, \texttt{vecnormalize.pkl}) sowie die TensorBoard-Logs \cite{TensorBoard}
wurden pro Seed-Version strukturiert gespeichert. Das Log-Verzeichnis folgt dem Schema
\texttt{runs/ppo\_sumo\_<SEED>\_<YYYY-MM-DD\_HH-MM-SS>}.
\texttt{tensorboard\_log} verweist auf diesen Ordner; \texttt{VecMonitor} schreibt zusätzlich \texttt{monitor.csv}.
TensorBoard wurde während des gesamten Trainings eingesetzt, um den Verlauf zentraler Kennzahlen
(z.\,B. Episodenreward, mittlere Geschwindigkeit, Wartezeiten, CO\textsubscript{2}-Emissionen,
Lernrate, Clip-Range) in Echtzeit zu überwachen.
Dies ermöglichte eine frühzeitige Erkennung von Instabilitäten (z.\,B. plötzliche Reward-Einbrüche)
sowie die Analyse des Lernfortschritts in unterschiedlichen Trainingsphasen.
Die Visualisierungen wurden zudem genutzt, um Hyperparameter-Anpassungen
in Vorversuchen gezielt zu validieren.

\subsubsection{Systemumgebung und Durchführung}


\paragraph{Hardware-Setup}
Alle Trainingsläufe wurden auf einem System mit
Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i7-6700K Prozessor
(4~Kerne, 8~Threads, 4{,}0\,GHz) und 16\,GB RAM durchgeführt.
Aufgrund der single-threaded Ausführung der SUMO-Simulation
wurde der RL-Algorithmus ebenfalls auf der CPU betrieben (\texttt{device='cpu'}),
was eine konsistente Laufzeitmessung und Reproduzierbarkeit sicherstellte.
Der durchschnittliche Speicherverbrauch pro Trainingslauf lag bei rund 9\,GB.
Die Multi-Core-Parallellisierung über \texttt{concat\_vec\_envs\_v1} war vorbereitet,
wurde jedoch nicht aktiviert, um Speicherverbrauch und Interprozesskommunikations-Overhead zu vermeiden.
Während des Trainings wurde eine durchschnittliche CPU-Auslastung von 60\,\%–70\,\% verzeichnet.

\paragraph{Versionen und Systemumgebung}
Die Experimente wurden unter folgender Systemumgebung durchgeführt:
\begin{itemize}
  \item \textbf{Betriebssystem:} Windows~10~Pro~(64-Bit)
  \item \textbf{Python:}~3.10.12
  \item \textbf{SUMO:}~1.18.0 (mit HBEFA~4.1-Emissionsmodell)
  \item \textbf{Stable-Baselines3:}~2.1.0
  \item \textbf{Gymnasium:}~0.29.1
  \item \textbf{PettingZoo:}~1.23.0
  \item \textbf{SuperSuit:}~3.9.0
  \item \textbf{PyTorch:}~2.1.2+cpu
  \item \textbf{sumo-rl:}~1.4.5
\end{itemize}
Die Pfade zu SUMO wurden über die Umgebungsvariable \texttt{SUMO\_HOME} gesetzt. Alle Experimente wurden mit deaktiviertem GUI-Modus gestartet, um den Simulationsfaktor zu maximieren.

\paragraph{Reproduzierbarkeit und Laufverwaltung}
Zur Sicherstellung der Reproduzierbarkeit wurden Seeds für NumPy, PyTorch und SUMO gesetzt; zusätzlich wird (falls verfügbar) die Umgebung selbst mit \texttt{env.seed(SEED)} initialisiert. Es wurden vier verschiedene Seed-Werte $\{546456, 678678, 234256, 678\}$ verwendet. Eine Hilfsroutine (\texttt{find\_latest\_complete\_run}) erlaubt die Identifikation des letzten vollständigen Laufs (finales Modell oder jüngster Checkpoint inkl. \texttt{VecNormalize}) zur Wiederaufnahme. Evaluationsläufe können nach dem Training mit deaktivierter Exploration (\texttt{deterministic=True}) gestartet werden.

\paragraph{Fehler- und Abbruchbehandlung}
Bei manuellem Abbruch (\texttt{KeyboardInterrupt}) werden der aktuelle Modellstand (\texttt{model\_interrupt.zip}) und Normalisierungsdaten (\texttt{vecnormalize\_interrupt.pkl}) gesichert. Im \texttt{finally}-Block wird die Normalisierung zusätzlich persistiert und die Umgebung sauber geschlossen.

\paragraph{Simulationsgeschwindigkeit und Parallelität}
Die Trainingsdauer für ein vollständiges Haupttraining mit $2 \times 10^6$ Schritten beträgt im Mittel etwa zwei Stunden pro Seed,
wobei die genaue Laufzeit stark von der Komplexität der verwendeten Reward-Funktion abhängt.
Eine Parallelisierung über mehrere CPU-Kerne wurde nicht umgesetzt,
da \texttt{sumo-rl} in der genutzten Version nicht vollständig thread-sicher ist
und \texttt{TraCI} keine automatisierte Portzuweisung für parallele SUMO-Instanzen erlaubt.
Der durchschnittliche Simulationsdurchsatz lag bei etwa $700$--$800$ Simulationsschritten pro realer Sekunde im \texttt{meso}-Modus
und $250$--$300$ Schritten/s im \texttt{default}-Modus (ohne GUI).


\subsubsection{Zusammenfassung}

Die konfigurierte RL-Umgebung erlaubt eine modulare und flexible Steuerung realer Verkehrsnetze auf Basis von SUMO. Durch die Kombination aus systematischer TLS-Auswahl, stabiler und geglätteter Reward-Funktion, adaptiven Hyperparameter-Schedules, vorbereiteter Multi-Core-Parallellisierung, konsequenter Reproduzierbarkeit und umfassendem Monitoring (inkl. Checkpoints, Metriklogging und Bestmodell-Erkennung) wurde eine solide Grundlage für die experimentelle Evaluation lernbasierter Verkehrssteuerung geschaffen.

\subsubsection{Evaluationsstrategie}
\label{sec:evaluation-strategy}

Zur abschließenden Bewertung der trainierten Modelle wird ein systematischer Evaluationsplan verfolgt (Details und Ergebnisse siehe Kapitel~\ref{chap:validation}).
Dabei wird stets das gleiche, bereinigte Verkehrsnetz verwendet, um die Vergleichbarkeit zu gewährleisten.
Variiert werden hingegen die folgenden Dimensionen:

\begin{itemize}
  \item \textbf{Trainierte Modelle:} Es werden mehrere Varianten von Agenten getestet, die sich durch unterschiedliche Belohnungsfunktionen unterscheiden (vgl. Abschnitt~\ref{sec:rl-konzept}).
  \item \textbf{Verkehrsszenarien:} Für die Simulation werden sowohl deterministisch definierte Routenflüsse (Trips) als auch stochastisch generierte Verkehrsmuster (RandomTrips) genutzt.
        Zusätzlich wird die Verkehrsstärke variiert, sodass Szenarien mit geringer, mittlerer und hoher Anzahl an Fahrzeugen betrachtet werden.
  \item \textbf{Baselines:} Als Vergleich dienen Referenzszenarien mit statischen Signalsteuerungen (z.\,B. feste Zykluszeiten, Zufallssteuerung).
\end{itemize}

Die Leistungsbewertung erfolgt anhand zentraler Verkehrs- und Umweltmetriken, darunter mittlere Reisezeit, mittlere Geschwindigkeit, Gesamtlänge der Warteschlangen sowie aggregierte CO\textsubscript{2}-Emissionen pro Fahrzeug und Zeiteinheit.
Alle Ergebnisse werden über mehrere unabhängige Seeds gemittelt, um die Robustheit der Aussagen zu erhöhen.
Darüber hinaus werden Evaluationsläufe mit zuvor nicht genutzten Seeds durchgeführt, um eine Überanpassung auf spezifische Verkehrsmuster auszuschließen.

Diese Strategie stellt sicher, dass die in Kapitel~\ref{chap:validation} präsentierten Ergebnisse reproduzierbar, belastbar und auf unterschiedliche Verkehrssituationen übertragbar sind.
