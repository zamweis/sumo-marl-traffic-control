\subsection{Einbindung des SUMO-Netzes in die RL-Umgebung}
\label{sec:sumo-rl-architektur}

Nach Abschluss der Netzbereinigung, der strukturellen Validierung und der Identifikation steuerbarer Lichtsignalanlagen (TLS) wurde das finale Verkehrsnetz in eine auf \texttt{sumo-rl} basierende Reinforcement-Learning-Umgebung integriert. Ziel war die Realisierung einer robusten, modularen Multiagentenumgebung, die eine lernbasierte Optimierung der Verkehrssteuerung unter realitätsnahen Bedingungen erlaubt.

\subsubsection{Gesamtsystem und Architektur}

Die Architektur der Lernumgebung ist als verteiltes Multiagentensystem ausgelegt, bei dem jede signalgesteuerte Kreuzung durch einen eigenständigen Agenten repräsentiert wird. Die Interaktion erfolgt über das TraCI-Protokoll \cite{TraCI} von SUMO \cite{sumo-doc}, das eine Echtzeitkommunikation zwischen Simulator und RL-Agenten ermöglicht. Die zentrale Steuerung und das Training der Agenten basiert auf der RL-Bibliothek \texttt{Stable-Baselines3} \cite{Stable-Baselines3}, konkret dem Algorithmus \texttt{Proximal Policy Optimization (PPO) \cite{PPO}}. \cite{sumo-rl_docs}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1cm]

        \node[box] (sim) {Verkehrssimulation\\\textbf{SUMO + TraCI}};
        \node[box, below=of sim] (mae) {Multi-Agent Environment (MAE)\\\texttt{sumo-rl.parallel\_env} + \texttt{emissions\_with\_speed\_reward}};
        \node[box, below=of mae] (wrap) {SuperSuit Wrappers\\\texttt{- pad\_obs / pad\_actions}\\\texttt{- pettingzoo\_to\_vec\_env}\\\texttt{- concat\_vec\_envs}};
        \node[box, below=of wrap] (vec) {VecNormalize + VecMonitor\\(Stable-Baselines3)};
        \node[box, below=of vec] (ppo) {PPO Agent (Stable-Baselines3)\\\texttt{- MLPPolicy}\\\texttt{- Schedules: LR (cosine warmup), Clip (cosine)}};
        \node[box, below=of ppo] (cb) {Callbacks\\\texttt{- TimeBasedCheckpoint}\\\texttt{- EpisodeMetricsLogger}\\\texttt{- BestModelSaver}};
        \node[box, below=of cb] (log) {Modell- \& Log-Speicherung\\\texttt{(pro Seed in runs/)}};

        \draw[arrow] (sim) -- (mae);
        \draw[arrow] (mae) -- (wrap);
        \draw[arrow] (wrap) -- (vec);
        \draw[arrow] (vec) -- (ppo);
        \draw[arrow] (ppo) -- (cb);
        \draw[arrow] (cb) -- (log);

    \end{tikzpicture}
    \caption{Architektur der RL-Trainingspipeline mit SUMO, MAE und Stable-Baselines3}
    \label{fig:rl_architektur}
\end{figure}

Zur Vereinheitlichung der Multiagentenumgebung kamen die Bibliotheken \texttt{PettingZoo} \cite{PettingZoo} und \texttt{SuperSuit} \cite{SuperSuit} zum Einsatz. \texttt{PettingZoo} stellt ein standardisiertes API für Multiagenten-Umgebungen bereit, analog zu \texttt{Gymnasium} \cite{Gymnasium}, jedoch speziell für Szenarien mit mehreren Agenten. \texttt{SuperSuit} erweitert diese Umgebungen durch eine Vielzahl an Wrappers, beispielsweise zur Vereinheitlichung von Beobachtungs- und Aktionsräumen (z.\,B. durch Padding) oder zur Umwandlung in vektorisierte Formate. Für parallele Ausführung ist die Umgebung mittels \texttt{concat\_vec\_envs\_v1} auf Mehrkernbetrieb vorbereitet (\texttt{num\_cpus = 8}); im vorliegenden Skript (siehe Anhang \ref{app:train_script}) wird eine Instanz (\texttt{num\_vec\_envs = 1}) betrieben, sodass die Skalierung ohne Codeänderungen möglich ist.

\subsubsection{Agentenzuordnung im Multiagentensystem}
Jede steuerbare Lichtsignalanlage (TLS) wird einem eindeutigen Agenten zugeordnet.
Die Zuordnung erfolgt deterministisch anhand der in der SUMO-Netzdatei vergebenen TLS-IDs.
Die Agenten handeln vollständig lokal, es findet kein direkter Informationsaustausch zwischen ihnen statt.
Jede Beobachtung wird vor der Weitergabe an die Policy mittels \texttt{VecNormalize} standardisiert,
sodass alle Eingaben über verschiedene Kreuzungen hinweg vergleichbar sind.
Die Beobachtungsvektoren werden mit \texttt{pad\_observations\_v0} auf eine feste Länge gebracht,
die sich aus der maximalen Spuranzahl einer Kreuzung multipliziert mit der Anzahl Spurfeatures (Queue, Geschwindigkeit, Dichte)
zuzüglich der Phasenfeatures (Phase, Phasendauer) ergibt.

\subsubsection{Vermeidung von Überanpassung}
Um zu verhindern, dass die trainierten Agenten lediglich auf wiederkehrende Verkehrsmuster optimiert werden,
wurden die Routenflüsse mit \texttt{randomTrips.py} generiert.
Dadurch variiert die Nutzung einzelner Strecken, und es entstehen unterschiedliche Verkehrsverteilungen innerhalb der Episoden.
Zusätzlich wurden während des Trainings mit \texttt{TensorBoard} zentrale Leistungsmetriken (mittlere Geschwindigkeit, Wartezeit, CO$_2$-Emissionen, Durchsatz) in Echtzeit überwacht,
um sicherzustellen, dass sich die Leistungsverbesserungen nicht nur auf die Trainingsdaten beziehen.
Evaluationsläufe mit zuvor nicht genutzten Zufallsseeds dienen der abschließenden Validierung (siehe Kapitel~\ref{sec:validation}).


\subsubsection{Konfiguration der Umgebung}
\label{sec:training-setup}
Die Trainingsumgebung basiert auf der Klasse \texttt{parallel\_env} aus dem \texttt{sumo-rl}-Framework, wurde jedoch stark erweitert und in eine
Stable-Baselines3-kompatible Vektor-Umgebung eingebettet.
Dabei wurden die folgenden Aspekte berücksichtigt:

\begin{itemize}
    \item \textbf{Verkehrsnetz:} Grundlage ist das bereinigte Netz \texttt{map.net.xml}.
    \item \textbf{Routenprofile:} Drei Routendateien (\texttt{flows\_low.rou.xml}, \texttt{flows\_medium.rou.xml}, \texttt{flows\_high.rou.xml}) werden
          zyklisch per Reset geladen, sodass das Modell im Training auf verschiedene Lastprofile trifft.
    \item \textbf{Seed-Handling:} Vier unterschiedliche Seeds (\{143534, 456, 635768, 13755\}) sorgen für Reproduzierbarkeit und Variation.
    \item \textbf{Episode-Länge:} Jede Episode läuft für 4096 Simulationssekunden.
    \item \textbf{Entscheidungsintervall:} Ampelaktionen werden alle 5\,s (\texttt{delta\_time}) neu gewählt.
    \item \textbf{Belohnungsfunktionen:} Durch die direkte Erweiterung des \texttt{sumo-rl}-Frameworks können unterschiedliche Reward-Funktionen flexibel eingebunden werden,
          wodurch eine einheitliche Schnittstelle für Experimente mit verschiedenen Zielfunktionen entsteht (siehe Anhang~\ref{app:rewardfunktionen}).
    \item \textbf{Systemmetriken:} \texttt{add\_system\_info=True} aktiviert globale Kennzahlen wie mittlere Wartezeit oder Gesamtemissionen.
    \item \textbf{Multi-Agent-Setup:} Jede steuerbare Ampel im Netz wird als eigenständiger Agent behandelt, was der PettingZoo-Schnittstelle entspricht.
    \item \textbf{Integration in SB3:} Über \texttt{SuperSuit} (Padding, Konvertierung, Vektorisierung) wird die Umgebung an Stable-Baselines3 angebunden.
    \item \textbf{Normalisierung:} Rewards und Beobachtungen werden mit \texttt{VecNormalize} z-standardisiert, um das Training zu stabilisieren.
\end{itemize}

Zusätzlich sind mehrere Callback-Mechanismen implementiert:
\begin{itemize}
    \item \textbf{Checkpoints:} Zeitbasiertes Speichern der Modelle (stündlich).
    \item \textbf{Logging:} Aggregierte Episodenmetriken (Wartezeiten, Queues, Fluss) werden in TensorBoard und CSV-Dateien geschrieben.
    \item \textbf{Bestes Modell:} Automatisches Sichern des jeweils besten Modells nach mittlerem Episoden-Reward.
\end{itemize}

Das Training erfolgt mit PPO (\texttt{stable-baselines3}) und nutzt eine Cosine-Schedule für Lernrate und Clip-Range,
sowie eine zweischichtige MLP-Policy (\texttt{[128,128]}). Jede Run wird im Ordner \texttt{runs/} mit Zeitstempel protokolliert.


Die Umgebung ist vollständig kompatibel mit \texttt{Gymnasium}, \texttt{PettingZoo} sowie den Wrapper-Bibliotheken \texttt{SuperSuit} und \texttt{VecEnv}, wodurch ein standardisiertes Interfacing mit RL-Algorithmen ermöglicht wird.

\subsubsection{Trainingsalgorithmus und Hyperparameter}

Das Training der Agenten erfolgte mittels \texttt{PPO}, wobei folgende Hyperparameter eingesetzt wurden (siehe Anhang \ref{app:train_script}):

\begin{itemize}
    \item \textbf{Policy-Architektur:} Zwei Hidden-Layer mit jeweils 128 Neuronen; getrennte Netze für \texttt{pi} und \texttt{vf} (\texttt{policy\_kwargs}).
    \item \textbf{Batchgröße:} 512.
    \item \textbf{Rollout-Länge (\texttt{n\_steps}):} 2048.
    \item \textbf{Lernrate:} Cosine-Warmup über 5\,\% der Trainingszeit mit anschließendem Cosine-Decay auf 10\,\% des Startwerts ($3\cdot10^{-4}$).
    \item \textbf{Clip-Range:} Cosine-Interpoliert von 0.2 auf 0.1.
    \item \textbf{Entropiekoeffizient:} 0.01 (Exploration).
    \item \textbf{Discount-Faktor:} $\gamma = 0.99$.
    \item \textbf{GAE-Lambda:} 0.95.
    \item \textbf{Device:} CPU (\texttt{torch.set\_num\_threads} auf Standard; SUMO-Simulation single-threaded).
\end{itemize}


Beobachtungen und Rewards wurden über \texttt{VecNormalize} \cite{VecNormalize} normalisiert (\texttt{clip\_obs = 10.0}, \texttt{clip\_reward = 10.0}, \texttt{norm\_obs = True}, \texttt{norm\_reward = True}), die Umgebung zusätzlich mit \texttt{VecMonitor} \cite{VecMonitor} überwacht (\texttt{monitor.csv}). Pro Seed wurden $2\cdot10^6$ Trainingsschritte durchgeführt.


Die Trainingsdauer pro Seed betrug im Mittel 2~Stunden für $2\cdot10^6$ Schritte
bei einer Episodenlänge von \texttt{num\_seconds = 5000},
inklusive periodischer Checkpoint-Speicherung und Logging. Dies variert mit der eingesetzten Belohnungsfunktion.
Kürzere Vergleichsläufe mit $5\cdot10^5$ Schritten wurden in etwa 35~Minuten abgeschlossen.


Die Wahl der Hyperparameter erfolgte auf Basis einer Kombination aus Literaturrecherche
und empirischen Vorversuchen auf einem synthetischen Verkehrsnetz,
das strukturell an das finale Zielnetz angelehnt war.
Die Netzarchitektur mit zwei Hidden-Layern à 128 Neuronen wurde gewählt,
da sie in vergleichbaren SUMO-RL-Szenarien
eine gute Balance zwischen Modellkapazität und Generalisierungsfähigkeit bietet \cite{Stable-Baselines3, PPO}.


Die Batchgröße von 512 und die Rollout-Länge von 2048 Schritten stellen sicher,
dass pro Policy-Update ausreichend diversifizierte Datenpunkte vorliegen,
ohne den Speicherbedarf zu stark zu erhöhen.
Der Cosine-Schedule für Lernrate und Clip-Range dient dazu,
zu Beginn eine hohe Exploration zuzulassen und gegen Ende des Trainings die Updates zu stabilisieren.
Der Entropiekoeffizient von 0{,}01 wurde so gewählt,
dass auch in späten Trainingsphasen eine gewisse Exploration erhalten bleibt.

\subsubsection{Checkpoints, Monitoring und Logging}

Zur Sicherstellung eines robusten und reproduzierbaren Trainingsprozesses wurde ein
umfassendes Callback- und Logging-System integriert (siehe Anhang~\ref{app:train_script}).
Dieses setzt direkt auf den Mechanismen von Stable-Baselines3 auf und umfasst:

\begin{itemize}
    \item \textbf{TimeBasedCheckpointCallback:}
          Zeitbasierte Sicherung des aktuellen Modells und der Normalisierungsdaten
          in festen Intervallen (standardmäßig stündlich).
          Die Dateien werden mit Schrittzähler im Namen versioniert
          und ermöglichen die Wiederaufnahme ab beliebigen Zwischenständen.

    \item \textbf{EpisodeMetricsLoggerCallback:}
          Aggregiert Kennzahlen direkt aus dem \texttt{infos}-Dictionary der SUMO-Umgebung
          und schreibt sie in das Stable-Baselines3-Logger-Interface.
          Damit wird ein kontinuierliches Monitoring pro Episode gewährleistet.

    \item \textbf{BestModelSaverCallback:}
          Automatische Speicherung des jeweils besten Modells,
          gemessen am mittleren Episodenreward (\texttt{ep\_info\_buffer}).
          Neben den Modellparametern werden auch die Normalisierungsdaten gesichert,
          sodass eine konsistente Reproduktion möglich ist.
\end{itemize}

\paragraph{Geloggte Metriken.}
Alle relevanten Metriken werden zusätzlich im CSV-Format und über TensorBoard \cite{TensorBoard} erfasst.
Die wichtigsten überwachten Größen sind:

\begin{itemize}
    \item \textbf{Szenario und Methode:} \texttt{scenario}, \texttt{method}, \texttt{run\_dir},
    \item \textbf{Systemmetriken:} Anzahl fahrender Fahrzeuge (\texttt{system\_total\_running}),
          gestoppter Fahrzeuge (\texttt{system\_total\_stopped}),
          eingetroffener Fahrzeuge (\texttt{system\_total\_arrived}),
          abgefertigter Fahrzeuge (\texttt{system\_total\_departed}),
          backlogged Fahrzeuge (\texttt{system\_total\_backlogged}),
          Teleports (\texttt{system\_total\_teleported}),
    \item \textbf{Leistungsmetriken:} mittlere Wartezeit (\texttt{system\_mean\_waiting\_time}),
          gesamte Wartezeit (\texttt{system\_total\_waiting\_time}),
          mittlere Geschwindigkeit (\texttt{system\_mean\_speed}),
    \item \textbf{Trainingsmetriken:} mittlerer Episodenreward (\texttt{ep\_rew\_mean}),
          Episodenlänge (\texttt{ep\_len\_mean}),
          verwendeter Seed (\texttt{ep\_seed\_mean}),
          sowie aggregierte Episodenstatistiken (\texttt{episode\_mean}).
\end{itemize}

TensorBoard diente als zentrales Werkzeug, um den Verlauf relevanter Kennzahlen in Echtzeit zu überwachen und Trainingstendenzen (z. B. Reward-Stabilität, Auswirkungen von Teleports oder Backlog-Aufbau) nachvollziehbar zu machen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{tensorboard.PNG}
    \caption{Beispielhafte Lernkurve eines Trainingslaufs, visualisiert mit TensorBoard. Der Reward stabilisiert sich nach einer Explorationsphase.}
    \label{fig:tensorboard_example}
\end{figure}


\paragraph{Artefakte und Struktur.}
Alle Modellartefakte (\texttt{.zip}, \texttt{vecnormalize.pkl}),
sowie die zugehörigen \texttt{monitor.csv}-Dateien und TensorBoard-Logs
werden pro Seed in einem Run-spezifischen Ordner gespeichert
(\texttt{runs/ppo\_sumo\_<SEED>\_<YYYY-MM-DD\_HH-MM-SS>}).
TensorBoard diente als zentrales Werkzeug, um den Verlauf relevanter Kennzahlen
in Echtzeit zu überwachen und Trainingstendenzen (z.\,B. Reward-Stabilität,
Auswirkungen von Teleports oder Backlog-Aufbau) nachvollziehbar zu machen.


\subsubsection{Systemumgebung und Durchführung}


\paragraph{Hardware-Setup}
Alle Trainingsläufe wurden auf einem System mit
Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i7-6700K Prozessor
(4~Kerne, 8~Threads, 4{,}0\,GHz) und 16\,GB RAM durchgeführt.
Aufgrund der single-threaded Ausführung der SUMO-Simulation
wurde der RL-Algorithmus ebenfalls auf der CPU betrieben (\texttt{device='cpu'}),
was eine konsistente Laufzeitmessung und Reproduzierbarkeit sicherstellte.
Der durchschnittliche Speicherverbrauch pro Trainingslauf lag bei rund 9\,GB.
Die Multi-Core-Parallellisierung über \texttt{concat\_vec\_envs\_v1} war vorbereitet,
wurde jedoch nicht aktiviert, um Speicherverbrauch und Interprozesskommunikations-Overhead zu vermeiden.
Während des Trainings wurde eine durchschnittliche CPU-Auslastung von 60\,\%–70\,\% verzeichnet.

\paragraph{Versionen und Systemumgebung}
Die Experimente wurden unter folgender Systemumgebung durchgeführt:
\begin{itemize}
    \item \textbf{Betriebssystem:} Windows~10~Pro~(64-Bit)
    \item \textbf{Python:}~3.10.12
    \item \textbf{SUMO:}~1.18.0 (mit HBEFA~4.1-Emissionsmodell)
    \item \textbf{Stable-Baselines3:}~2.1.0
    \item \textbf{Gymnasium:}~0.29.1
    \item \textbf{PettingZoo:}~1.23.0
    \item \textbf{SuperSuit:}~3.9.0
    \item \textbf{PyTorch:}~2.1.2+cpu
    \item \textbf{sumo-rl:}~1.4.5
\end{itemize}
Die Pfade zu SUMO wurden über die Umgebungsvariable \texttt{SUMO\_HOME} gesetzt. Alle Experimente wurden mit deaktiviertem GUI-Modus gestartet, um den Simulationsfaktor zu maximieren.

\paragraph{Reproduzierbarkeit und Laufverwaltung}
Zur Sicherstellung der Reproduzierbarkeit wurden Seeds für NumPy, PyTorch und SUMO gesetzt; zusätzlich wird (falls verfügbar) die Umgebung selbst mit \texttt{env.seed(SEED)} initialisiert. Es wurden vier verschiedene Seed-Werte $\{546456, 678678, 234256, 678\}$ verwendet. Eine Hilfsroutine (\texttt{find\_latest\_complete\_run}) erlaubt die Identifikation des letzten vollständigen Laufs (finales Modell oder jüngster Checkpoint inkl. \texttt{VecNormalize}) zur Wiederaufnahme. Evaluationsläufe können nach dem Training mit deaktivierter Exploration (\texttt{deterministic=True}) gestartet werden.

\paragraph{Fehler- und Abbruchbehandlung}
Bei manuellem Abbruch (\texttt{KeyboardInterrupt}) werden der aktuelle Modellstand (\texttt{model\_interrupt.zip}) und Normalisierungsdaten (\texttt{vecnormalize\_interrupt.pkl}) gesichert. Im \texttt{finally}-Block wird die Normalisierung zusätzlich persistiert und die Umgebung sauber geschlossen.

\paragraph{Simulationsgeschwindigkeit und Parallelität}
Die Trainingsdauer für ein vollständiges Haupttraining mit $2 \times 10^6$ Schritten beträgt im Mittel etwa zwei Stunden pro Seed,
wobei die genaue Laufzeit stark von der Komplexität der verwendeten Reward-Funktion abhängt.
Eine Parallelisierung über mehrere CPU-Kerne wurde nicht umgesetzt,
da \texttt{sumo-rl} in der genutzten Version nicht vollständig thread-sicher ist
und \texttt{TraCI} keine automatisierte Portzuweisung für parallele SUMO-Instanzen erlaubt.
Der durchschnittliche Simulationsdurchsatz lag bei etwa $700$--$800$ Simulationsschritten pro realer Sekunde im \texttt{meso}-Modus
und $250$--$300$ Schritten/s im \texttt{default}-Modus (ohne GUI).


\subsubsection{Zusammenfassung}

Die konfigurierte RL-Umgebung erlaubt eine modulare und flexible Steuerung realer Verkehrsnetze auf Basis von SUMO. Durch die Kombination aus systematischer TLS-Auswahl, stabiler und geglätteter Reward-Funktion, adaptiven Hyperparameter-Schedules, vorbereiteter Multi-Core-Parallellisierung, konsequenter Reproduzierbarkeit und umfassendem Monitoring (inkl. Checkpoints, Metriklogging und Bestmodell-Erkennung) wurde eine solide Grundlage für die experimentelle Evaluation lernbasierter Verkehrssteuerung geschaffen.

