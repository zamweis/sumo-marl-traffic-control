\subsection{Reinforcement-Learning-Konzept}
\label{sec:rl-konzept}

\subsubsection{Formulierung des RL-Problems}
Das Problem der Verkehrssteuerung wird als sequentielles Entscheidungsproblem \cite{sutton-barto} modelliert und mit Hilfe von Reinforcement Learning (RL) gelöst.
Ein Agent interagiert dabei mit der SUMO-Simulationsumgebung, welche den Verkehrsfluss in diskreten Zeitschritten abbildet.
Der Agent beobachtet den aktuellen Zustand \(s_t\), wählt eine Aktion \(a_t\), erhält eine Belohnung \(r_t\) und beeinflusst dadurch den Folgezustand \(s_{t+1}\).
Ziel ist es, eine Policy \(\pi(a|s)\) zu erlernen, die langfristig die erwartete kumulierte Belohnung maximiert.

Im Trainingsskript sind die verschiedenen Belohnungsfunktionen jeweils als eigene Python-Funktion implementiert und werden über den Parameter \texttt{reward\_fn} in der SUMO-Umgebung ausgewählt.
Dadurch können unterschiedliche Optimierungsziele systematisch untersucht werden.
Die Anbindung zwischen RL-Agent und Simulation erfolgt über das \textbf{Traffic Control Interface (TraCI)}, das eine Laufzeitsteuerung von SUMO ermöglicht (siehe Kaptitel ~\ref{sec:TraCI}).
Über TraCI werden sowohl Zustandsgrößen wie Rückstaulängen oder aktuelle Ampelphasen abgefragt als auch die Entscheidungen des Agenten direkt in die Simulation zurückgeschrieben.
Dies bildet die Rückkopplungsschleife, die für den Trainingsprozess im Reinforcement Learning erforderlich ist.

\subsubsection{Auswahl des RL-Algorithmus}
Für das Training wurde der Algorithmus \textit{Proximal Policy Optimization} (PPO) eingesetzt.
PPO ist ein \textit{on-policy} Policy-Gradient-Verfahren, das durch eine Clip-Funktion für Policy-Updates stabile Lernkurven und gute Sample-Effizienz ermöglicht~\cite{PPO}.
Der Algorithmus hat sich in zahlreichen Studien zur Verkehrsoptimierung mit SUMO als robust und leistungsfähig erwiesen, insbesondere im Multiagenten-Setting~\cite{PPO,zheng2022}.

\paragraph{Begründung der Algorithmuswahl}
Alternative Verfahren wie \textit{Deep Q-Networks (DQN)} oder \textit{Advantage Actor-Critic (A2C)} wurden verworfen, da sie bei kontinuierlichen Zustandsräumen und in Multiagentenszenarien oft instabile Lernverläufe zeigen.
\textit{Soft Actor-Critic (SAC)} gilt zwar als state-of-the-art für kontinuierliche Steuerungsprobleme, erfordert aber große Replay-Puffer und hohe Rechenressourcen.
PPO stellt somit einen sinnvollen Kompromiss zwischen Stabilität, Implementationsaufwand und empirischer Leistungsfähigkeit dar und integriert sich nahtlos in die bestehende \texttt{sumo-rl}-Umgebung.

\subsubsection{Zustände}
Der Zustand \(s_t\) beschreibt die Verkehrssituation an einer Kreuzung zum Zeitpunkt \(t\).
Die Zustandsrepräsentation umfasst: \cite{sumo-doc}
\begin{itemize}
  \item Fahrzeuganzahl pro Spur (mittels \texttt{laneAreaDetector}),
  \item Länge der Warteschlange (Anzahl Fahrzeuge mit \(v < 0.1 \,\mathrm{m/s}\)),
  \item Durchschnittsgeschwindigkeit pro Spur,
  \item aktuelle Ampelphase (TLS state),
  \item Dauer der aktuellen Phase,
  \item Binärmasken für die Phasenwechselbarkeit\footnote{Die Phasenwechselbarkeit gibt an, ob ein Wechsel zu einer bestimmten nächsten Phase im aktuellen Zeitpunkt zulässig ist. Dies hängt von internen Restriktionen wie Mindestgrün- oder Gelbzeiten ab. Die Binärmaske verhindert damit ungültige Agentenaktionen.},
  \item optional aggregierte Zustände benachbarter Kreuzungen (Multiagentensetting).
\end{itemize}
Die Merkmale werden normiert und als Eingabevektor an das neuronale Netz des Agenten übergeben.

\subsubsection{Aktionen}
Die Aktionsmenge \(A\) beschreibt die Eingriffsmöglichkeiten in die Steuerung der Lichtsignalanlage:
\begin{enumerate}
  \item \textbf{Phasenwechsel-Modell:} Der Agent entscheidet binär, ob die aktuelle Phase fortgesetzt oder gewechselt wird (\(A = \{\texttt{keep}, \texttt{switch}\}\)).
  \item \textbf{Direktwahl-Modell:} Der Agent wählt direkt eine der möglichen Phasen (\(A = \{\texttt{phase}_0, \ldots, \texttt{phase}_n\}\)).
\end{enumerate}
Einschränkungen sind u.\,a. Mindestgrünzeiten, automatisch generierte Zwischenphasen (Gelb, Räumzeit) und die Synchronität mehrerer Agentenentscheidungen.
Zur Vermeidung hektischer Schaltungen wird ein Action-Interval (z.\,B. 5\,s) definiert.
\subsubsection{Belohnungsfunktionen}
\label{sec:rewardfunktionen}

Im Rahmen dieser Arbeit wurden vier unterschiedliche Belohnungsfunktionen entwickelt und getestet.
Eine Übersicht der Varianten, ihrer Optimierungsziele und Bezüge zur Literatur zeigt Tabelle~\ref{tab:reward-overview}.

\begin{table}[H]
  \centering
  \begin{tabular}{p{3cm} p{6cm} p{3.5cm}}
    \toprule
    \textbf{Reward-Variante}   & \textbf{Optimierungsziel}                                                                           & \textbf{Bezug zur Literatur}                      \\ \midrule
    \texttt{diff-waiting-time} & Reduktion der kumulierten Wartezeit durch Minimierung der Wartezeitdifferenz zwischen Zeitschritten & \cite{sumo-rl-tls}                                \\
    \texttt{queue}             & Minimierung von Staus, gemessen über die Anzahl gestoppter Fahrzeuge                                & \cite{sumo-rl-tls}                                \\
    \texttt{realworld}         & Kombination mehrerer in der Praxis messbarer Größen (Geschwindigkeit, Queues, Wartezeit)            & Eigene Erweiterung                                \\
    \texttt{emissions}         & Erweiterung um CO\textsubscript{2}-Emissionen zur Berücksichtigung von Nachhaltigkeit               & \cite{zheng2022, mschrader15}, Eigene Erweiterung \\
    \bottomrule
  \end{tabular}
  \caption{Übersicht der implementierten Reward-Funktionen und deren theoretische Motivation.}
  \label{tab:reward-overview}
\end{table}

Für das Haupttraining wurde die Funktion \texttt{realworld} verwendet, da sie ausschließlich auf Metriken basiert,
die in realen Städten mit vergleichsweise einfacher Sensorik (z.\,B. Induktionsschleifen, Kameras) erhoben werden können.
Die übrigen Varianten wurden für Vergleichsstudien eingesetzt (siehe Anhang~\ref{app:rewardfunktionen}).

\paragraph{Beschreibung der implementierten Funktionen}

\begin{itemize}
  \item \textbf{\texttt{diff-waiting-time}}
        (siehe Anhang~\ref{app:reward_diff_waiting_time})
        Diese Funktion misst die Differenz der akkumulierten Wartezeit zwischen zwei Zeitschritten.
        Eine Abnahme der Gesamtwartezeit wird positiv belohnt.

  \item \textbf{\texttt{queue}}
        (siehe Anhang~\ref{app:reward_queue})
        Hier wird die Anzahl der gestoppten Fahrzeuge direkt als negativer Reward verwendet.
        Weniger Fahrzeuge im Stau führen zu höherem Reward.

  \item \textbf{\texttt{realworld}}
        (siehe Anhang~\ref{app:reward_realworld})
        Diese Belohnungsfunktion kombiniert drei Metriken: durchschnittliche Geschwindigkeit, Länge der Warteschlangen und mittlere Wartezeit.
        Sie bildet einen Kompromiss zwischen Verkehrsfluss und Staureduktion.

  \item \textbf{\texttt{emissions}}
        (siehe Anhang~\ref{app:reward_emissions})
        Neben den Metriken aus \texttt{realworld} wird zusätzlich ein Term für CO\textsubscript{2}-Emissionen berücksichtigt.
        Damit soll eine Balance zwischen Verkehrsleistung und Umweltverträglichkeit gefunden werden.
\end{itemize}

\subsubsection{Hyperparameter-Anpassung}
Für PPO wurden neben festen Werten auch dynamische \emph{Schedules} eingesetzt, um Lernrate, Clip-Range oder Entropiekoeffizient während des Trainings anzupassen.
Implementiert wurden u.\,a. Cosine-Warmup mit anschließendem Absenken, lineare Absenkung und adaptive Entropie-Steuerung.

\paragraph{Motivation der gewählten Schedules}
Diese Schedules vermeiden typische Probleme wie frühes Overfitting oder stagnierende Exploration.
Das Cosine-Warmup erlaubt eine initiale Erkundung bei hoher Lernrate, während der anschließende Cosine-Decay den Lernprozess stabilisiert.
Die adaptive Entropiesteuerung sorgt dafür, dass zu Beginn stark exploriert wird und im weiteren Verlauf zunehmend exploitation-basiert gelernt wird, ein Vorgehen, das sich in verwandten Arbeiten bewährt hat~\cite{Adaptive_Exploration}.
