\subsection{Reinforcement-Learning-Konzept}
\subsubsection{Formulierung des RL-Problems}
Das Problem der Verkehrssteuerung wird als sequentielles Entscheidungsproblem \cite{sutton-barto} modelliert und mit Hilfe von Reinforcement Learning (RL) gelöst.
Es wurden mehrere RL-Modelle trainiert, die sich in der verwendeten Belohnungsfunktion und den Hyperparameter-Strategien unterscheiden.
Im Trainingsskript ist jede Belohnungsfunktion als eigene Python-Funktion implementiert, und die jeweils gewünschte Variante wird durch Setzen des Parameters \texttt{reward\_fn} im Aufruf der SUMO-Umgebung ausgewählt.
So lassen sich einzelne Varianten gezielt ein- oder ausschalten.
Die Umgebung besteht aus dem simulierten Straßennetz, wie es in SUMO definiert ist. \cite{TraCI}
Die Interaktion erfolgt über das TraCI-Interface, das eine Laufzeitsteuerung der Ampelanlagen erlaubt.

\paragraph{Zustände}
Der Zustand \( s_t \) eines Reinforcement-Learning-Agenten beschreibt die Verkehrssituation an einer einzelnen Kreuzung zum Zeitpunkt \( t \). Ziel ist es, dem Agenten ausreichend Informationen über den lokalen Verkehrsfluss zur Verfügung zu stellen, damit er fundierte Entscheidungen über die Steuerung der Lichtsignalanlage treffen kann.

Die Zustandsrepräsentation basiert auf den folgenden Komponenten: \cite{sumo-doc}
\begin{itemize}
  \item \textbf{Fahrzeuganzahl pro Zufahrtsspur:} Für jede dem Knoten zuführende Fahrspur wird die aktuelle Anzahl an Fahrzeugen ermittelt. Dies geschieht über sogenannte \texttt{laneAreaDetector}, die für jede Spur individuell in SUMO definiert werden. Die Werte werden periodisch über \texttt{TraCI} abgefragt.
  \item \textbf{Warteschlangenlänge (queue length):} Gibt die Anzahl der Fahrzeuge an, die sich auf einer Spur mit Geschwindigkeit \texttt{< 0.1 m/s} befinden. Dies ist ein wichtiges Maß für Rückstaus an Kreuzungen.
  \item \textbf{Durchschnittliche Geschwindigkeit pro Spur:} Diese Kenngröße erlaubt Rückschlüsse auf den Verkehrsfluss pro Richtung und ergänzt die reine Anzahlinformation.
  \item \textbf{Ampelphase (TLS state):} Die aktuell geschaltete Ampelphase wird als diskrete Phase kodiert (z.\,B. 0, 1, 2, ...). In SUMO entspricht dies dem Index der aktiven Phase im Phasenplan der TLS.
  \item \textbf{Dauer der aktuellen Phase:} Die Anzahl der Zeitschritte seit Beginn der aktuellen Phase. Diese Information ist notwendig, um Phasenlängen sinnvoll zu steuern (z.\,B. Mindestgrünzeit).
  \item \textbf{Binärmasken zur Phasenwechselbarkeit:} Kodierung, ob ein Wechsel zur nächsten Phase gemäß Übergangsbedingungen (z.\,B. Mindestgrünzeit) aktuell möglich ist. Diese Information ist erforderlich, falls das Aktionsmodell auch direkte Sprünge zwischen nicht direkt benachbarten Phasen erlaubt.
  \item \textbf{Optional – Nachbarschaftszustand:} In Multi-Agent-Settings kann es sinnvoll sein, zusätzlich aggregierte Zustandsinformationen benachbarter Knoten einzubeziehen (z.\,B. Gesamtwarteschlange auf ausgehenden Spuren, die zu benachbarten TLS führen).
\end{itemize}

Die Zustände werden zu einem normierten Merkmalsvektor kombiniert und bilden damit die Eingabe für das neuronale Entscheidungsmodell des Agenten.

\paragraph{Aktionen}
Die Aktionsmenge \( A \) eines Agenten beschreibt die Eingriffsmöglichkeiten in den Steuerungsablauf der jeweiligen Ampelkreuzung. Dabei wird zwischen zwei gängigen Aktionsmodellen unterschieden: \cite{sumo-rl_docs}
\begin{enumerate}
  \item \textbf{Phasenwechsel-Modell:} Der Agent entscheidet, ob die aktuelle Phase fortgesetzt oder zur nächsten gewechselt werden soll. Es handelt sich um ein binäres Aktionsmodell:
        \[
          A = \{ \texttt{keep},\ \texttt{switch} \}
        \]
        Diese Variante wird häufig in klassischen SUMO-RL-Implementierungen verwendet (z.\,B. \texttt{sumo-rl}). Die Reihenfolge der Phasen ist dabei festgelegt (z.\,B. zyklischer Übergang).
  \item \textbf{Direktwahl-Modell:} Der Agent wählt direkt aus allen möglichen Phasen die nächste aus:
        \[
          A = \{ \texttt{phase}_0,\ \texttt{phase}_1,\ \ldots,\ \texttt{phase}_n \}
        \]
        Diese Variante erfordert eine eigene Definition der Übergangslogik in SUMO (z.\,B. über permissive TLS-Ketten), erlaubt aber größere Flexibilität und exploratives Verhalten.
\end{enumerate}

Unabhängig vom Modell gelten folgende Einschränkungen: \cite{sumo-doc,sumo-rl_docs}
\begin{itemize}
  \item \textbf{Mindestgrünzeiten:} Ein Wechsel der Phase darf erst nach einer definierten Mindestgrünzeit erfolgen (z.\,B. 5\,s), um realistische Signalisierung und Verkehrssicherheit zu gewährleisten.
  \item \textbf{Sicherheitsbedingte Zwischenphasen:} SUMO erzwingt automatisch Zwischenphasen wie Gelb- oder Räumzeiten. Der Agent gibt nur den Phasenwunsch an, die exakte Ablaufsteuerung erfolgt durch das TLS-Modell in SUMO.
  \item \textbf{Simultane Agentenentscheidung:} Bei mehreren Knoten wird jeder TLS-Agent unabhängig gesteuert, es sei denn, ein zentrales Multi-Agent-Training wird implementiert.
\end{itemize}

Zur Reduktion der Aktionsfrequenz wird häufig ein sogenanntes \textbf{Action Interval} festgelegt (z.\,B. alle 5\,s), sodass Entscheidungen nur in bestimmten Zeitschritten getroffen werden können. Dies verhindert zu hektisches Umschalten der Ampeln.

\paragraph{Belohnungsfunktionen}
Es wurden mehrere alternative Belohnungsfunktionen implementiert, um unterschiedliche Optimierungsziele zu untersuchen:
\begin{itemize}
  \item \textbf{Emissions-basiert mit Geschwindigkeit:} kombiniert mittlere CO\(_2\)-Emissionen, mittlere Geschwindigkeit, Fahrzeugfluss und Warteschlangenlänge.
  \item \textbf{Reisezeit-basiert:} optimiert die mittlere Reisezeit aller Fahrzeuge.
  \item \textbf{Realworld-Variante:} berücksichtigt u.\,a. Warteschlangenaufbau, Fahrzeugfluss und Phasenwechselkosten.
  \item \textbf{Custom-Variante:} gewichtet Wartezeiten, Warteschlangenänderungen, Ankünfte, Teleports und Kollisionen.
  \item \textbf{Zeit-Emission-Mix:} kombiniert Reisezeit- und Emissionsoptimierung.
\end{itemize}
Die aktive Variante wird im Code durch Setzen von \texttt{reward\_fn} ausgewählt (siehe Anhang \ref{app:train_script}).

\paragraph{Hyperparameter-Anpassung}
Für die RL-Algorithmen (hier PPO) \cite{PPO} wurden neben festen Parametern auch zeitabhängige \emph{Schedules} eingesetzt, um Lernrate, Clip-Range oder Entropiekoeffizient während des Trainings dynamisch anzupassen.
Umgesetzt wurden u.\,a. Cosine-Warmup-Strategien \cite{warmup} mit anschließendem Absenken, lineare Absenkung \cite{Adaptive_Exploration} und adaptive Entropie-Steuerung.
Die jeweiligen Anpassungsfunktionen werden beim Erstellen des Modells übergeben und steuern die Werte abhängig vom Trainingsfortschritt (siehe Anhang \ref{app:train_script}).
