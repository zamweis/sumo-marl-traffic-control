\subsection{Reinforcement-Learning-Konzept}
\label{sec:rl-konzept}

\subsubsection{Formulierung des RL-Problems}
Das Problem der Verkehrssteuerung wird als sequentielles Entscheidungsproblem \cite{sutton-barto} modelliert und mit Hilfe von Reinforcement Learning (RL) gelöst.
Ein Agent interagiert dabei mit der SUMO-Simulationsumgebung, welche den Verkehrsfluss in diskreten Zeitschritten abbildet.
Der Agent beobachtet den aktuellen Zustand \(s_t\), wählt eine Aktion \(a_t\), erhält eine Belohnung \(r_t\) und beeinflusst dadurch den Folgezustand \(s_{t+1}\).
Ziel ist es, eine Policy \(\pi(a|s)\) zu erlernen, die langfristig die erwartete kumulierte Belohnung maximiert.

Im Trainingsskript sind die verschiedenen Belohnungsfunktionen jeweils als eigene Python-Funktion implementiert und werden über den Parameter \texttt{reward\_fn} in der SUMO-Umgebung ausgewählt.
Dadurch können unterschiedliche Optimierungsziele systematisch untersucht werden.
Die Kommunikation mit der Simulation erfolgt über das TraCI-Interface \cite{TraCI}, welches eine Laufzeitsteuerung der Lichtsignalanlagen erlaubt.

\subsubsection{Auswahl des RL-Algorithmus}
Für das Training wurde der Algorithmus \textit{Proximal Policy Optimization} (PPO) eingesetzt.
PPO ist ein \textit{on-policy} Policy-Gradient-Verfahren, das durch eine Clip-Funktion für Policy-Updates stabile Lernkurven und gute Sample-Effizienz ermöglicht~\cite{PPO}.
Der Algorithmus hat sich in zahlreichen Studien zur Verkehrsoptimierung mit SUMO als robust und leistungsfähig erwiesen, insbesondere im Multiagenten-Setting~\cite{PPO,zheng2022}.

\paragraph{Begründung der Algorithmuswahl}
Alternative Verfahren wie \textit{Deep Q-Networks (DQN)} oder \textit{Advantage Actor-Critic (A2C)} wurden verworfen, da sie bei kontinuierlichen Zustandsräumen und in Multiagentenszenarien oft instabile Lernverläufe zeigen.
\textit{Soft Actor-Critic (SAC)} gilt zwar als state-of-the-art für kontinuierliche Steuerungsprobleme, erfordert aber große Replay-Puffer und hohe Rechenressourcen.
PPO stellt somit einen sinnvollen Kompromiss zwischen Stabilität, Implementationsaufwand und empirischer Leistungsfähigkeit dar und integriert sich nahtlos in die bestehende \texttt{sumo-rl}-Umgebung.

\subsubsection{Zustände}
Der Zustand \(s_t\) beschreibt die Verkehrssituation an einer Kreuzung zum Zeitpunkt \(t\).
Die Zustandsrepräsentation umfasst: \cite{sumo-doc}
\begin{itemize}
  \item Fahrzeuganzahl pro Spur (mittels \texttt{laneAreaDetector}),
  \item Warteschlangenlänge (Anzahl Fahrzeuge mit \(v < 0.1 \,\mathrm{m/s}\)),
  \item Durchschnittsgeschwindigkeit pro Spur,
  \item aktuelle Ampelphase (TLS state),
  \item Dauer der aktuellen Phase,
  \item Binärmasken zur Phasenwechselbarkeit,
  \item optional aggregierte Zustände benachbarter Kreuzungen (Multiagentensetting).
\end{itemize}
Die Merkmale werden normiert und als Eingabevektor an das neuronale Netz des Agenten übergeben.

\subsubsection{Aktionen}
Die Aktionsmenge \(A\) beschreibt die Eingriffsmöglichkeiten in die Steuerung der Lichtsignalanlage:
\begin{enumerate}
  \item \textbf{Phasenwechsel-Modell:} Der Agent entscheidet binär, ob die aktuelle Phase fortgesetzt oder gewechselt wird (\(A = \{\texttt{keep}, \texttt{switch}\}\)).
  \item \textbf{Direktwahl-Modell:} Der Agent wählt direkt eine der möglichen Phasen (\(A = \{\texttt{phase}_0, \ldots, \texttt{phase}_n\}\)).
\end{enumerate}
Einschränkungen sind u.\,a. Mindestgrünzeiten, automatisch generierte Zwischenphasen (Gelb, Räumzeit) und die Synchronität mehrerer Agentenentscheidungen.
Zur Vermeidung hektischer Schaltungen wird ein Action-Interval (z.\,B. 5\,s) definiert.

\subsubsection{Belohnungsfunktionen}

\texttt{sumo-rl} unterstützt verschiedene standardisierte Reward-Funktionen: \cite{sumo-rl_docs}

\begin{itemize}
  \item \texttt{"diff-waiting-time"}: Reduktion der Differenz kumulierter Wartezeiten,
  \item \texttt{"average-speed"}: Maximierung der mittleren Geschwindigkeit im Netz,
  \item \texttt{"queue"}: Minimierung der Gesamtlänge aller Warteschlangen.
\end{itemize}

Im Rahmen dieser Arbeit wurden zusätzlich mehrere eigene Reward-Funktionen implementiert.
Eine Übersicht zeigt Tabelle~\ref{tab:reward-overview}.

\begin{table}[H]
  \centering
  \begin{tabular}{p{3cm} p{5cm} p{3.5cm}}
    \toprule
    \textbf{Reward-Variante} & \textbf{Optimierungsziel}                   & \textbf{Bezug zur Literatur} \\ \midrule
    Emissions-basiert        & Reduktion lokaler Emissionen                & \cite{mschrader15,zheng2022} \\
    Reisezeit-basiert        & Minimierung mittlerer Reisezeit             & Eigene Erweiterung           \\
    Realworld-Variante       & Praktisch messbare Größen (Queues, Fluss)   & Eigene Erweiterung           \\
    Custom-Variante          & Kombination mehrerer Indikatoren            & Eigene Erweiterung           \\
    Zeit-Emission-Mix        & Trade-off zwischen Zeit und Umweltbelastung & \cite{mschrader15}           \\ \bottomrule
  \end{tabular}
  \caption{Übersicht der implementierten Reward-Funktionen und deren theoretische Motivation.}
  \label{tab:reward-overview}
\end{table}

Für das Haupttraining wurde die \texttt{realworld\_reward}-Funktion verwendet, da sie ausschließlich auf Metriken basiert,
die in der Praxis mit vergleichsweise einfacher Sensorik (z.\,B. Kameras, Induktionsschleifen) erfasst werden können.
Für Vergleichsläufe kamen die weiteren Varianten aus Tabelle~\ref{tab:reward-overview} zum Einsatz (siehe Anhang \ref{app:train_script}).

\paragraph{Formale Definition der Realworld-Belohnungsfunktion}
Die Belohnungsfunktion \texttt{realworld\_reward} berechnet für jede Zeiteinheit $t$ den Reward $R_t$ wie folgt:

\begin{equation}
  R_t = - w_q \cdot q_{\mathrm{norm}} - w_{\mathrm{build}} \cdot b_{\mathrm{norm}} + w_f \cdot f_{\mathrm{norm}} - w_{\mathrm{switch}} \cdot \mathrm{phase\_sw}
\end{equation}

mit:
\begin{align}
  q_{\mathrm{norm}} & = \frac{\mathrm{EMA}(Q_t)}{Q_{\max}}                \\
  b_{\mathrm{norm}} & = \frac{\max(0, Q_t - Q_{t-1})}{0.2 \cdot Q_{\max}} \\
  f_{\mathrm{norm}} & = \frac{\mathrm{EMA}(F_t)}{F_{\max}}
\end{align}

wobei gilt:
\begin{itemize}
  \item $Q_t$: aktuelle Warteschlangenlänge
  \item $F_t$: aktuelle Durchflussrate
  \item $\mathrm{EMA}(\cdot)$: Exponentieller gleitender Mittelwert mit $\alpha = 0.3$
  \item $Q_{\max} = 40$ Fahrzeuge
  \item $F_{\max} = 8$ Fahrzeuge/Zeitschritt
  \item $\mathrm{phase\_sw} \in \{0, 1\}$: Indikator für Phasenwechsel im aktuellen Schritt
  \item Gewichtungen: $w_q = 1.0$, $w_{\mathrm{build}} = 0.8$, $w_f = 0.7$, $w_{\mathrm{switch}} = 0.1$
\end{itemize}

Der Reward wird auf $[-5, 5]$ geclippt, um Ausreißer zu begrenzen.
Er belohnt hohe Durchflussraten und stabile Warteschlangen, während unnötige Phasenwechsel sowie starker Warteschlangenaufbau bestraft werden.


\subsubsection{Hyperparameter-Anpassung}
Für PPO wurden neben festen Werten auch dynamische \emph{Schedules} eingesetzt, um Lernrate, Clip-Range oder Entropiekoeffizient während des Trainings anzupassen.
Implementiert wurden u.\,a. Cosine-Warmup mit anschließendem Absenken, lineare Absenkung und adaptive Entropie-Steuerung.

\paragraph{Motivation der gewählten Schedules}
Diese Schedules vermeiden typische Probleme wie frühes Overfitting oder stagnierende Exploration.
Das Cosine-Warmup erlaubt eine initiale Erkundung bei hoher Lernrate, während der anschließende Cosine-Decay den Lernprozess stabilisiert.
Die adaptive Entropiesteuerung sorgt dafür, dass zu Beginn stark exploriert wird und im weiteren Verlauf zunehmend exploitation-basiert gelernt wird – ein Vorgehen, das sich in verwandten Arbeiten bewährt hat~\cite{Adaptive_Exploration}.
