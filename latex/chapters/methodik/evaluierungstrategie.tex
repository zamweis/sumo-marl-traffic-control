\subsection{Evaluationsstrategie}
\label{sec:evaluation-strategy}

Zur Bewertung der trainierten Modelle wird ein systematischer, skriptbasierter Evaluationsplan verfolgt (Details und Ergebnisse siehe Kapitel~\ref{sec:validation}).
Dabei wird stets dasselbe, bereinigte Verkehrsnetz (\texttt{map.net.xml}) verwendet, um die Vergleichbarkeit zu gewährleisten. Variiert werden Signalsteuerung, Routenbelegung und Seeds.
Statische Baselines werden direkt im Evaluationslauf mitgeführt (fester Phasenplan und aktuiert).

\subsubsection{Architektur}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=6mm]

  % --- Kopf ---
  \node[box-small] (cfg) {Config\\\scriptsize RUNS, SCENARIOS, N\_EPISODES, SEEDS};

  \node[box-small, below=of cfg] (loops) {Schleifen\\\scriptsize for scenario \& for episode};

  % --- Drei Pfade nebeneinander ---
  \node[box-small, below=10mm of loops, xshift=-50mm] (fix_env) {Baseline \textit{fixed-time}\\\scriptsize make\_env\_baseline(True)};
  \node[box-small, below=of fix_env] (fix_roll) {Rollout\\\scriptsize rollout\_baseline()};

  \node[box-small, below=10mm of loops] (act_env) {Baseline \textit{actuated}\\\scriptsize make\_env\_baseline(False)};
  \node[box-small, below=of act_env] (act_roll) {Rollout\\\scriptsize rollout\_baseline()};

  \node[box-small, below=10mm of loops, xshift=50mm] (rl_env) {RL (pro Run)\\\scriptsize make\_env()};
  \node[box-small, below=of rl_env] (rl_load) {Laden\\\scriptsize VecNormalize + PPO};
  \node[box-small, below=of rl_load] (rl_roll) {Rollout\\\scriptsize deterministisch};

  % --- Sammeln / Loggen ---
  \node[box-small, below=22mm of act_roll] (metrics) {Metriken\\\scriptsize mean\_* \& total\_*};
  \node[box-small, below=of metrics] (log) {Logging\\\scriptsize TensorBoard};
  \node[box-small, below=of log] (save) {Export\\\scriptsize eval\_results.json};

  % --- JSON2CSV ---
  \node[box-small, below=of save] (csv) {json2csv\\\scriptsize raw.csv, agg.csv, pro Methode};

  % --- Pfeile ---
  \draw[arrow] (cfg) -- (loops);

  % von loops aus nach links/mitte/rechts
  \draw[arrow] (loops.west) -| (fix_env.north);
  \draw[arrow] (loops.south) -- (act_env.north);
  \draw[arrow] (loops.east) -| (rl_env.north);

  \draw[arrow] (fix_env) -- (fix_roll);
  \draw[arrow] (act_env) -- (act_roll);
  \draw[arrow] (rl_env) -- (rl_load) -- (rl_roll);

  \draw[arrow] (fix_roll) |- (metrics.west);
  \draw[arrow] (act_roll) -- (metrics);
  \draw[arrow] (rl_roll) |- (metrics.east);

  \draw[arrow] (metrics) -- (log) -- (save) -- (csv);

\end{tikzpicture}
\caption{Evaluations-Architektur: Szenario/Episode → \{Fixed, Actuated, RL\} → Metriken → Logging → JSON → CSV}
\label{fig:rl_eval_arch_with_csv}
\end{figure}


\subsubsection{Vergleichsszenarien}
Die Evaluation der RL-Agenten und der Baselines erfolgt vollautomatisch über \texttt{evaluate.py}.
Vier Szenarien decken typische Lastprofile ab:
\begin{itemize}
  \item \textbf{morning\_peak} (\texttt{flows\_morning.rou.xml}): morgendliche Lastspitze,
  \item \textbf{evening\_peak} (\texttt{flows\_evening.rou.xml}): abendliche Lastspitze,
  \item \textbf{uniform} (\texttt{flows\_uniform.rou.xml}): gleichmäßige Zuflüsse,
  \item \textbf{random\_heavy} (\texttt{flows\_random\_heavy.rou.xml}): stochastisch dichter Verkehr.
\end{itemize}

Die Lastprofile \emph{morning\_peak} und \emph{evening\_peak} wurden so definiert,
dass morgens der Verkehr überwiegend in das Stadtzentrum hineinführt,
während er abends nach außen abfließt.
Das Szenario \emph{random\_heavy} dient dazu, das Netz an seine Kapazitätsgrenzen zu bringen
und die Robustheit der Steuerungsansätze unter hoher Last zu überprüfen.

\subsubsection{Bewertete Modelle}
Die zu evaluierenden RL-Modelle werden automatisch aus \texttt{runs/} ermittelt:
\begin{itemize}
  \item Alle Unterordner mit Präfix \texttt{ppo\_sumo\_*}.
  \item Aus jedem Lauf wird das \textbf{beste Modell} \texttt{best\_model.zip} sowie die zugehörige \textbf{Normalisierung} \texttt{vecnormalize.pkl} geladen.
\end{itemize}
Zusätzlich zu den RL-Läufen werden im selben Skript die Baselines (Fixed-Time, Actuated) ausgeführt.

\subsubsection{Reward-Funktionen}
Im Evaluationslauf werden Rewards nicht genutzt. Stattdessen basiert die Bewertung ausschließlich auf den von SUMO gelieferten Systemmetriken (\texttt{system\_mean\_*}, \texttt{system\_total\_*}).
Dies stellt sicher, dass unterschiedliche Modelle und Baselines unabhängig von der jeweiligen Trainings-Rewardfunktion objektiv vergleichbar sind.

\subsubsection{Simulations- und Umgebungsparameter}
Für alle Simulationen (RL und Baselines) gelten konsistente Parameter:
\begin{itemize}
  \item Episodenlänge: \texttt{EP\_LENGTH\_S = 4096}~s,
  \item Minimaldauer Grünphase: \texttt{min\_green = 5}~s,
  \item Maximaler Abfahrtsverzug: \texttt{max\_depart\_delay = 100}~s,
  \item Zusätzliche Systemmetriken: \texttt{add\_system\_info = True},
  \item Pro-Agent-Infos: \texttt{add\_per\_agent\_info = False},
  \item SUMO läuft ohne GUI (\texttt{use\_gui=False}).
\end{itemize}

\paragraph{RL-Umgebung.}
Die PettingZoo-Umgebung wird mit SuperSuit vorbereitet (\texttt{pad\_observations\_v0}, \texttt{pad\_action\_space\_v0}, \texttt{pettingzoo\_env\_to\_vec\_env\_v1}, \texttt{concat\_vec\_envs\_v1}) und mit \texttt{VecMonitor} versehen.
Die Normalisierung wird aus \texttt{vecnormalize.pkl} geladen und eingefroren (\texttt{training=False}, \texttt{norm\_reward=False}); die Modelle werden deterministisch ausgeführt.

\paragraph{Baselines.}
Für die Baselines wird die SUMO-interne Steuerung genutzt (\texttt{fixed\_ts=True} für Fixed-Time, \texttt{fixed\_ts=False} für Actuated).
Ein \emph{Dummy-Reward} (\texttt{0.0}) stellt sicher, dass kein RL-Einfluss auf die Steuerung erfolgt; die Aktionen bestehen aus einer konstant wiederholten, gültigen Sample-Aktion.

\subsubsection{Ablauf je Episode}
Der Ablauf ist für RL und Baselines einheitlich:
\begin{enumerate}
  \item Reset der Umgebung,
  \item Simulation bis zum Terminalzustand,
  \item Pro Schritt: (i) Aktion (bei RL deterministisch via \texttt{model.predict}, bei Baselines konstante Dummy-Aktion), (ii) \texttt{env.step}, (iii) Metrik-Sampling aus \texttt{infos},
  \item Episodenende: Mittelwerte aller \texttt{system\_mean\_*}-Keys sowie Übernahme der finalen \texttt{system\_total\_*}-Werte.
\end{enumerate}

\subsubsection{Seeds und Replikationsdesign}
Das Seed-Design ist eindimensional: Es wird eine feste Menge von zehn \emph{Episoden-Seeds} (\texttt{EP\_SEEDS}) verwendet, die vom Training entkoppelt sind.
Pro Kombination aus \emph{Szenario}, \emph{Methode} und \emph{Modelllauf} werden \texttt{N\_EPISODES = 10} Episoden ausgewertet.
Da zwei Baselines (Fixed-Time, Actuated) sowie alle RL-Läufe unter \texttt{runs/ppo\_sumo\_*} berücksichtigt werden, ergibt sich insgesamt (mit 4 Seeds):

\begin{align*}
  \#\text{Episoden} & = (2 + |\texttt{RUNS}|)\cdot |\texttt{SCENARIOS}|\cdot \texttt{N\_EPISODES} \\
                    & = (2 + |\texttt{RUNS}|)\cdot 4 \cdot 10                                     \\
                    & = 240
\end{align*}


\subsubsection{Metriken und Logging}
Es werden alle numerischen \texttt{system\_mean\_*}- und \texttt{system\_total\_*}-Keys gespeichert.
Für die Visualisierung werden Präfixe teilweise entfernt (\texttt{system\_*} $\rightarrow$ ohne Präfix).
Pro Kombination werden Metriken für \texttt{Baseline\_FixedTime}, \texttt{Baseline\_Actuated} und \texttt{RL} geloggt (\texttt{tensorboard} und \texttt{stdout}); die Logs liegen unter
\texttt{evaluation/logs/eval\_\{scenario\}}.
Die aggregierten Ergebnisse werden zusätzlich in \texttt{evaluation/eval\_results.json} persistiert.

\subsubsection{Reproduzierbarkeit}
Die Evaluation ist deterministisch durch:
\begin{enumerate}
  \item feste \texttt{EP\_SEEDS},
  \item deterministische Aktionswahl bei RL (\texttt{deterministic=True}),
  \item eingefrorene \texttt{VecNormalize}-Parameter,
  \item feste Simulationsdauer, identische Netz- und Routen-Dateien.
\end{enumerate}

Diese Strategie stellt sicher, dass die in Kapitel~\ref{sec:validation} präsentierten Ergebnisse reproduzierbar, belastbar und zwischen RL-Varianten sowie Baselines vergleichbar sind.

Das gesamte Vorgehen ist in einem dedizierten Evaluationsskript (\texttt{evaluate.py}) implementiert,
welches im Anhang dokumentiert ist (siehe Anhang~\ref{app:evaluate_script}).
Dieses Skript stellt sicher, dass sämtliche hier beschriebenen Schritte
systematisch und reproduzierbar umgesetzt werden.

